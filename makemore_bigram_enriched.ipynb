{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea290c8f",
   "metadata": {},
   "source": [
    "\n",
    "# makemore – Part 1: Bigram Language Model (Tutorial + Exercises)\n",
    "\n",
    "In this notebook we will **build a tiny character‑level language model** for baby names,\n",
    "inspired by Andrej Karpathy’s *makemore* project.\n",
    "\n",
    "We’ll start with **pure counting** (a simple statistical model), then build a tiny\n",
    "**neural network** that does almost the same thing but is trained with **gradient descent**.\n",
    "\n",
    "You do **not** need the original video to follow this.  \n",
    "This notebook is designed to be **self‑contained** and exercise‑driven.\n",
    "\n",
    "You will:\n",
    "\n",
    "- Load a dataset of names.\n",
    "- Build a **bigram** model — a model of \"next character given current character\".\n",
    "- Turn raw counts into probabilities and **sample new names**.\n",
    "- Compute the **log‑likelihood** of the data under your model.\n",
    "- Rebuild the model as a tiny neural network and **train it with gradient descent**.\n",
    "\n",
    "Each section has:\n",
    "\n",
    "1. **Concept / motivation** – what we’re doing and why.  \n",
    "2. **Exercise cell** – with `### YOUR CODE HERE` and `raise NotImplementedError(...)`.  \n",
    "3. **Solution cell** – a reference implementation.\n",
    "\n",
    "Try the exercise first, then check the solution.\n",
    "\n",
    "---\n",
    "### Quick mental picture\n",
    "\n",
    "- A **language model** is a function that sees some text and tells you how likely different next characters (or words) are.\n",
    "- In this notebook, the \"text\" is just a baby name like `isabella` or `liam`.\n",
    "- We treat every name as a sequence of characters, e.g. `isabella` → `i, s, a, b, e, l, l, a`.\n",
    "\n",
    "So a bigram model is learning numbers like:\n",
    "\n",
    "- “If the current character is `i`, how likely is the next character `s`?”\n",
    "- “If the current character is `l`, how likely is the next character `l` again?”\n",
    "- “If we just saw the special start symbol `.`, how likely is `m` as the first letter?”\n",
    "\n",
    "Once we know **all those probabilities**, we can let the model \"babble\" new names by repeatedly sampling a next character.\n",
    "\n",
    "---\n",
    "\n",
    "### Toy example (to keep in your head)\n",
    "\n",
    "Imagine the dataset is only three names:\n",
    "\n",
    "```text\n",
    "ana\n",
    "bob\n",
    "amy\n",
    "```\n",
    "\n",
    "We’d look at all consecutive character pairs (including a start `.` and end `.`):\n",
    "\n",
    "- `.a`, `a.n`, `n.a`, `a.`\n",
    "- `.b`, `b.o`, `o.b`, `b.`\n",
    "- `.a`, `a.m`, `m.y`, `y.`\n",
    "\n",
    "From just these counts we could already say things like:\n",
    "\n",
    "- After `.`, `a` is more common than `b`.\n",
    "- After `a`, `n`, `.` and `m` are all possible next characters.\n",
    "\n",
    "Everything we do later is just a **fancier, vectorized, neural‑networkified** version of this toy story.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a85036",
   "metadata": {},
   "source": [
    "\n",
    "## Setup – imports and dataset\n",
    "\n",
    "We’ll work with a text file `names.txt` that has one baby name per line, e.g.\n",
    "\n",
    "```text\n",
    "emma\n",
    "oliver\n",
    "ava\n",
    "liam\n",
    "...\n",
    "```\n",
    "\n",
    "Each name is a **sequence of characters**. We’ll think of this as text data and learn\n",
    "a model of what character tends to come next.\n",
    "\n",
    "First we’ll import some libraries and load the list of names into memory.\n",
    "\n",
    "---\n",
    "### What the `words` list really is\n",
    "\n",
    "After running the code cell below, `words` is just a **plain Python list of strings**:\n",
    "\n",
    "```python\n",
    "['emma', 'oliver', 'ava', 'liam', ...]\n",
    "```\n",
    "\n",
    "You can use all normal list operations on it:\n",
    "\n",
    "```python\n",
    "words[0]      # first name\n",
    "len(words)    # how many names we have\n",
    "len(words[0]) # how many characters in the first name\n",
    "```\n",
    "\n",
    "Try poking at `words` a bit when you run the notebook; getting very comfortable with the raw data will make the later tensor code feel much less mysterious.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d3ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the dataset: one name per line\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "len(words), words[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e53de9",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 1 – Exploring the dataset\n",
    "\n",
    "Before building any model, always **look at your data**.\n",
    "\n",
    "We have `words`, which is just a Python list of strings.  \n",
    "Let’s answer a few basic questions:\n",
    "\n",
    "- How many names are there?\n",
    "- What are some examples?\n",
    "- What is the **shortest** name?\n",
    "- What is the **longest** name?\n",
    "\n",
    "This gives us a feel for the scale of the problem and typical sequence lengths.\n",
    "\n",
    "### Task\n",
    "\n",
    "Using the list `words`:\n",
    "\n",
    "1. Display the first 10 names.\n",
    "2. Compute:\n",
    "   - the total number of names,\n",
    "   - the minimum length,\n",
    "   - the maximum length.\n",
    "\n",
    "Print them in a readable way.\n",
    "\n",
    "---\n",
    "### Solution discussion\n",
    "\n",
    "The key ideas:\n",
    "\n",
    "- `len(words)` gives the **number of names** in the dataset.\n",
    "- `words[:10]` slices the first 10 elements of the list (Python slicing).\n",
    "- To talk about **lengths of names**, you take `len(w)` for each `w` in `words`:\n",
    "\n",
    "  ```python\n",
    "  lengths = [len(w) for w in words]\n",
    "  ```\n",
    "\n",
    "  Now:\n",
    "  - `min(lengths)` is the length of the shortest name,\n",
    "  - `max(lengths)` is the length of the longest name.\n",
    "\n",
    "Why this matters:\n",
    "\n",
    "- Knowing typical lengths (say 2–15 characters) helps us reason about how many prediction steps our model will usually have to make.\n",
    "- Later, when we generate new names, you’ll recognize if their lengths look “reasonable” compared to the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c780f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 1 – your turn\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 1: explore the names dataset\")\n",
    "\n",
    "# Hints:\n",
    "# - len(words)          -> number of names\n",
    "# - len(w) for w in ... -> name lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1f1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 1 – exploring the dataset\n",
    "\n",
    "print(\"Number of names:\", len(words))\n",
    "print(\"First 10 names:\", words[:10])\n",
    "\n",
    "lengths = [len(w) for w in words]\n",
    "print(\"Shortest name length:\", min(lengths))\n",
    "print(\"Longest   name length:\", max(lengths))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60bbb3e",
   "metadata": {},
   "source": [
    "\n",
    "## Bigram models – what are we trying to learn?\n",
    "\n",
    "We want a model that can **generate new names** that look \"name‑like\".\n",
    "\n",
    "One very simple idea is a **bigram model**:\n",
    "\n",
    "> The probability of the next character depends only on the current character.\n",
    "\n",
    "So the model needs to know, for example:\n",
    "\n",
    "- After `'a'`, how likely is `'n'`, `'l'`, `' '` (end), etc.?\n",
    "- After `'b'`, how likely is `'r'`, `'a'`, ...?\n",
    "\n",
    "We’ll also introduce special **start** and **end** markers so we know where names begin and end.\n",
    "\n",
    "- Let’s use `'<S>'` for \"start\" and `'<E>'` for \"end\" (we’ll also later use `'.'` as a start/end symbol).\n",
    "\n",
    "For each name `w`, we will look at all consecutive character pairs:\n",
    "\n",
    "```text\n",
    "<S> a n a <E>\n",
    "    ^ ^ ^ ^\n",
    "    | | | |\n",
    "  bigrams: (<S>, 'a'), ('a', 'n'), ('n', 'a'), ('a', '<E>')\n",
    "```\n",
    "\n",
    "Our goal in this section is to **count** how often each bigram occurs in the dataset.\n",
    "\n",
    "---\n",
    "### Why “bigram”?\n",
    "\n",
    "A **bigram** just means “a pair of things in a row”.\n",
    "\n",
    "Here the “things” are characters. For the name `emma` (ignoring start/end for a moment) the character bigrams are:\n",
    "\n",
    "```text\n",
    "e m\n",
    "m m\n",
    "m a\n",
    "```\n",
    "\n",
    "When we *do* include start/end (`.`), we get:\n",
    "\n",
    "```text\n",
    ". e\n",
    "e m\n",
    "m m\n",
    "m a\n",
    "a .\n",
    "```\n",
    "\n",
    "Our model will learn numbers like:\n",
    "\n",
    "- \\(P(\\text{'m'} \\mid \\text{'e'})\\) – “If I just saw an `e`, how likely is the next character `m`?”\n",
    "- \\(P(\\text{'.'} \\mid \\text{'a'})\\) – “If I just saw an `a`, how likely is the name to end now?”\n",
    "\n",
    "We **ignore everything except the current character** when predicting the next one.  \n",
    "That’s what makes it a bigram model instead of a more powerful “look at the last 3 or 10 characters” model.\n",
    "\n",
    "Later in the course we’ll relax this and let the network remember *longer* context, but bigrams are the perfect sandbox to understand all the core ideas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44148b8",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2 – Bigram counts with a Python dictionary\n",
    "\n",
    "We’ll start with a simple Python dictionary `b` that maps\n",
    "\n",
    "```python\n",
    "(ch1, ch2) -> count\n",
    "```\n",
    "\n",
    "where `ch1` and `ch2` are characters (or `'<S>'` / `'<E>'`).\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "1. Start with an empty dict `b = {}`.\n",
    "2. For each word `w` in `words`:\n",
    "   - Build a list of characters including start and end markers:\n",
    "\n",
    "     ```python\n",
    "     chs = ['<S>'] + list(w) + ['<E>']\n",
    "     ```\n",
    "\n",
    "   - Loop over adjacent pairs `(ch1, ch2)` using `zip(chs, chs[1:])`.\n",
    "   - Increment `b[(ch1, ch2)]` by 1 (using `.get(..., 0)` for convenience).\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Build the dictionary `b` of bigram counts.\n",
    "2. Print how many **distinct** bigrams appear (`len(b)`).\n",
    "3. Print the **top 20 most frequent** bigrams and their counts, sorted by decreasing count.\n",
    "\n",
    "---\n",
    "### Solution discussion\n",
    "\n",
    "We use a Python **dictionary** `b` where:\n",
    "\n",
    "```python\n",
    "b[('a', 'n')] = 123\n",
    "```\n",
    "\n",
    "means the pair `'a'` followed by `'n'` occurred 123 times in the whole dataset.\n",
    "\n",
    "The important pieces:\n",
    "\n",
    "- `chs = ['<S>'] + list(w) + ['<E>']`  \n",
    "  turns `\"emma\"` into `['<S>', 'e', 'm', 'm', 'a', '<E>']`.\n",
    "\n",
    "- `zip(chs, chs[1:])`  \n",
    "  lines up all consecutive pairs:\n",
    "\n",
    "  ```python\n",
    "  ['<S>', 'e', 'm', 'm', 'a', '<E>']\n",
    "       ['e',  'm', 'm', 'a', '<E>']\n",
    "  # → ('<S>', 'e'), ('e', 'm'), ('m', 'm'), ('m', 'a'), ('a', '<E>')\n",
    "  ```\n",
    "\n",
    "- `b[bigram] = b.get(bigram, 0) + 1`  \n",
    "  says “look up the existing count (0 if we’ve never seen this bigram), then add 1”.\n",
    "\n",
    "Sorting by the value (`key=lambda kv: -kv[1]`) lets us see **which character pairs are most common**.  \n",
    "Those common pairs will be the ones the model leans on most when it starts generating names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e975cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 2 – your turn: bigram counts in a Python dict\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 2: build bigram counts dict 'b'\")\n",
    "\n",
    "# Hints:\n",
    "# - Use: chs = ['<S>'] + list(w) + ['<E>']\n",
    "# - Use: for ch1, ch2 in zip(chs, chs[1:]): ...\n",
    "# - Use: b[bigram] = b.get(bigram, 0) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 2 – bigram counts in a dict\n",
    "\n",
    "b = {}\n",
    "for w in words:\n",
    "    chs = ['<S>'] + list(w) + ['<E>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1\n",
    "\n",
    "print(\"Number of distinct bigrams:\", len(b))\n",
    "\n",
    "# Sort by decreasing count\n",
    "top20 = sorted(b.items(), key=lambda kv: -kv[1])[:20]\n",
    "for (ch1, ch2), count in top20:\n",
    "    print(f\"{ch1!r}{ch2!r}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771bc748",
   "metadata": {},
   "source": [
    "\n",
    "## From dictionary counts to a matrix\n",
    "\n",
    "Python dicts are nice for inspection, but we eventually want to work with **tensors**.\n",
    "\n",
    "We’ll:\n",
    "\n",
    "1. Build a **vocabulary** of characters.\n",
    "2. Assign each character an **index**.\n",
    "3. Build a **27×27 matrix** `N` of bigram counts:\n",
    "   - 27 = 26 lowercase letters + 1 special symbol `'.'` that we’ll use as **both** start and end.\n",
    "\n",
    "We will:\n",
    "\n",
    "- Use `'.'` to represent both the beginning and the end of a word.\n",
    "- Map characters to integers with a dict `stoi` (\"string to index\").\n",
    "- Map indices back to characters with `itos` (\"index to string\").\n",
    "- Fill `N[ix1, ix2]` with the count of how often character with index `ix2` follows `ix1`.\n",
    "\n",
    "---\n",
    "### Why bother with a 27×27 matrix?\n",
    "\n",
    "You can think of `N` as a **big 2D table**:\n",
    "\n",
    "- Rows = “current character”\n",
    "- Columns = “next character”\n",
    "- Entry `N[i, j]` = “how many times did char `j` follow char `i`?”\n",
    "\n",
    "So if we order the characters as:\n",
    "\n",
    "```text\n",
    ". a b c d ... z\n",
    "```\n",
    "\n",
    "then `N[stoi['a'], stoi['n']]` is the number of `\"an\"` bigrams in the dataset.\n",
    "\n",
    "Neural networks like working with **tensors** (multi‑dimensional arrays), not Python dicts.  \n",
    "By moving to a matrix:\n",
    "\n",
    "- We can normalise all rows in one go.\n",
    "- We can visualise the whole thing as an image.\n",
    "- Later we’ll literally treat this matrix as the **weights** of a neural network.\n",
    "\n",
    "The `stoi` / `itos` pair is just a **translator** between:\n",
    "\n",
    "- human‑friendly world: `'a'`, `'b'`, `'.'`\n",
    "- tensor‑friendly world: `0, 1, 2, ...`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cdc840",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 3 – Character vocabulary and bigram count matrix `N`\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Build the set of characters present in `words` (excluding the start/end marker for now).\n",
    "2. Sort them and store in `chars`.\n",
    "3. Build a dict `stoi`:\n",
    "   - assign index `0` to `'.'`,\n",
    "   - assign indices `1..` to all characters in `chars`.\n",
    "4. Build the reverse dict `itos` such that `itos[ix]` returns the character.\n",
    "5. Create a tensor `N` of shape `(27, 27)` (dtype `torch.int32`) filled with zeros.\n",
    "6. Loop over all words and all bigrams using `'.'` as start/end and increment the corresponding `N[ix1, ix2]`.\n",
    "\n",
    "Afterwards, print:\n",
    "\n",
    "- `chars`\n",
    "- `stoi`\n",
    "- `N.shape` and `N[0]` (counts of characters that follow `'.'`).\n",
    "\n",
    "---\n",
    "### Solution discussion\n",
    "\n",
    "Steps you should see in your code:\n",
    "\n",
    "1. **Build the character set**\n",
    "\n",
    "   ```python\n",
    "   chars = sorted(list(set(''.join(words))))\n",
    "   ```\n",
    "\n",
    "   - `''.join(words)` glues all names into one long string.\n",
    "   - `set(...)` throws away duplicates.\n",
    "   - `sorted(...)` gives you `'a'` to `'z'` in order.\n",
    "\n",
    "2. **Index the characters**\n",
    "\n",
    "   ```python\n",
    "   stoi = {s: i + 1 for i, s in enumerate(chars)}\n",
    "   stoi['.'] = 0\n",
    "   itos = {i: s for s, i in stoi.items()}\n",
    "   ```\n",
    "\n",
    "   - We reserve index `0` for `'.'` (start/end),\n",
    "   - Letters get indices `1..26`.\n",
    "\n",
    "3. **Fill the matrix**\n",
    "\n",
    "   For each bigram `(ch1, ch2)`, you do:\n",
    "\n",
    "   ```python\n",
    "   ix1 = stoi[ch1]\n",
    "   ix2 = stoi[ch2]\n",
    "   N[ix1, ix2] += 1\n",
    "   ```\n",
    "\n",
    "   After the loop, `N` contains exactly the same information as our dictionary from Exercise 2, just in a dense, GPU‑friendly format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223814de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 3 – your turn: vocabulary and bigram count matrix\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 3: build chars, stoi, itos, and N\")\n",
    "\n",
    "# Hints:\n",
    "# - chars = sorted(list(set(''.join(words))))\n",
    "# - stoi = {s: i+1 for i, s in enumerate(chars)}; then set stoi['.'] = 0\n",
    "# - itos = {i: s for s, i in stoi.items()}\n",
    "# - N = torch.zeros((27, 27), dtype=torch.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ea3547",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 3 – vocabulary and bigram count matrix\n",
    "\n",
    "# 1. Character vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i + 1 for i, s in enumerate(chars)}  # 1..26\n",
    "stoi['.'] = 0                                   # 0 is the special start/end\n",
    "itos = {i: s for s, i in stoi.items()}\n",
    "\n",
    "print(\"chars:\", chars)\n",
    "print(\"stoi:\", stoi)\n",
    "\n",
    "# 2. Bigram count matrix\n",
    "N = torch.zeros((27, 27), dtype=torch.int32)\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        N[ix1, ix2] += 1\n",
    "\n",
    "print(\"N.shape:\", N.shape)\n",
    "print(\"Counts of characters following '.':\")\n",
    "print(N[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de3eb92",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 4 – Visualizing bigram counts\n",
    "\n",
    "The matrix `N` tells us, for each character on the **rows**, how often each character on the\n",
    "**columns** followed it in the dataset.\n",
    "\n",
    "A nice way to understand these counts is to **visualize** the matrix as an image.\n",
    "\n",
    "We’ll use `matplotlib` to plot `N` as a heatmap and overlay:\n",
    "\n",
    "- the bigram string `itos[i] + itos[j]`,\n",
    "- the count `N[i, j]`.\n",
    "\n",
    "This lets us see which transitions are very common and which almost never happen.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Use `plt.imshow(N, cmap='Blues')` to show the matrix.\n",
    "2. For each `i, j` in `0..26`, write:\n",
    "   - the bigram string near the top of the cell in gray,\n",
    "   - the count near the bottom.\n",
    "3. Turn off axes for a clean image.\n",
    "\n",
    "---\n",
    "### Reading the heatmap\n",
    "\n",
    "Each **cell** in the image corresponds to one entry `N[i, j]`:\n",
    "\n",
    "- The **background colour** (from light to dark blue) shows how big the count is.\n",
    "- The **text label** shows:\n",
    "  - the bigram itself, e.g. `th` or `a.`\n",
    "  - the exact count, e.g. `4321`.\n",
    "\n",
    "Typical things you’ll notice:\n",
    "\n",
    "- Some columns or rows are almost all dark: very common letters.\n",
    "- Some pairs basically never occur in English‑like names (e.g. `qz`).\n",
    "- The row for `'.'` shows which letters commonly **start** names.\n",
    "- The column for `'.'` shows which letters commonly **end** names.\n",
    "\n",
    "This kind of visual intuition is super helpful later when we compare to the neural model:  \n",
    "the neural network is trying to learn a matrix whose heatmap “rhymes” with this one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a6ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 4 – your turn: visualize N\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 4: visualize bigram count matrix N\")\n",
    "\n",
    "# Hints:\n",
    "# - Use nested for loops: for i in range(27): for j in range(27):\n",
    "# - chstr = itos[i] + itos[j]\n",
    "# - Use plt.text(j, i, ...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583c028e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 4 – visualize bigram count matrix\n",
    "\n",
    "plt.figure(figsize=(16, 16))\n",
    "plt.imshow(N, cmap='Blues')\n",
    "\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = itos[i] + itos[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b559afa8",
   "metadata": {},
   "source": [
    "\n",
    "## From counts to probabilities\n",
    "\n",
    "Right now `N[i, j]` is a **count**: how many times did character `j` follow character `i`?\n",
    "\n",
    "We want to interpret each row as a **probability distribution** over the next character. For row `i`:\n",
    "\n",
    "\\[\n",
    "P(i \\to j) = \\frac{N_{ij}}{\\sum_k N_{ik}}\n",
    "\\]\n",
    "\n",
    "So:\n",
    "\n",
    "- Take row `i` as a vector of counts.\n",
    "- Convert to float.\n",
    "- Divide by the sum of the row.\n",
    "\n",
    "This gives a vector `p` that sums to 1: a valid discrete probability distribution.\n",
    "\n",
    "We’ll first play with a single row, then turn the whole matrix into a probability matrix `P`.\n",
    "\n",
    "---\n",
    "### Tiny numeric example\n",
    "\n",
    "Suppose one row of `N` (say, for the character `'a'`) looks like:\n",
    "\n",
    "```text\n",
    "[ 0, 10, 5 ]   # pretend there are only 3 possible next characters\n",
    "```\n",
    "\n",
    "- Total count = `10 + 5 = 15`.\n",
    "- Probabilities:\n",
    "\n",
    "  ```text\n",
    "  [ 0/15, 10/15, 5/15 ] = [0.0, 0.666..., 0.333...]\n",
    "  ```\n",
    "\n",
    "Interpretation:\n",
    "\n",
    "- After `'a'`, the first character never appears.\n",
    "- The second character appears about **two‑thirds** of the time.\n",
    "- The third character appears about **one‑third** of the time.\n",
    "\n",
    "We’ll do this normalisation for **every row** of `N`, turning raw frequencies into proper probability distributions that always sum to 1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a36a4f",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 5a – Single‑row probabilities and sampling\n",
    "\n",
    "Let’s look at the row for the start/end symbol `'.'` (index 0).  \n",
    "This row tells us: \"How likely is each character as the **first** character of a name?\"\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Take `N[0]`, convert to float (`.float()`), and normalise it to sum to 1.\n",
    "2. Verify that `p.sum()` is 1 (or very close, floating‑point wise).\n",
    "3. Using a fixed random generator `g`, sample indices from this distribution with\n",
    "   `torch.multinomial` and map them back to characters using `itos`.\n",
    "\n",
    "---\n",
    "### Solution discussion\n",
    "\n",
    "For the start/end symbol `'.'` (index `0`):\n",
    "\n",
    "1. `p = N[0].float()`  \n",
    "   copies the first row and converts it from integers (counts) to floats.\n",
    "\n",
    "2. `p = p / p.sum()`  \n",
    "   divides each entry by the **row sum**, so that `p` now sums to 1.\n",
    "\n",
    "   This is just the formula:\n",
    "\n",
    "   \\[\n",
    "   p_j = \\frac{N_{0j}}{\\sum_k N_{0k}}\n",
    "   \\]\n",
    "\n",
    "3. `torch.multinomial(p, num_samples=..., replacement=True)`  \n",
    "\n",
    "   - Think of `multinomial` as “roll a weighted 27‑sided dice”.\n",
    "   - It returns indices where index `j` is drawn with probability `p[j]`.\n",
    "\n",
    "   Using `itos[ix]` turns those indices back into actual characters.\n",
    "\n",
    "If you print a bunch of samples you should see that *more common starting letters in the dataset* (like `a`, `m`, `k`, etc.) appear more often in your drawn characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f91a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 5a – your turn: one row -> probabilities and sampling\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 5a: compute p from N[0] and sample characters\")\n",
    "\n",
    "# Hints:\n",
    "# - p = N[0].float()\n",
    "# - p = p / p.sum()\n",
    "# - g = torch.Generator().manual_seed(2147483647)\n",
    "# - torch.multinomial(p, num_samples=10, replacement=True, generator=g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088b7c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 5a – one row -> probabilities and sampling\n",
    "\n",
    "# row for '.'\n",
    "p = N[0].float()\n",
    "p = p / p.sum()\n",
    "print(\"p:\", p)\n",
    "print(\"sum of p:\", p.sum().item())\n",
    "\n",
    "# sample some starting characters\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "sampled_ix = torch.multinomial(p, num_samples=20, replacement=True, generator=g)\n",
    "print(\"Sampled indices:\", sampled_ix.tolist())\n",
    "print(\"Sampled chars:  \", ''.join(itos[ix.item()] for ix in sampled_ix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf44947",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 5b – Full probability matrix `P` and sampling new names\n",
    "\n",
    "We want to turn the whole `N` into a probability matrix `P` where **each row** sums to 1.\n",
    "\n",
    "We’ll also add **smoothing** (add 1 to every count) so that we never get zero probabilities\n",
    "for transitions that just didn’t happen in our training data.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Create `P = (N + 1).float()`   (add‑one smoothing).\n",
    "2. Divide each row by its row sum: `P /= P.sum(1, keepdims=True)`.\n",
    "\n",
    "Now `P[ix]` is the probability distribution of the next character, given the current character index `ix`.\n",
    "\n",
    "### Sampling names\n",
    "\n",
    "We can now **sample names** from this bigram model:\n",
    "\n",
    "1. Start from `ix = 0` (for `'.'`).\n",
    "2. Sample the next index from `P[ix]` with `torch.multinomial`.\n",
    "3. Convert the index to a character with `itos[ix]` and append it to an output list.\n",
    "4. Repeat from step 2, using the new `ix`, until you sample `ix == 0` again (which means end of name).\n",
    "5. Join the characters to form a sampled name.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Build the smoothed probability matrix `P` from `N`.\n",
    "2. Use `P` to sample and print 10 random names.\n",
    "\n",
    "---\n",
    "### Why add 1 everywhere? (smoothing)\n",
    "\n",
    "If a bigram **never** appears in the training data, its raw count is 0, so its probability would also be 0.\n",
    "\n",
    "That causes two problems:\n",
    "\n",
    "- The model will say some perfectly reasonable names have *exactly* zero probability just because we never saw them.\n",
    "- In log space, `log(0)` is `-∞`, which explodes our loss.\n",
    "\n",
    "Adding 1 to **every entry**:\n",
    "\n",
    "```python\n",
    "P = (N + 1).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "```\n",
    "\n",
    "is called **add‑one smoothing** (also “Laplace smoothing”):\n",
    "\n",
    "- It’s as if we pretended we saw every possible bigram **once**.\n",
    "- Rare or unseen bigrams get a small, but nonzero, probability.\n",
    "\n",
    "---\n",
    "\n",
    "### Sampling names again\n",
    "\n",
    "The sampling loop is the same idea as Exercise 5a, but repeated:\n",
    "\n",
    "```text\n",
    ". → (sample) → first letter\n",
    "(first letter) → (sample) → second letter\n",
    "(second letter) → (sample) → third letter\n",
    "...\n",
    "until we sample '.' again\n",
    "```\n",
    "\n",
    "Each step only looks at **the current character** and uses the corresponding row of `P` as the distribution over the next one.\n",
    "\n",
    "If your code is right, the sampled names should “feel” more name‑like than pure noise, but still obviously worse than real names — that’s the limitation of bigrams.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74403dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 5b – your turn: build P and sample names\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 5b: build P and sample names\")\n",
    "\n",
    "# Hints:\n",
    "# - P = (N + 1).float()\n",
    "# - P /= P.sum(1, keepdims=True)\n",
    "# - see loop description in the text above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927667d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 5b – full probability matrix and sampling\n",
    "\n",
    "# Build smoothed probability matrix\n",
    "P = (N + 1).float()\n",
    "P /= P.sum(1, keepdims=True)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0  # start with '.'\n",
    "    while True:\n",
    "        p = P[ix]\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380639af",
   "metadata": {},
   "source": [
    "\n",
    "## Log‑likelihood of the data under the bigram model\n",
    "\n",
    "So far we’ve **built** a bigram model from data by counting, and we can **sample** from it.\n",
    "\n",
    "We’d also like a **single number** that measures *how well* the model explains the dataset.\n",
    "\n",
    "A natural choice is the **log‑likelihood**:\n",
    "\n",
    "- For each name, you can compute the probability that the model assigns to that exact name:\n",
    "  \\\n",
    "  P(w) = \\prod_{\\text{bigrams }(c_1, c_2)\\text{ in }w} P(c_2 \\mid c_1)\n",
    "  \\\n",
    "- Products of many small numbers quickly underflow, so we work with **log** probabilities:\n",
    "  \\\n",
    "  \\log P(w) = \\sum \\log P(c_2 \\mid c_1)\n",
    "  \\\n",
    "- For the entire dataset, we sum over all names:\n",
    "  \\\n",
    "  \\text{log\\_likelihood} = \\sum_{w \\in \\text{data}} \\log P(w)\n",
    "  \\\n",
    "\n",
    "Higher log‑likelihood = better model.  \n",
    "People often use **average negative log‑likelihood** as a **loss** to minimise:\n",
    "\n",
    "\\[\n",
    "\\text{loss} = - \\frac{1}{N} \\sum \\log P(c_2 \\mid c_1)\n",
    "\\]\n",
    "\n",
    "where the sum is over all bigrams in the dataset.\n",
    "\n",
    "---\n",
    "### Why products become sums (and why we care)\n",
    "\n",
    "If a name has bigram probabilities:\n",
    "\n",
    "```text\n",
    "P = [0.4, 0.2, 0.5, 0.1]   # one for each bigram in the name\n",
    "```\n",
    "\n",
    "then\n",
    "\n",
    "- Likelihood of that name:\n",
    "  \\[\n",
    "  P(w) = 0.4 \\times 0.2 \\times 0.5 \\times 0.1\n",
    "  \\]\n",
    "- Log‑likelihood:\n",
    "  \\[\n",
    "  \\log P(w) = \\log 0.4 + \\log 0.2 + \\log 0.5 + \\log 0.1\n",
    "  \\]\n",
    "\n",
    "Taking logs turns a product of many tiny numbers into a **sum of reasonably sized negatives**, which:\n",
    "\n",
    "- is numerically safer (less underflow),\n",
    "- plays nicely with calculus (sums are easier to differentiate than long products),\n",
    "- matches what most deep learning libraries expect (losses are usually expressed as averages of per‑example terms).\n",
    "\n",
    "The average negative log‑likelihood we compute in the next exercise is exactly the same quantity that appears as the **cross‑entropy loss** in classification problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd38f40",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 6 – Log‑likelihood and average negative log‑likelihood\n",
    "\n",
    "Using the probability matrix `P` you built in Exercise 5b, compute:\n",
    "\n",
    "1. Total log‑likelihood `log_likelihood` of the entire dataset.\n",
    "2. Total number of bigrams `n` used in this computation.\n",
    "3. Average negative log‑likelihood: `nll = -log_likelihood / n`.\n",
    "\n",
    "Use `torch.log(prob)` for log probabilities.\n",
    "\n",
    "### Task\n",
    "\n",
    "Fill in the loop and print the values.\n",
    "\n",
    "---\n",
    "### Solution discussion\n",
    "\n",
    "For each bigram `(ch1, ch2)` in every word:\n",
    "\n",
    "1. Convert characters to indices: `ix1 = stoi[ch1]`, `ix2 = stoi[ch2]`.\n",
    "2. Look up the model’s probability for that transition: `prob = P[ix1, ix2]`.\n",
    "3. Accumulate the log‑probability:\n",
    "\n",
    "   ```python\n",
    "   logprob = torch.log(prob)\n",
    "   log_likelihood += logprob\n",
    "   n += 1\n",
    "   ```\n",
    "\n",
    "At the end:\n",
    "\n",
    "- `log_likelihood` is the sum of all \\(\\log P(c_2 \\mid c_1)\\) terms.\n",
    "- `n` is the total **number of bigrams** we saw.\n",
    "- The average negative log‑likelihood\n",
    "\n",
    "  ```python\n",
    "  nll = -log_likelihood / n\n",
    "  ```\n",
    "\n",
    "  is our scalar “how bad is the model?” number.\n",
    "\n",
    "If you compare this `nll` to what you later get from the neural network model, they should be very close — that’s how we know the neural network succeeded in re‑learning the same bigram statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d7d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 6 – your turn: log-likelihood\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 6: compute log_likelihood and average NLL\")\n",
    "\n",
    "# Hints:\n",
    "# - Outer loop over all words\n",
    "# - Inner loop over bigrams of ['.'] + list(w) + ['.']\n",
    "# - Use P[ix1, ix2] to get probability\n",
    "# - Accumulate log_likelihood and n (count of bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1df502",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 6 – log-likelihood and average NLL\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "\n",
    "print(f\"log_likelihood = {log_likelihood.item()}\")\n",
    "nll = -log_likelihood\n",
    "print(f\"total negative log-likelihood = {nll.item()}\")\n",
    "print(f\"average NLL per bigram        = {nll.item() / n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f6c1e9",
   "metadata": {},
   "source": [
    "\n",
    "## From counts to a tiny neural network\n",
    "\n",
    "The model we built using counts and normalisation can be viewed as a **neural network** with:\n",
    "\n",
    "- Input: the index of the current character (an integer 0..26).\n",
    "- Representation: a **one‑hot vector** of length 27 (all zeros except a 1 at the current character).\n",
    "- Parameters: a matrix `W` of shape `(27, 27)`.\n",
    "- Output: probabilities for the next character.\n",
    "\n",
    "Forward pass idea:\n",
    "\n",
    "1. Convert input indices (shape `[num_examples]`) to one‑hot encodings (shape `[num_examples, 27]`).\n",
    "2. Multiply by `W` to get **logits**: `logits = xenc @ W`.\n",
    "3. Turn logits into (unnormalised) positive numbers by exponentiating: `counts = logits.exp()`.\n",
    "4. Normalise row‑wise to get probabilities: `probs = counts / counts.sum(1, keepdims=True)`.\n",
    "\n",
    "This `probs` has the same shape as `P`, but now it comes from **parameters** in `W` instead of counts.\n",
    "We can now **learn** `W` by gradient descent instead of computing it directly from counts.\n",
    "\n",
    "---\n",
    "### Connecting the two views\n",
    "\n",
    "You can literally think of the count‑based bigram table as a **neural network layer**:\n",
    "\n",
    "- One‑hot input of length 27:\n",
    "  - row vector: `[0, 0, 0, 1, 0, ..., 0]` means “current character is index 3”.\n",
    "- Weight matrix `W` of shape `(27, 27)`:\n",
    "  - row `i` contains 27 numbers controlling the scores for all possible next characters given current character `i`.\n",
    "\n",
    "Then:\n",
    "\n",
    "```python\n",
    "xenc @ W      # pick out the row corresponding to the 1 in xenc\n",
    "```\n",
    "\n",
    "is just a fancy way of saying “look up the row for the current character”.\n",
    "\n",
    "The only difference from the pure‑count model is:\n",
    "\n",
    "- before, the entries of the matrix were **fixed counts**;\n",
    "- now, the entries of `W` are **trainable parameters** we will adjust to make the log‑likelihood as high as possible.\n",
    "\n",
    "This is a very gentle first example of how probabilistic models and neural networks are just two ways to talk about the **same underlying math**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aeae08",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 7 – Building a small (x, y) training set\n",
    "\n",
    "To train a neural network, we want input–label pairs `(x, y)` where:\n",
    "\n",
    "- `x` is the index of the current character,\n",
    "- `y` is the index of the next character.\n",
    "\n",
    "For a word `w`, we again look at `chs = ['.'] + list(w) + ['.']` and record all bigrams.\n",
    "\n",
    "To understand shapes, let’s start with just **one word**.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. For the *first* word in `words` only, build two Python lists `xs` and `ys`.\n",
    "2. For each bigram `(ch1, ch2)`:\n",
    "   - append `stoi[ch1]` to `xs`,\n",
    "   - append `stoi[ch2]` to `ys`.\n",
    "3. Convert `xs` and `ys` to integer tensors with `torch.tensor(...)`.\n",
    "4. Print `xs`, `ys` and their shapes.\n",
    "\n",
    "---\n",
    "### Solution discussion\n",
    "\n",
    "For a concrete example, suppose the first word is `\"emma\"` and we add `'.'` on both sides:\n",
    "\n",
    "```text\n",
    "['.', 'e', 'm', 'm', 'a', '.']\n",
    "```\n",
    "\n",
    "The bigrams are:\n",
    "\n",
    "```text\n",
    "('.', 'e'),\n",
    "('e', 'm'),\n",
    "('m', 'm'),\n",
    "('m', 'a'),\n",
    "('a', '.')\n",
    "```\n",
    "\n",
    "We now replace characters with their indices using `stoi`:\n",
    "\n",
    "```python\n",
    "xs = [stoi['.'], stoi['e'], stoi['m'], stoi['m'], stoi['a']]\n",
    "ys = [stoi['e'], stoi['m'], stoi['m'], stoi['a'], stoi['.']]\n",
    "```\n",
    "\n",
    "After converting to tensors:\n",
    "\n",
    "```python\n",
    "xs = tensor([...])\n",
    "ys = tensor([...])\n",
    "```\n",
    "\n",
    "each position `i` in `xs` / `ys` is one supervised training example:\n",
    "\n",
    "- input `xs[i]` → “current character index”\n",
    "- label `ys[i]` → “true next character index”\n",
    "\n",
    "The rest of the notebook just scales this idea up from **one word** to **all words**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bf4645",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 7 – your turn: small training set for one word\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 7: build xs and ys for first word\")\n",
    "\n",
    "# Hints:\n",
    "# - w = words[0]\n",
    "# - chs = ['.'] + list(w) + ['.']\n",
    "# - use zip(chs, chs[1:]) as before\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dfffc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 7 – small training set for one word\n",
    "\n",
    "xs, ys = [], []\n",
    "\n",
    "w = words[0]\n",
    "print(\"First word:\", w)\n",
    "chs = ['.'] + list(w) + ['.']\n",
    "for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    print(ch1, ch2)\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "print(\"xs:\", xs)\n",
    "print(\"ys:\", ys)\n",
    "print(\"xs.shape:\", xs.shape)\n",
    "print(\"ys.shape:\", ys.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81385537",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 8 – One‑hot encoding and visualisation\n",
    "\n",
    "Neural networks usually take **vectors** as input, not integer indices.  \n",
    "We’ll convert indices `xs` into a 2D tensor of one‑hot vectors using:\n",
    "\n",
    "```python\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "```\n",
    "\n",
    "- Shape will be `[num_examples, 27]`.\n",
    "- Each row corresponds to one bigram’s first character.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Use `torch.nn.functional.one_hot` to compute `xenc` from `xs` with `num_classes=27`.\n",
    "2. Check its shape and dtype.\n",
    "3. Visualise it with `plt.imshow(xenc)` (you should see a pattern of 1s along columns).\n",
    "\n",
    "---\n",
    "### Solution discussion\n",
    "\n",
    "`F.one_hot(xs, num_classes=27)` turns something like:\n",
    "\n",
    "```python\n",
    "xs = tensor([0, 5, 13, 13, 1])\n",
    "```\n",
    "\n",
    "into a 2D tensor:\n",
    "\n",
    "```text\n",
    "[[1, 0, 0, ..., 0],   # index 0\n",
    " [0, 0, 0, ..., 0],   # index 5 has a 1 somewhere in the middle\n",
    " ...\n",
    "]\n",
    "```\n",
    "\n",
    "Key points:\n",
    "\n",
    "- Shape is `(num_examples, 27)`.\n",
    "- Every row has exactly **one** 1 and the rest are 0s → hence “one‑hot”.\n",
    "- Casting to `.float()` gives the `float32` type that neural nets expect.\n",
    "\n",
    "When you plot it with `plt.imshow(xenc)`, each row shows a single bright pixel at the column corresponding to the input character. Later, multiplying this by `W` effectively **selects a row** of `W` for each example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225bb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 8 – your turn: one-hot encoding and visualisation\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 8: compute xenc and visualise\")\n",
    "\n",
    "# Hints:\n",
    "# - xenc = F.one_hot(xs, num_classes=27).float()\n",
    "# - xenc.shape\n",
    "# - plt.imshow(xenc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09616acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 8 – one-hot encoding and visualisation\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "print(\"xenc.shape:\", xenc.shape)\n",
    "print(\"xenc.dtype:\", xenc.dtype)\n",
    "\n",
    "plt.imshow(xenc)\n",
    "plt.title(\"One-hot encoding of first word's bigrams\")\n",
    "plt.xlabel(\"character index\")\n",
    "plt.ylabel(\"bigram example index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e04632f",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 9 – A tiny linear bigram model on a few examples\n",
    "\n",
    "We’ll now treat the bigram model as a tiny one‑layer neural net:\n",
    "\n",
    "- Input: one‑hot vectors `xenc` (shape `[num_examples, 27]`).\n",
    "- Parameters: weight matrix `W` (shape `[27, 27]`).\n",
    "- Output: `probs` (shape `[num_examples, 27]`) — probability distribution over next character.\n",
    "\n",
    "Forward pass:\n",
    "\n",
    "```python\n",
    "W = torch.randn((27, 27))  # random initial weights\n",
    "logits = xenc @ W          # shape: (num_examples, 27)\n",
    "counts = logits.exp()      # positive\n",
    "probs = counts / counts.sum(1, keepdims=True)  # row-wise normalisation\n",
    "```\n",
    "\n",
    "For now we’ll work with the small `xs, ys` from the first word only.\n",
    "\n",
    "We’ll define the **loss** as the average negative log probability of the correct character:\n",
    "\n",
    "```python\n",
    "loss = -probs[torch.arange(num_examples), ys].log().mean()\n",
    "```\n",
    "\n",
    "This is the same as the average negative log‑likelihood we computed earlier, but now for a neural net.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Initialise a random weight matrix `W` of shape `(27, 27)` with a fixed random seed.\n",
    "2. Compute `xenc`, `logits`, `counts`, and `probs` as above.\n",
    "3. Compute and print the loss.\n",
    "\n",
    "---\n",
    "### Solution discussion\n",
    "\n",
    "We’re now building the tiniest possible neural bigram model:\n",
    "\n",
    "1. **Random weights**\n",
    "\n",
    "   ```python\n",
    "   g = torch.Generator().manual_seed(2147483647)\n",
    "   W = torch.randn((27, 27), generator=g)\n",
    "   ```\n",
    "\n",
    "   - Each row in `W` will *eventually* behave like one row of the bigram table.\n",
    "   - Right now they’re random, so predictions are bad.\n",
    "\n",
    "2. **Forward pass**\n",
    "\n",
    "   ```python\n",
    "   xenc = F.one_hot(xs, num_classes=27).float()\n",
    "   logits = xenc @ W          # shape: (num_examples, 27)\n",
    "   counts = logits.exp()\n",
    "   probs = counts / counts.sum(1, keepdims=True)\n",
    "   ```\n",
    "\n",
    "   - `logits` are arbitrary real numbers.\n",
    "   - `exp` makes them positive (“fake counts”).\n",
    "   - Dividing by row sums turns them into probabilities (“softmax”).\n",
    "\n",
    "3. **Loss**\n",
    "\n",
    "   ```python\n",
    "   loss = -probs[torch.arange(num), ys].log().mean()\n",
    "   ```\n",
    "\n",
    "   This is exactly the average negative log‑likelihood, but now the probabilities come from `W` instead of the count matrix.\n",
    "\n",
    "If you print the loss, it should be **a bit larger** than the loss of the count‑based model (because random `W` is very bad). Training in the next exercises will drive this loss down.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e70bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 9 – your turn: tiny linear model on a few bigrams\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 9: build W, forward pass, and loss\")\n",
    "\n",
    "# Hints:\n",
    "# - g = torch.Generator().manual_seed(2147483647)\n",
    "# - W = torch.randn((27, 27), generator=g)\n",
    "# - num = xs.nelement()\n",
    "# - loss = -probs[torch.arange(num), ys].log().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d9975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 9 – tiny linear model on a few bigrams\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g)\n",
    "\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xenc @ W           # shape: (num_examples, 27)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "num = xs.nelement()\n",
    "loss = -probs[torch.arange(num), ys].log().mean()\n",
    "\n",
    "print(\"probs.shape:\", probs.shape)\n",
    "print(\"loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b6ed67",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 10 – One training step with autograd\n",
    "\n",
    "So far we just computed the loss for a randomly initialised `W`.  \n",
    "Now let’s **train** `W` a bit using **PyTorch autograd**.\n",
    "\n",
    "Steps for *one* gradient descent step:\n",
    "\n",
    "1. Create `W` with `requires_grad=True` so PyTorch tracks its gradients.\n",
    "2. Forward pass to compute `loss` as before.\n",
    "3. Zero the gradient: `W.grad = None`.\n",
    "4. Backward pass: `loss.backward()` to fill `W.grad` with the gradient of `loss` w.r.t. `W`.\n",
    "5. Update: `W.data += -learning_rate * W.grad`.\n",
    "\n",
    "### Task\n",
    "\n",
    "Implement one training step on the small `(xs, ys)` you built earlier and print:\n",
    "\n",
    "- the loss before the update,\n",
    "- the loss after the update (recompute the forward pass after updating `W`).\n",
    "\n",
    "---\n",
    "### Solution discussion\n",
    "\n",
    "This is your first full **gradient descent step** in PyTorch:\n",
    "\n",
    "1. Create `W` with `requires_grad=True` so PyTorch tracks how the loss depends on each element of `W`.\n",
    "2. Do the forward pass to compute `loss`.\n",
    "3. Clear old gradients with `W.grad = None`.\n",
    "4. Call `loss.backward()`:\n",
    "   - PyTorch walks the computation graph *backwards*,\n",
    "   - fills `W.grad` with \\(\\frac{\\partial \\text{loss}}{\\partial W_{ij}}\\) for every entry \\(W_{ij}\\).\n",
    "\n",
    "5. Update:\n",
    "\n",
    "   ```python\n",
    "   W.data += -lr * W.grad\n",
    "   ```\n",
    "\n",
    "   - If a gradient entry is positive, we subtract a bit → decreasing that weight lowers the loss.\n",
    "   - If it’s negative, subtracting makes the weight bigger.\n",
    "\n",
    "When you recompute `loss` after the update, it should be **slightly smaller**, showing that we moved in a good direction in parameter space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09c4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 10 – your turn: one autograd training step\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 10: one training step on small dataset\")\n",
    "\n",
    "# Hints:\n",
    "# - Use a fresh W with requires_grad=True\n",
    "# - Use a learning rate like 0.1 or 1.0 (just to see the effect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d392f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 10 – one autograd training step\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "def forward(xs, ys):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    num = xs.nelement()\n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "    return loss\n",
    "\n",
    "# Loss before update\n",
    "loss_before = forward(xs, ys)\n",
    "print(\"loss before:\", loss_before.item())\n",
    "\n",
    "# Backprop and update\n",
    "W.grad = None\n",
    "loss_before.backward()\n",
    "lr = 1.0\n",
    "W.data += -lr * W.grad\n",
    "\n",
    "# Loss after update\n",
    "loss_after = forward(xs, ys)\n",
    "print(\"loss after :\", loss_after.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4269776",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 11 – Training on the full dataset\n",
    "\n",
    "Now we scale up from the first word to **all** words.\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Build bigram dataset `(xs, ys)` for **all** names:\n",
    "   - Loop over all words,\n",
    "   - For each, loop over bigrams of `['.'] + list(w) + ['.']`,\n",
    "   - Append `stoi[ch1]` to `xs`, `stoi[ch2]` to `ys`.\n",
    "2. Convert `xs` and `ys` to tensors.\n",
    "3. Create weight matrix `W` with `requires_grad=True`.\n",
    "4. Run a training loop for some number of steps:\n",
    "   - Forward pass on all examples.\n",
    "   - Compute loss as average negative log‑likelihood plus a small regularisation term, e.g. `0.01*(W**2).mean()` to discourage very large weights.\n",
    "   - Zero gradients, backprop (`loss.backward()`), and update `W` with a learning rate.\n",
    "\n",
    "We will not worry about efficiency or batching here – just a clear, simple implementation.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Build the full `(xs, ys)` dataset.\n",
    "2. Implement a training loop for, say, 50 steps and print the loss every few steps.\n",
    "\n",
    "---\n",
    "### Solution discussion\n",
    "\n",
    "Conceptually we just scaled up what you already did for a single word:\n",
    "\n",
    "1. **Build the big dataset**\n",
    "\n",
    "   - Loop over *all* words.\n",
    "   - For each name, add all its bigrams to `xs` / `ys`.\n",
    "   - You should end up with tens of thousands of examples.\n",
    "\n",
    "2. **Single big forward pass**\n",
    "\n",
    "   ```python\n",
    "   xenc = F.one_hot(xs, num_classes=27).float()\n",
    "   logits = xenc @ W\n",
    "   counts = logits.exp()\n",
    "   probs = counts / counts.sum(1, keepdims=True)\n",
    "   ```\n",
    "\n",
    "3. **Loss with regularisation**\n",
    "\n",
    "   ```python\n",
    "   loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "   ```\n",
    "\n",
    "   - First term: average negative log‑likelihood (fit the data).\n",
    "   - Second term: **weight decay** / L2 regularisation (keep weights small, avoid over‑confident weird spikes).\n",
    "\n",
    "4. **Training loop**\n",
    "\n",
    "   On each iteration:\n",
    "\n",
    "   - Zero `W.grad`,\n",
    "   - `loss.backward()`,\n",
    "   - `W.data += -lr * W.grad`.\n",
    "\n",
    "With a reasonably large learning rate (e.g. `50.0`) and ~50 steps, the loss should settle near the value you got from the pure counting model, showing that the neural network has **re‑discovered** the same bigram probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e000c2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 11 – your turn: full dataset and training loop\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 11: build full dataset and train W\")\n",
    "\n",
    "# Hints:\n",
    "# - Follow the steps described in the markdown above.\n",
    "# - Use a learning rate like 50.0 (as in the original notebook) so loss goes down quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 11 – full dataset and training loop\n",
    "\n",
    "# 1. Build the dataset\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(\"number of bigram examples:\", num)\n",
    "\n",
    "# 2. Initialise the network\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27, 27), generator=g, requires_grad=True)\n",
    "\n",
    "# 3. Training loop\n",
    "for k in range(50):\n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()  # one-hot inputs\n",
    "    logits = xenc @ W                             # predict log-counts\n",
    "    counts = logits.exp()                         # counts (unnormalised)\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "\n",
    "    # loss: negative log likelihood + small weight decay\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.01 * (W**2).mean()\n",
    "\n",
    "    # backward pass\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    lr = 50.0\n",
    "    W.data += -lr * W.grad\n",
    "\n",
    "    if k % 10 == 0 or k == 49:\n",
    "        print(f\"step {k:03d}  loss = {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae75fb2",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 12 – Sampling names from the trained neural model\n",
    "\n",
    "After training, the learned weight matrix `W` defines a probability distribution over next characters,\n",
    "very similar to our count‑based `P`, but now **learned** via gradient descent.\n",
    "\n",
    "To sample names from the neural net model:\n",
    "\n",
    "1. Start with `ix = 0` (for `'.'`).\n",
    "2. For each step:\n",
    "   - Build a one‑hot encoding for index `ix`.\n",
    "   - Compute logits with `logits = xenc @ W`.\n",
    "   - Turn logits into counts with `counts = logits.exp()`.\n",
    "   - Normalise counts into probabilities `p = counts / counts.sum(1, keepdims=True)`.\n",
    "   - Sample `ix` from `p` with `torch.multinomial`.\n",
    "   - Append the character `itos[ix]` to the output list.\n",
    "   - Stop when `ix == 0` again (end of word).\n",
    "\n",
    "### Task\n",
    "\n",
    "Using the trained `W` (from Exercise 11), sample and print 10 random names from the neural bigram model.\n",
    "\n",
    "---\n",
    "### Solution discussion\n",
    "\n",
    "Sampling from the trained neural net mirrors what we did with the count‑based matrix:\n",
    "\n",
    "For each new name:\n",
    "\n",
    "1. Start with index `ix = 0` (the `'.'` start symbol).\n",
    "2. Turn `ix` into a one‑hot vector and run it through the network:\n",
    "\n",
    "   ```python\n",
    "   xenc   = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "   logits = xenc @ W\n",
    "   counts = logits.exp()\n",
    "   p      = counts / counts.sum(1, keepdims=True)\n",
    "   ```\n",
    "\n",
    "3. Use `torch.multinomial(p, ...)` to sample the next index.\n",
    "4. Append the character and repeat from step 2 with the new `ix`.\n",
    "5. Stop when you sample `ix == 0` again (end of name).\n",
    "\n",
    "Because `W` was trained to match the bigram statistics of the dataset, the names you get now should look **very similar** to the ones from the count‑based model — that’s a good sanity check that your training loop worked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ee92e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 12 – your turn: sampling from the neural net model\n",
    "\n",
    "### YOUR CODE HERE\n",
    "raise NotImplementedError(\"Exercise 12: sample names using trained W\")\n",
    "\n",
    "# Hints:\n",
    "# - Use the loop described above.\n",
    "# - Use a fixed generator seed for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d29482",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 12 – sampling from the neural net model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0  # start from '.'\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        p = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09817763",
   "metadata": {},
   "source": [
    "\n",
    "## Wrap‑up\n",
    "\n",
    "In this notebook you:\n",
    "\n",
    "- Explored a dataset of baby names.\n",
    "- Built a **bigram language model** by counting character pairs.\n",
    "- Converted counts to probabilities and **sampled new names**.\n",
    "- Measured model quality with **(average) negative log‑likelihood**.\n",
    "- Reinterpreted the bigram model as a tiny **neural network**:\n",
    "  - one‑hot inputs,\n",
    "  - weight matrix `W`,\n",
    "  - softmax outputs.\n",
    "- Trained `W` with **gradient descent** using PyTorch autograd.\n",
    "- Sampled new names from the **trained neural net**.\n",
    "\n",
    "This is the conceptual foundation for much more powerful models:\n",
    "\n",
    "- Replace \"previous single character\" with \"previous N characters\" (n‑grams).\n",
    "- Replace one linear layer with deeper and more expressive networks.\n",
    "- Replace characters with words, tokens, etc.\n",
    "\n",
    "But the core ideas stay the same:\n",
    "\n",
    "> Model defines probabilities for the next symbol →  \n",
    "> We train parameters to make the training data **likely** under the model →  \n",
    "> We can **sample** from the model to generate new, similar data.\n",
    "\n",
    "From here, you can experiment with:\n",
    "\n",
    "- Changing the vocabulary (uppercase, accent marks, etc.).\n",
    "- Modifying the architecture (different hidden sizes, multiple layers).\n",
    "- Comparing the count‑based bigram `P` and the learned `W` model quantitatively.\n",
    "\n",
    "---\n",
    "### Mental checklist of concepts you now own\n",
    "\n",
    "Before moving on, make sure you’re comfortable with:\n",
    "\n",
    "- Turning raw text into **bigrams** and counts.\n",
    "- Converting counts → probabilities → samples.\n",
    "- Computing **(average) negative log‑likelihood** as a loss.\n",
    "- Viewing a table of probabilities as a **one‑layer neural network** with:\n",
    "  - one‑hot inputs,\n",
    "  - weight matrix `W`,\n",
    "  - softmax outputs.\n",
    "- Using PyTorch’s:\n",
    "  - `F.one_hot`,\n",
    "  - matrix multiplication (`@`),\n",
    "  - `exp` and normalisation,\n",
    "  - `loss.backward()` and manual weight updates.\n",
    "\n",
    "In the next stages (multi‑layer MLPs, RNNs, and finally Transformers), only the **forward pass** gets more complicated. The training recipe — “compute logits → softmax → NLL loss → backward → update” — stays exactly the same.\n",
    "\n",
    "If you really want to solidify this:\n",
    "\n",
    "- Try changing the dataset (e.g. use a list of Pokémon names or movie titles).\n",
    "- Swap the bigram model for a **trigram** model where you condition on the previous 2 characters.\n",
    "- Compare the losses of the count‑based and learned models to make sure they match.\n",
    "\n",
    "Once you’re happy with bigrams, you’re ready for much more powerful language models.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}