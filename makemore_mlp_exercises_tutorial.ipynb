{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n", "# makemore \u2013 Part 2: MLP Character-Level Language Model (Tutorial + Exercises)\n", "\n", "In this notebook we build a **multi-layer perceptron (MLP)** character-level language model\n", "for baby names, following the second makemore video.\n", "\n", "This notebook is designed so that you can understand the **entire video and original notebook**\n", "just by working through:\n", "\n", "- Every new idea is introduced in **plain language**, with **small examples**.\n", "- Then there is an **Exercise** cell (`### YOUR CODE HERE` + `NotImplementedError(...)`).\n", "- Then a **Solution** cell with a detailed, commented implementation.\n", "- Many solutions also have a short **solution discussion** in markdown.\n", "\n", "We assume you already saw the **bigram** notebook (Part 1), but we briefly recap the key ideas.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## What we are going to build\n", "\n", "We will:\n", "\n", "1. **Load** a list of names from `names.txt`.\n", "2. Turn it into a dataset of many small examples `(context, next_char)`:\n", "   - context is a window of the previous `block_size` characters,\n", "   - next_char is the character that actually follows in the training name.\n", "3. Build a **character embedding** lookup table:\n", "   - each character gets a small learned vector.\n", "4. Build a **2-layer MLP**:\n", "   - input: concatenated embeddings for the context,\n", "   - hidden layer with `tanh` nonlinearity,\n", "   - output layer that predicts a distribution over the next character.\n", "5. Train the network with **mini-batch gradient descent** and `F.cross_entropy`.\n", "6. Measure **train / dev / test** loss and discuss under/overfitting.\n", "7. Visualise character embeddings and **sample new names**.\n", "\n", "Along the way we\u2019ll explain:\n", "\n", "- why a pure **count table** explodes when we use longer context,\n", "- how embeddings help generalisation (like in Bengio et al. 2003),\n", "- how `view`, broadcasting, and advanced indexing in PyTorch work,\n", "- why `F.cross_entropy` is preferred over a manual softmax.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 1. Setup \u2013 imports and dataset\n", "\n", "We\u2019ll use:\n", "\n", "- `torch` for tensors and autograd,\n", "- `torch.nn.functional` (imported as `F`) for things like `one_hot`, `cross_entropy`, `softmax`, \u2026\n", "- `matplotlib` just for a few plots.\n", "\n", "We also load `names.txt`, which should contain **one name per line**:\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import torch\n", "import torch.nn.functional as F\n", "import matplotlib.pyplot as plt\n", "\n", "%matplotlib inline\n", "\n", "# Load the dataset: one name per line\n", "words = open('names.txt', 'r').read().splitlines()\n", "len(words), words[:10]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## Exercise 1 \u2013 Exploring the names dataset (recap)\n", "\n", "Before diving into the MLP, let\u2019s warm up (and confirm that the file looks sane).\n", "\n", "Using the list `words`:\n", "\n", "1. Print the **number of names**.\n", "2. Print the **first 10 names**.\n", "3. Compute the **minimum** and **maximum** name length (in characters).\n", "\n", "This is similar to the Part 1 notebook, but it\u2019s good practice.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 1 \u2013 your turn: basic dataset stats\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 1: print dataset stats (count, examples, min/max length)\")\n", "\n", "# Hints:\n", "# - len(words) -> number of names\n", "# - lengths = [len(w) for w in words]\n", "# - min(lengths), max(lengths)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 1 \u2013 dataset stats\n", "\n", "print(\"Number of names:\", len(words))\n", "print(\"First 10 names:\", words[:10])\n", "\n", "lengths = [len(w) for w in words]\n", "print(\"Shortest name length:\", min(lengths))\n", "print(\"Longest  name length:\", max(lengths))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**Solution discussion:**\n", "\n", "- `len(words)` gives the number of names (rows in the file).\n", "- `words[:10]` slices the first 10 names so you can eyeball them.\n", "- We build a `lengths` list with a simple list comprehension, then use `min` and `max`.\n", "\n", "This tells us roughly how long typical names are and how wide our context window (number of previous characters) might reasonably be.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 2. Character vocabulary\n", "\n", "We work at the **character** level. We need:\n", "\n", "- a list of all characters in the dataset,\n", "- a mapping from character \u2192 integer index (`stoi`),\n", "- a mapping from index \u2192 character (`itos`),\n", "- a special character `'.'` to represent both **start** and **end** of a word.\n", "\n", "We will:\n", "\n", "- assign index **0** to `'.'`,\n", "- and indices **1..26** to `a..z`.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Build the character vocabulary and mappings\n", "\n", "chars = sorted(list(set(''.join(words))))  # all unique characters\n", "stoi = {s: i + 1 for i, s in enumerate(chars)}  # reserve 0 for '.'\n", "stoi['.'] = 0\n", "itos = {i: s for s, i in stoi.items()}\n", "\n", "vocab_size = len(stoi)\n", "print(\"chars:\", chars)\n", "print(\"vocab_size:\", vocab_size)\n", "print(\"stoi:\", stoi)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 3. From bigrams to longer context (and why we need an MLP)\n", "\n", "In the **bigram** model (Part 1), we used a 27\u00d727 table of counts/probabilities:\n", "\n", "- rows: previous character,\n", "- columns: next character.\n", "\n", "This worked but was weak: it only looks at **one** previous character.\n", "\n", "If we tried to extend this table to longer context:\n", "\n", "- 1-char context: 27 possible histories \u2192 27 rows.\n", "- 2-char context: 27\u00b2 = 729 rows.\n", "- 3-char context: 27\u00b3 = 19\u202f683 rows.\n", "- 4-char context: 27\u2074 = 531\u202f441 rows.\n", "\n", "Two problems:\n", "\n", "1. The table grows **exponentially** with context length.\n", "2. Many rows will be almost empty; we won\u2019t have enough data to estimate them.\n", "\n", "Instead, Bengio et al. (2003) proposed:\n", "\n", "- give each symbol (word in their paper, character here) a **learnable embedding vector**, e.g. 10\u2011dimensional,\n", "- feed a **fixed-length block of previous symbols** into a neural network,\n", "- have the network output a probability distribution over the next symbol,\n", "- train the whole thing by **maximising log-likelihood** (minimising negative log-likelihood).\n", "\n", "Even if the model hasn\u2019t seen the **exact** context `\"dog was running in a ...\"`, it may have seen:\n", "\n", "- `\"the dog was running in a ...\"`\n", "- `\"a cat was running in a ...\"`\n", "\n", "and learn that `\"a\"` and `\"the\"` have similar embeddings, `\"dog\"` and `\"cat\"` are similar, etc.\n", "This way, it can **generalise** to new but related contexts.\n", "\n", "We will build a character-level version of that idea.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 4. Context windows (`block_size`) and a rolling example\n", "\n", "We choose a **context length** `block_size`, the number of previous characters we give the model.\n", "\n", "In this notebook we\u2019ll start with:\n", "\n", "```python\n", "block_size = 3\n", "```\n", "\n", "That means:\n", "\n", "- For each position in a word, the input is the previous 3 characters (padded with `'.'`),\n", "- The target is the current character.\n", "\n", "For example, for `block_size = 3` and the word `\"emma\"` we will generate:\n", "\n", "```text\n", "... -> e\n", "..e -> m\n", ".e m -> m\n", "emm -> a\n", "mma -> .   (end of name)\n", "```\n", "\n", "Let\u2019s build these contexts for **just one word** to see exactly what\u2019s happening.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 2 \u2013 your turn: contexts for one word\n", "\n", "block_size = 3\n", "\n", "# Use the FIRST word in the dataset\n", "w = words[0]\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 2: print all (context -> next_char) pairs for one word\")\n", "\n", "# Hints:\n", "# - start with context = [0] * block_size  ('.' -> index 0)\n", "# - loop over characters in w + '.'\n", "# - convert ch to index with stoi[ch]\n", "# - print ''.join(itos[i] for i in context), '--->', itos[ix]\n", "# - then update context = context[1:] + [ix]"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 2 \u2013 contexts for one word\n", "\n", "block_size = 3\n", "\n", "w = words[0]\n", "print(\"Word:\", w)\n", "\n", "context = [0] * block_size  # start with '...' (all dots)\n", "for ch in w + '.':\n", "    ix = stoi[ch]\n", "    # show the mapping from context to next char\n", "    print(''.join(itos[i] for i in context), '--->', itos[ix])\n", "    # roll the context window forward\n", "    context = context[1:] + [ix]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**Solution discussion:**\n", "\n", "- We start with `context = [0, 0, 0]`, which decodes to `\"...\"`.\n", "- For each character `ch` in `w + '.'`:\n", "  - we look up its index `ix = stoi[ch]`,\n", "  - we print the current context and the next character,\n", "  - we **slide the window**: drop the oldest index (`context[1:]`) and append the new one (`+ [ix]`).\n", "\n", "This pattern is exactly how we\u2019ll build the full dataset.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 5. Building the full dataset and splitting into train/dev/test\n", "\n", "Now we\u2019ll turn *all* words into a big dataset of `(context, next_char)` pairs.\n", "\n", "We\u2019ll wrap the logic in a function:\n", "\n", "```python\n", "def build_dataset(words):\n", "    X, Y = [], []\n", "    # for each word:\n", "    #   start with context = [0] * block_size\n", "    #   loop over characters in w + '.'\n", "    #   append context to X and ix (target) to Y\n", "    # return X, Y as tensors\n", "```\n", "\n", "Then we\u2019ll:\n", "\n", "1. Shuffle the list of words.\n", "2. Split into 3 parts:\n", "   - 80% train,\n", "   - 10% dev (validation),\n", "   - 10% test.\n", "3. Build `Xtr, Ytr`, `Xdev, Ydev`, `Xte, Yte` with `build_dataset`.\n", "\n", "We\u2019ll use these splits to talk about **overfitting** and **generalisation** later.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 3 \u2013 your turn: build dataset + splits\n", "\n", "block_size = 3  # keep using 3-char context\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 3: implement build_dataset and create train/dev/test splits\")\n", "\n", "# Hints:\n", "# - def build_dataset(words):\n", "#       X, Y = [], []\n", "#       for each word w:\n", "#           context = [0] * block_size\n", "#           for ch in w + '.':\n", "#               ix = stoi[ch]\n", "#               X.append(context)\n", "#               Y.append(ix)\n", "#               context = context[1:] + [ix]\n", "#       convert X, Y to tensors and return\n", "# - use random.seed(...) then random.shuffle(words)\n", "# - n1 = int(0.8 * len(words))\n", "#   n2 = int(0.9 * len(words))\n", "# - words[:n1], words[n1:n2], words[n2:]"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 3 \u2013 build dataset + splits\n", "\n", "import random\n", "\n", "def build_dataset(words_subset):\n", "    X, Y = [], []\n", "    for w in words_subset:\n", "        context = [0] * block_size\n", "        for ch in w + '.':\n", "            ix = stoi[ch]\n", "            X.append(context)\n", "            Y.append(ix)\n", "            context = context[1:] + [ix]\n", "    X = torch.tensor(X)\n", "    Y = torch.tensor(Y)\n", "    print(\"built dataset:\", X.shape, Y.shape)\n", "    return X, Y\n", "\n", "# shuffle words once with a fixed seed for reproducibility\n", "random.seed(42)\n", "random.shuffle(words)\n", "\n", "n1 = int(0.8 * len(words))\n", "n2 = int(0.9 * len(words))\n", "\n", "Xtr, Ytr = build_dataset(words[:n1])\n", "Xdev, Ydev = build_dataset(words[n1:n2])\n", "Xte, Yte = build_dataset(words[n2:])\n", "\n", "Xtr.shape, Ytr.shape, Xdev.shape, Ydev.shape, Xte.shape, Yte.shape"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**Solution discussion:**\n", "\n", "- `build_dataset` encapsulates the context-building logic we tested on a single word.\n", "- We convert `X` and `Y` to tensors so they work nicely with PyTorch.\n", "- We split the shuffled names into 3 disjoint sets:\n", "  - **train** (`Xtr, Ytr`) is used to fit model parameters,\n", "  - **dev** (`Xdev, Ydev`) is used to tune hyperparameters (like hidden size, embedding size, learning rate),\n", "  - **test** (`Xte, Yte`) is only used at the very end to get a final performance number.\n", "\n", "Now we have a large supervised learning dataset of shape roughly `(200k+, 3)` \u2192 `(200k+, )` for the train split.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 6. Character embeddings \u2013 replacing huge tables with vectors\n", "\n", "The bigram model stored probabilities in a huge **table**.\n", "\n", "Instead, we\u2019ll give every character a small **embedding vector** of size `n_embed`.\n", "\n", "- Think of `C` as a matrix of shape `(vocab_size, n_embed)`.\n", "- Row `i` is the embedding for character with index `i`.\n", "- These embeddings start as random values and are tuned by gradient descent.\n", "\n", "This is exactly like Bengio\u2019s word embeddings, but at the character level.\n", "\n", "To build intuition, let\u2019s work with a tiny embedding dimension (2D) and embed a single index in two ways:\n", "\n", "1. Direct lookup: `C[ix]`\n", "2. One-hot \u2192 matrix multiply (like a linear layer with no bias).\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 4 \u2013 your turn: embedding a single index in two equivalent ways\n", "\n", "vocab_size = len(stoi)\n", "C_demo = torch.randn((vocab_size, 2))  # 2D embeddings for illustration\n", "\n", "ix = 5  # arbitrary character index (not special in any way)\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 4: embed ix with direct indexing and with one-hot @ C_demo\")\n", "\n", "# Hints:\n", "# - emb_direct = C_demo[ix]\n", "# - one_hot = F.one_hot(torch.tensor(ix), num_classes=vocab_size).float()\n", "# - emb_via_one_hot = one_hot @ C_demo\n", "# - print both and compare"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 4 \u2013 embedding a single index in two equivalent ways\n", "\n", "vocab_size = len(stoi)\n", "C_demo = torch.randn((vocab_size, 2))\n", "\n", "ix = 5  # some index\n", "\n", "# 1) Direct lookup: treat C_demo as a lookup table\n", "emb_direct = C_demo[ix]\n", "\n", "# 2) One-hot encode ix, then multiply by C_demo\n", "one_hot = F.one_hot(torch.tensor(ix), num_classes=vocab_size).float()  # shape: (vocab_size,)\n", "emb_via_one_hot = one_hot @ C_demo  # shape: (2,)\n", "\n", "print(\"emb_direct     :\", emb_direct)\n", "print(\"emb_via_one_hot:\", emb_via_one_hot)\n", "print(\"difference     :\", (emb_direct - emb_via_one_hot).abs().max().item())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**Solution discussion:**\n", "\n", "- The one-hot vector is all zeros except a 1 at position `ix`.\n", "- When we compute `one_hot @ C_demo`, the matrix multiply picks out exactly the `ix`\u2011th row of `C_demo`.\n", "- So `C_demo[ix]` and `one_hot @ C_demo` are numerically identical (up to floating-point rounding).\n", "\n", "This shows that:\n", "\n", "> \u201cEmbedding lookup\u201d is just a special, efficient case of a linear layer with one-hot inputs.\n", "\n", "We\u2019ll mostly use the **lookup** style (`C[X]`) because it\u2019s faster and more concise.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 7. Embedding an entire batch of contexts\n", "\n", "We don\u2019t want to embed characters one at a time; we want to embed entire **batches** of contexts.\n", "\n", "Recall:\n", "\n", "- `Xtr` has shape `(N, block_size)` and stores character indices.\n", "- If we have an embedding matrix `C` of shape `(vocab_size, n_embed)`, then `C[Xtr]` gives us:\n", "\n", "```python\n", "C[Xtr].shape == (N, block_size, n_embed)\n", "```\n", "\n", "Let\u2019s check this on a small batch.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 5 \u2013 your turn: embed a batch of contexts\n", "\n", "# Take a small batch of 4 training examples\n", "X_batch_demo = Xtr[:4]\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 5: embed X_batch_demo and print shapes\")\n", "\n", "# Hints:\n", "# - for now, reuse C_demo with n_embed = 2 (just for shape intuition)\n", "# - emb = C_demo[X_batch_demo]\n", "# - print X_batch_demo.shape and emb.shape"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 5 \u2013 embed a small batch and inspect shapes\n", "\n", "X_batch_demo = Xtr[:4]  # shape: (4, block_size)\n", "\n", "emb_demo = C_demo[X_batch_demo]  # shape: (4, block_size, 2)\n", "\n", "print(\"X_batch_demo.shape:\", X_batch_demo.shape)\n", "print(\"emb_demo.shape    :\", emb_demo.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**Solution discussion:**\n", "\n", "- `X_batch_demo` has shape `(4, 3)` \u2192 4 examples, each a context of length 3.\n", "- `C_demo` has shape `(vocab_size, 2)`. Indexing with a 2D tensor `X_batch_demo` gives a 3D tensor:\n", "  - first dimension: which example (0..3),\n", "  - second dimension: position in the context (0..2),\n", "  - third dimension: embedding dimension (0..1).\n", "\n", "We\u2019ll now switch from the toy `C_demo` to a \u201creal\u201d embedding matrix with a larger dimension (e.g. 10)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 8. Defining the MLP parameters\n", "\n", "We now define our actual model hyperparameters:\n", "\n", "- `n_embed`: embedding dimension (e.g. 10),\n", "- `n_hidden`: number of neurons in the hidden layer (e.g. 200),\n", "- `block_size = 3`: context length (already chosen).\n", "\n", "Our parameters will be:\n", "\n", "- `C` : `(vocab_size, n_embed)` \u2013 character embedding table,\n", "- `W1`: `(block_size * n_embed, n_hidden)` \u2013 weights of hidden layer,\n", "- `b1`: `(n_hidden,)` \u2013 biases of hidden layer,\n", "- `W2`: `(n_hidden, vocab_size)` \u2013 weights of output layer,\n", "- `b2`: `(vocab_size,)` \u2013 biases of output layer.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Define model hyperparameters\n", "n_embed = 10   # embedding dimension\n", "n_hidden = 200 # hidden layer size\n", "\n", "g = torch.Generator().manual_seed(2147483647)  # for reproducibility\n", "\n", "C = torch.randn((vocab_size, n_embed), generator=g)\n", "W1 = torch.randn((block_size * n_embed, n_hidden), generator=g)\n", "b1 = torch.randn(n_hidden, generator=g)\n", "W2 = torch.randn((n_hidden, vocab_size), generator=g)\n", "b2 = torch.randn(vocab_size, generator=g)\n", "\n", "parameters = [C, W1, b1, W2, b2]\n", "print(\"Total parameters:\", sum(p.nelement() for p in parameters))\n", "\n", "for p in parameters:\n", "    p.requires_grad = True"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 9. Forward pass: from indices to logits\n", "\n", "Given a batch `Xb` of shape `(B, block_size)` and targets `Yb` of shape `(B,)`, the forward pass is:\n", "\n", "1. `emb = C[Xb]`\n", "   - shape: `(B, block_size, n_embed)`\n", "2. `emb_flat = emb.view(B, block_size * n_embed)`\n", "   - shape: `(B, block_size * n_embed)` \u2013 concatenate embeddings in the context.\n", "3. Hidden layer: `h = torch.tanh(emb_flat @ W1 + b1)`\n", "   - `@` is matrix multiplication,\n", "   - `b1` is broadcast over the batch dimension.\n", "4. Output layer: `logits = h @ W2 + b2`\n", "   - `logits` shape: `(B, vocab_size)`.\n", "\n", "Let\u2019s implement this for a small batch and check shapes.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 6 \u2013 your turn: forward pass to logits on a small batch\n", "\n", "batch_size = 32\n", "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n", "Xb = Xtr[ix]\n", "Yb = Ytr[ix]\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 6: compute emb, emb_flat, h, logits and print shapes\")\n", "\n", "# Hints:\n", "# - emb = C[Xb]\n", "# - B = emb.shape[0]\n", "# - emb_flat = emb.view(B, -1)\n", "# - h = torch.tanh(emb_flat @ W1 + b1)\n", "# - logits = h @ W2 + b2"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 6 \u2013 forward pass to logits\n", "\n", "batch_size = 32\n", "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n", "Xb = Xtr[ix]\n", "Yb = Ytr[ix]\n", "\n", "# 1. Embed\n", "emb = C[Xb]                    # (B, block_size, n_embed)\n", "B = emb.shape[0]\n", "\n", "# 2. Flatten context embeddings\n", "emb_flat = emb.view(B, -1)     # (B, block_size * n_embed)\n", "\n", "# 3. Hidden layer with tanh nonlinearity\n", "h = torch.tanh(emb_flat @ W1 + b1)  # (B, n_hidden)\n", "\n", "# 4. Output layer logits\n", "logits = h @ W2 + b2           # (B, vocab_size)\n", "\n", "print(\"emb.shape     :\", emb.shape)\n", "print(\"emb_flat.shape:\", emb_flat.shape)\n", "print(\"h.shape       :\", h.shape)\n", "print(\"logits.shape  :\", logits.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**Solution discussion:**\n", "\n", "Key PyTorch tricks:\n", "\n", "- `emb.view(B, -1)` **reshapes** the last two dimensions `(block_size, n_embed)` into one `(block_size * n_embed)`.\n", "  - This is much more efficient than manually concatenating tensors.\n", "- When we do `emb_flat @ W1 + b1`:\n", "  - `emb_flat` is `(B, 30)` (for `block_size=3, n_embed=10`),\n", "  - `W1` is `(30, n_hidden)`,\n", "  - result is `(B, n_hidden)`,\n", "  - adding `b1` (shape `(n_hidden,)`) uses **broadcasting**: the same bias vector is added to every row.\n", "\n", "So far we\u2019ve built a pure **linear MLP** that outputs unnormalised scores (`logits`) for each possible next character.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 10. From logits to loss: softmax and cross-entropy\n", "\n", "To train the model, we want a **loss** that measures how well the logits match the actual next characters.\n", "\n", "Steps (manually):\n", "\n", "1. Convert logits to **unnormalised counts** by exponentiating: `counts = logits.exp()`\n", "2. Convert counts to probabilities by row-wise normalisation:\n", "   \\[ P_{ij} = \\frac{\\exp(\\text{logits}_{ij})}{\\sum_k \\exp(\\text{logits}_{ik})} \\]\n", "3. For each example `i`, take the probability of the correct class `Yb[i]`.\n", "4. Take log-probabilities and average the **negative** log-probabilities.\n", "\n", "PyTorch has a ready-made function `F.cross_entropy(logits, targets)` that does all this:\n", "\n", "- applies softmax internally in a numerically stable way,\n", "- then computes the average negative log-likelihood.\n", "\n", "Let\u2019s compute the loss both ways and compare.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 7 \u2013 your turn: manual NLL vs F.cross_entropy\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 7: compute manual NLL and compare with F.cross_entropy\")\n", "\n", "# Hints:\n", "# - counts = logits.exp()\n", "# - probs = counts / counts.sum(1, keepdims=True)\n", "# - log_probs_correct = probs[torch.arange(B), Yb].log()\n", "# - loss_manual = -log_probs_correct.mean()\n", "# - loss_ce = F.cross_entropy(logits, Yb)\n", "# - print(loss_manual.item(), loss_ce.item())"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 7 \u2013 manual NLL vs F.cross_entropy\n", "\n", "# Reuse logits and Yb from the previous cell\n", "B = logits.shape[0]\n", "\n", "# Manual computation\n", "counts = logits.exp()\n", "probs = counts / counts.sum(1, keepdims=True)\n", "log_probs_correct = probs[torch.arange(B), Yb].log()\n", "loss_manual = -log_probs_correct.mean()\n", "\n", "# Using PyTorch helper\n", "loss_ce = F.cross_entropy(logits, Yb)\n", "\n", "print(\"Manual NLL loss :\", loss_manual.item())\n", "print(\"F.cross_entropy :\", loss_ce.item())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**Solution discussion:**\n", "\n", "- Both methods compute the **average negative log-likelihood** over the batch.\n", "- `F.cross_entropy` and our manual version should match very closely.\n", "\n", "Why use `F.cross_entropy` in practice?\n", "\n", "1. **Efficiency** \u2013 it doesn\u2019t need to store unnecessary intermediate tensors.\n", "2. **Numerical stability** \u2013 it effectively subtracts `logits.max()` inside the softmax to avoid overflow when logits get large.\n", "3. **Convenience** \u2013 less boilerplate code and less room for subtle bugs.\n", "\n", "From now on we\u2019ll use `F.cross_entropy` for the loss.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 11. One gradient descent step with autograd\n", "\n", "Let\u2019s now train the network a tiny bit.\n", "\n", "The steps for **one update** are:\n", "\n", "1. Pick a mini-batch of examples.\n", "2. Forward pass \u2192 logits \u2192 loss.\n", "3. Zero all parameter gradients.\n", "4. Backward pass: `loss.backward()`.\n", "5. Update each parameter with gradient descent:\n", "   \\[ p \\leftarrow p - \\text{learning\\_rate} \\times p.\\text{grad} \\]\n", "\n", "We\u2019ll implement just **one update** and check that the loss goes down **on the same batch**.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 8 \u2013 your turn: one training step on a mini-batch\n", "\n", "# Use a fresh random mini-batch\n", "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n", "Xb = Xtr[ix]\n", "Yb = Ytr[ix]\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 8: do one gradient descent step and print loss before/after\")\n", "\n", "# Hints:\n", "# - Forward pass to get loss_before (use F.cross_entropy)\n", "# - Zero gradients: for p in parameters: p.grad = None\n", "# - loss_before.backward()\n", "# - learning rate, e.g. lr = 0.1\n", "# - update parameters: p.data += -lr * p.grad\n", "# - recompute loss_after on the *same* batch and print both"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 8 \u2013 one training step on a mini-batch\n", "\n", "# Fresh mini-batch\n", "ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n", "Xb = Xtr[ix]\n", "Yb = Ytr[ix]\n", "\n", "# Forward pass \u2013 loss before update\n", "emb = C[Xb]\n", "B = emb.shape[0]\n", "emb_flat = emb.view(B, -1)\n", "h = torch.tanh(emb_flat @ W1 + b1)\n", "logits = h @ W2 + b2\n", "loss_before = F.cross_entropy(logits, Yb)\n", "\n", "print(\"loss before update:\", loss_before.item())\n", "\n", "# Backward pass\n", "for p in parameters:\n", "    p.grad = None\n", "loss_before.backward()\n", "\n", "# Gradient descent update\n", "lr = 0.1\n", "for p in parameters:\n", "    p.data += -lr * p.grad\n", "\n", "# Forward pass \u2013 loss after update (on the *same* batch)\n", "emb = C[Xb]\n", "B = emb.shape[0]\n", "emb_flat = emb.view(B, -1)\n", "h = torch.tanh(emb_flat @ W1 + b1)\n", "logits = h @ W2 + b2\n", "loss_after = F.cross_entropy(logits, Yb)\n", "\n", "print(\"loss after  update:\", loss_after.item())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**Solution discussion:**\n", "\n", "- `loss.backward()` walks backward through all operations and fills `p.grad` for each parameter `p`.\n", "- The gradient tells us which direction **increases** the loss.\n", "- Stepping with `p.data += -lr * p.grad` roughly moves us towards a parameter setting with lower loss.\n", "\n", "On a **single batch** the loss should almost always go down after one step. On the full training set it\u2019s more noisy, which is why we average over many mini-batches.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 12. Full training loop with mini-batches and a simple LR schedule\n", "\n", "We now put it all together:\n", "\n", "- mini-batch size: 32\n", "- number of steps: e.g. 200,000\n", "- learning rate schedule:\n", "  - `lr = 0.1` for the first 100,000 steps,\n", "  - `lr = 0.01` for the remaining 100,000 steps.\n", "\n", "In practice, you can lower these numbers if you\u2019re just experimenting or if training is too slow.\n", "\n", "We will record the loss every 1000 steps so we can plot it.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 9 \u2013 your turn: training loop with mini-batches\n", "\n", "max_steps = 200000\n", "batch_size = 32\n", "\n", "loss_history = []\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 9: implement the training loop over many mini-batches\")\n", "\n", "# Hints:\n", "# For i in range(max_steps):\n", "#   - sample ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n", "#   - forward to compute loss (F.cross_entropy)\n", "#   - zero grads, backward, update with lr = 0.1 if i < 100000 else 0.01\n", "#   - every 1000 steps: append loss.item() to loss_history and print it"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 9 \u2013 training loop with mini-batches\n", "\n", "max_steps = 200000\n", "batch_size = 32\n", "\n", "loss_history = []\n", "\n", "for i in range(max_steps):\n", "    # Sample a mini-batch of indices\n", "    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\n", "    Xb = Xtr[ix]\n", "    Yb = Ytr[ix]\n", "\n", "    # Forward pass\n", "    emb = C[Xb]               # (B, block_size, n_embed)\n", "    B = emb.shape[0]\n", "    emb_flat = emb.view(B, -1)\n", "    h = torch.tanh(emb_flat @ W1 + b1)\n", "    logits = h @ W2 + b2\n", "    loss = F.cross_entropy(logits, Yb)\n", "\n", "    # Backward pass\n", "    for p in parameters:\n", "        p.grad = None\n", "    loss.backward()\n", "\n", "    # Update\n", "    lr = 0.1 if i < 100000 else 0.01\n", "    for p in parameters:\n", "        p.data += -lr * p.grad\n", "\n", "    # Logging\n", "    if i % 1000 == 0:\n", "        loss_history.append(loss.item())\n", "        print(f\"step {i:6d} | loss = {loss.item():.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "> **Note:** If 200k steps is too slow for your machine, feel free to reduce `max_steps`\n", "and/or the hidden size and embedding size. The logic remains exactly the same.\n", "\n", "Let\u2019s plot the training loss samples.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["plt.figure()\n", "plt.plot(loss_history)\n", "plt.xlabel(\"checkpoint (every 1000 steps)\")\n", "plt.ylabel(\"mini-batch loss\")\n", "plt.title(\"Training loss over time\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 13. Evaluating train and dev loss\n", "\n", "The training loss we plotted is computed on **mini-batches**, not the whole dataset.\n", "\n", "To see how well the model really fits the data, we compute:\n", "\n", "- **train loss** on *all* `Xtr, Ytr`,\n", "- **dev loss** on *all* `Xdev, Ydev`.\n", "\n", "We expect the dev loss to be a bit higher than the train loss (because the model was trained on train, not dev).\n", "If dev loss is much higher than train loss, we are **overfitting**.\n", "If dev loss is similar but both are high, we are probably **underfitting**.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 10 \u2013 your turn: compute train and dev loss\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 10: compute average loss on train and dev splits\")\n", "\n", "# Hints:\n", "# - define a helper function split_loss(X, Y):\n", "#      emb = C[X]\n", "#      N = emb.shape[0]\n", "#      emb_flat = emb.view(N, -1)\n", "#      h = torch.tanh(emb_flat @ W1 + b1)\n", "#      logits = h @ W2 + b2\n", "#      return F.cross_entropy(logits, Y)\n", "# - then call it on Xtr, Ytr and Xdev, Ydev"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 10 \u2013 compute train and dev loss\n", "\n", "def split_loss(X, Y):\n", "    emb = C[X]\n", "    N = emb.shape[0]\n", "    emb_flat = emb.view(N, -1)\n", "    h = torch.tanh(emb_flat @ W1 + b1)\n", "    logits = h @ W2 + b2\n", "    return F.cross_entropy(logits, Y)\n", "\n", "loss_train = split_loss(Xtr, Ytr)\n", "loss_dev = split_loss(Xdev, Ydev)\n", "\n", "print(\"train loss:\", loss_train.item())\n", "print(\"dev   loss:\", loss_dev.item())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**Solution discussion:**\n", "\n", "- `split_loss` is just the forward pass applied to *all* examples in a split at once.\n", "- We get a single scalar cross-entropy loss for each split.\n", "- In the original lesson, typical numbers (after enough training) were roughly:\n", "  - train \u2248 2.12\n", "  - dev   \u2248 2.17\n", "\n", "If your numbers are similar, your model is behaving as in the video.\n", "If dev is much lower or higher, you may have changed hyperparameters or training length.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 14. Visualising character embeddings\n", "\n", "Character embeddings live in a `n_embed`-dimensional space (here 10D), so we can\u2019t really \u201csee\u201d them directly.\n", "\n", "However, we can at least plot the **first two** dimensions to see whether the model has learned anything interesting.\n", "\n", "Often you\u2019ll see:\n", "\n", "- vowels clustering near each other,\n", "- special tokens like `'.'` or rare letters (`'q'`) sitting in special places.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 11 \u2013 your turn: scatter plot of embeddings (first two dims)\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 11: scatter plot C[:,0] vs C[:,1] with character labels\")\n", "\n", "# Hints:\n", "# - plt.figure(figsize=(6,6))\n", "# - plt.scatter(C[:,0].data, C[:,1].data)\n", "# - loop over i in range(vocab_size): plt.text(C[i,0].item(), C[i,1].item(), itos[i], ...)\n", "# - plt.grid(True)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 11 \u2013 scatter plot of embeddings\n", "\n", "plt.figure(figsize=(6, 6))\n", "plt.scatter(C[:, 0].data, C[:, 1].data, s=200)\n", "\n", "for i in range(vocab_size):\n", "    ch = itos[i]\n", "    plt.text(C[i, 0].item(), C[i, 1].item(), ch, ha='center', va='center', color='white')\n", "\n", "plt.xlabel(\"embedding dim 0\")\n", "plt.ylabel(\"embedding dim 1\")\n", "plt.grid(True)\n", "plt.title(\"Character embeddings (first two dimensions)\")\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "**Solution discussion:**\n", "\n", "Remember that these embeddings started as random noise.\n", "\n", "After training, the network has moved them around so that characters with **similar roles** in names\n", "(e.g. vowels, consonants, start/end token `'.'`) tend to have related vectors.\n", "\n", "In the Bengio paper, this was done at the **word** level; here we see a smaller, character-level analogue.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 15. Sampling new names from the trained MLP\n", "\n", "We can now use the trained model to **generate** names.\n", "\n", "Sampling procedure:\n", "\n", "1. Start with `context = [0] * block_size` (all dots).\n", "2. Repeat:\n", "   - embed the context,\n", "   - run through MLP to get logits,\n", "   - convert logits to probabilities with `F.softmax`,\n", "   - sample an index from the distribution using `torch.multinomial`,\n", "   - shift the context and append this index,\n", "   - save the index in an output list,\n", "   - stop when we sample `ix == 0` (end token `'.'`).\n", "\n", "Let\u2019s implement a loop that samples 20 names.\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Exercise 12 \u2013 your turn: sampling names from the trained model\n", "\n", "g = torch.Generator().manual_seed(2147483647 + 10)  # deterministic sampling\n", "\n", "### YOUR CODE HERE\n", "raise NotImplementedError(\"Exercise 12: implement sampling loop to print 20 generated names\")\n", "\n", "# Hints:\n", "# for _ in range(20):\n", "#   context = [0] * block_size\n", "#   out = []\n", "#   while True:\n", "#       x = torch.tensor([context])\n", "#       emb = C[x]\n", "#       emb_flat = emb.view(1, -1)\n", "#       h = torch.tanh(emb_flat @ W1 + b1)\n", "#       logits = h @ W2 + b2\n", "#       probs = F.softmax(logits, dim=1)\n", "#       ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n", "#       context = context[1:] + [ix]\n", "#       out.append(ix)\n", "#       if ix == 0: break\n", "#   print(''.join(itos[i] for i in out))"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Solution 12 \u2013 sampling names from the trained model\n", "\n", "g = torch.Generator().manual_seed(2147483647 + 10)\n", "\n", "for _ in range(20):\n", "    context = [0] * block_size\n", "    out = []\n", "    while True:\n", "        x = torch.tensor([context])          # (1, block_size)\n", "        emb = C[x]                           # (1, block_size, n_embed)\n", "        emb_flat = emb.view(1, -1)           # (1, block_size * n_embed)\n", "        h = torch.tanh(emb_flat @ W1 + b1)   # (1, n_hidden)\n", "        logits = h @ W2 + b2                 # (1, vocab_size)\n", "        probs = F.softmax(logits, dim=1)     # (1, vocab_size)\n", "\n", "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n", "        context = context[1:] + [ix]\n", "        out.append(ix)\n", "        if ix == 0:\n", "            break\n", "\n", "    print(''.join(itos[i] for i in out))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "You should see names that are much more realistic than the pure bigram model:\n", "\n", "- They tend to have plausible syllable structure,\n", "- They often look like variations of real names,\n", "- But many will be unique (not exactly present in `names.txt`).\n", "\n", "This is exactly what the MLP is designed to do: capture longer-range patterns than bigrams,\n", "without exploding into a gigantic count table.\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## 16. Wrap-up and next steps\n", "\n", "In this notebook you:\n", "\n", "- Revisited the bigram model\u2019s limitation: **too little context** or an **exploding table**.\n", "- Built a **character-level dataset** of `(context, next_char)` pairs for arbitrary `block_size`.\n", "- Implemented a **learned embedding table** and saw its equivalence to one-hot + linear.\n", "- Constructed a **2-layer MLP** by hand (no `nn.Module` magic):\n", "  - embedding lookup,\n", "  - concatenation via `view`,\n", "  - hidden layer with `tanh`,\n", "  - output layer producing logits.\n", "- Used **`F.cross_entropy`** to compute negative log-likelihood in a numerically stable way.\n", "- Trained with **mini-batch stochastic gradient descent** and a simple learning rate schedule.\n", "- Evaluated **train vs dev loss** and discussed under- and overfitting.\n", "- Visualised character embeddings (first two dimensions).\n", "- Sampled new names from the trained MLP.\n", "\n", "From here, you can experiment with:\n", "\n", "- **Changing context length** (`block_size`).\n", "- **Changing embedding size** (`n_embed`).\n", "- **Changing hidden size** (`n_hidden`).\n", "- Trying different **learning rate schedules**, batch sizes, or optimisers.\n", "- Implementing a **learning rate finder** like in the video (sweep over log-spaced learning rates and plot loss).\n", "\n", "All the core ideas from the MLP makemore video are now encoded in this notebook as exercises + solutions,\n", "so you can re-derive everything yourself just by working through it.\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10"}}, "nbformat": 4, "nbformat_minor": 5}