{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: makemore (part 3) — init, activation stats, and batch norm (exercise-based)\n",
    "\n",
    "This notebook turns the **video transcript** + the **original companion notebook** into a guided, exercise-based tutorial.\n",
    "\n",
    "**Format per exercise**:  \n",
    "1) Motivation / concept explanation  \n",
    "2) Exercise (you do it)  \n",
    "3) Solution (check yourself)\n",
    "\n",
    "We’ll start from “nothing” and end with:\n",
    "- a working character-level MLP language model,\n",
    "- a deep “PyTorchified” MLP,\n",
    "- careful diagnosis of **logits**, **tanh saturation**, and **gradient flow**,\n",
    "- principled initialization (fan-in scaling + gain),\n",
    "- batch norm (including running stats + inference behavior),\n",
    "- diagnostic plots (activations, gradients, update-to-data ratios).\n",
    "\n",
    "> Notes:\n",
    "> - This is **character-level** language modeling on the popular `names.txt` dataset used in makemore.\n",
    "> - Training to full convergence can take time on CPU. Default settings are chosen to be reasonable; you can scale them up.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Exercise 0 (Setup): imports, reproducibility, and data download helpers\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# notebook niceties\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "\n",
    "# reproducibility\n",
    "GLOBAL_SEED = 2147483647\n",
    "g = torch.Generator().manual_seed(GLOBAL_SEED)\n",
    "random.seed(42)\n",
    "\n",
    "def get_device():\n",
    "    # For simplicity we default to CPU, but allow GPU if available.\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 — Load words and build a vocabulary\n",
    "\n",
    "### Motivation / concept\n",
    "We will model the next character in a name given the previous `block_size` characters.  \n",
    "To do this we need:\n",
    "- the list of training words (names),\n",
    "- a character vocabulary,\n",
    "- integer encodings (`stoi`, `itos`).\n",
    "\n",
    "In makemore, the special token `'.'` is used both as:\n",
    "- a **start/end** marker (we prepend context of dots, and we terminate names with a dot).\n",
    "\n",
    "### Exercise\n",
    "1) Load the file `names.txt`. If it doesn’t exist, download it.\n",
    "2) Print:\n",
    "   - first 8 words,\n",
    "   - total number of words,\n",
    "   - vocabulary size,\n",
    "   - `stoi`/`itos` for sanity.\n",
    "\n",
    "### Solution\n",
    "Run the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: load words + vocabulary\n",
    "\n",
    "# If names.txt is missing, download a copy from the original makemore dataset location.\n",
    "# (If your environment has no internet, you can place names.txt next to this notebook.)\n",
    "NAMES_PATH = \"names.txt\"\n",
    "if not os.path.exists(NAMES_PATH):\n",
    "    import urllib.request\n",
    "    url = \"https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\"\n",
    "    print(f\"Downloading {url} -> {NAMES_PATH}\")\n",
    "    urllib.request.urlretrieve(url, NAMES_PATH)\n",
    "\n",
    "words = open(NAMES_PATH, \"r\", encoding=\"utf-8\").read().splitlines()\n",
    "\n",
    "print(\"First 8 words:\", words[:8])\n",
    "print(\"Number of words:\", len(words))\n",
    "\n",
    "# build the vocabulary\n",
    "chars = sorted(list(set(\"\".join(words))))\n",
    "stoi = {ch:i+1 for i,ch in enumerate(chars)}  # reserve 0 for '.'\n",
    "stoi[\".\"] = 0\n",
    "itos = {i:ch for ch,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "print(\"itos:\", itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 — Build the dataset (X, Y) with a context window\n",
    "\n",
    "### Motivation / concept\n",
    "Each training example is:\n",
    "- `X`: a length-`block_size` context of character indices\n",
    "- `Y`: the next character index\n",
    "\n",
    "Example with `block_size=3` for word `\"emma\"`:\n",
    "\n",
    "- context `...` → target `e`\n",
    "- context `..e` → target `m`\n",
    "- context `.em` → target `m`\n",
    "- context `emm` → target `a`\n",
    "- context `mma` → target `.` (end token)\n",
    "\n",
    "We will create three splits:\n",
    "- train (80%), dev/val (10%), test (10%)\n",
    "\n",
    "### Exercise\n",
    "Implement `build_dataset(words_subset, block_size)`:\n",
    "- iterate through each word + `'.'`\n",
    "- keep a sliding window context list\n",
    "- collect X and Y tensors\n",
    "\n",
    "### Solution\n",
    "Run the cell. Also verify the shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: dataset builder\n",
    "\n",
    "block_size = 3  # context length\n",
    "\n",
    "def build_dataset(words_subset, block_size):\n",
    "    X, Y = [], []\n",
    "    for w in words_subset:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + \".\":\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]  # shift left, append\n",
    "    X = torch.tensor(X, dtype=torch.long)\n",
    "    Y = torch.tensor(Y, dtype=torch.long)\n",
    "    return X, Y\n",
    "\n",
    "# split the words reproducibly\n",
    "words_shuf = words[:]\n",
    "random.shuffle(words_shuf)\n",
    "n1 = int(0.8 * len(words_shuf))\n",
    "n2 = int(0.9 * len(words_shuf))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words_shuf[:n1], block_size)\n",
    "Xdev, Ydev = build_dataset(words_shuf[n1:n2], block_size)\n",
    "Xte, Yte = build_dataset(words_shuf[n2:], block_size)\n",
    "\n",
    "print(\"train:\", Xtr.shape, Ytr.shape)\n",
    "print(\"dev:  \", Xdev.shape, Ydev.shape)\n",
    "print(\"test: \", Xte.shape, Yte.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 — Implement the baseline MLP forward pass (embedding → tanh → logits)\n",
    "\n",
    "### Motivation / concept\n",
    "We’ll implement the exact MLP structure from the original notebook:\n",
    "\n",
    "1) Embed each character index into a learnable vector (embedding table `C`)\n",
    "2) Concatenate the `block_size` embeddings into one vector\n",
    "3) Linear layer → `tanh` nonlinearity\n",
    "4) Linear layer to produce logits for all characters\n",
    "5) Cross-entropy loss\n",
    "\n",
    "### Exercise\n",
    "Create a function `forward_mlp(Xb, params)` that returns `(logits, loss)` where:\n",
    "- `Xb` is a minibatch of contexts (shape `(B, block_size)`)\n",
    "- `Yb` is targets (shape `(B,)`)\n",
    "- `params` contains `C, W1, b1?, W2, b2`\n",
    "\n",
    "We’ll start with a **single hidden layer**.\n",
    "\n",
    "### Solution\n",
    "Run the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: baseline single-hidden-layer MLP forward pass\n",
    "\n",
    "@dataclass\n",
    "class MLPParams:\n",
    "    C: torch.Tensor\n",
    "    W1: torch.Tensor\n",
    "    b1: torch.Tensor | None\n",
    "    W2: torch.Tensor\n",
    "    b2: torch.Tensor\n",
    "\n",
    "def init_baseline_params(vocab_size, n_embd=10, n_hidden=200, block_size=3, *,\n",
    "                         seed=GLOBAL_SEED, device=device):\n",
    "    g_local = torch.Generator(device=\"cpu\").manual_seed(seed)  # deterministic on CPU generator\n",
    "    C  = torch.randn((vocab_size, n_embd), generator=g_local, device=device)\n",
    "    W1 = torch.randn((n_embd*block_size, n_hidden), generator=g_local, device=device)\n",
    "    b1 = torch.randn((n_hidden,), generator=g_local, device=device)  # (we'll later discuss why this can be problematic)\n",
    "    W2 = torch.randn((n_hidden, vocab_size), generator=g_local, device=device)\n",
    "    b2 = torch.randn((vocab_size,), generator=g_local, device=device)\n",
    "    return MLPParams(C=C, W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "\n",
    "def forward_mlp(Xb, Yb, p: MLPParams, block_size=3):\n",
    "    # Xb: (B, block_size) of integer indices\n",
    "    emb = p.C[Xb]                 # (B, block_size, n_embd)\n",
    "    embcat = emb.view(emb.shape[0], -1)  # (B, block_size * n_embd)\n",
    "    hpre = embcat @ p.W1\n",
    "    if p.b1 is not None:\n",
    "        hpre = hpre + p.b1\n",
    "    h = torch.tanh(hpre)          # (B, n_hidden)\n",
    "    logits = h @ p.W2 + p.b2      # (B, vocab_size)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    return logits, loss\n",
    "\n",
    "# quick sanity check on a small batch\n",
    "p0 = init_baseline_params(vocab_size=vocab_size, device=device)\n",
    "ix = torch.randint(0, Xtr.shape[0], (32,), generator=g, device=device)\n",
    "Xb, Yb = Xtr[ix].to(device), Ytr[ix].to(device)\n",
    "logits, loss = forward_mlp(Xb, Yb, p0, block_size=block_size)\n",
    "logits.shape, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 — What loss should we expect at initialization?\n",
    "\n",
    "### Motivation / concept\n",
    "At initialization, we typically want the network to behave like:\n",
    "- it has **no idea** what the correct next character is,\n",
    "- so it outputs an approximately **uniform** distribution over the `vocab_size` characters.\n",
    "\n",
    "If `p = 1/vocab_size`, then the negative log-likelihood loss is `-log(p)`.\n",
    "\n",
    "For `vocab_size=27`, that’s about `3.296`.\n",
    "\n",
    "If you see a loss like **27** at iteration 0, it’s a big red flag: the network is **very confident and very wrong**, usually due to extreme logits.\n",
    "\n",
    "### Exercise\n",
    "Compute:\n",
    "- expected init loss = `-log(1/vocab_size)`\n",
    "- compare it with the actual first-batch loss from the baseline init\n",
    "\n",
    "### Solution\n",
    "Run this cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: expected init loss vs actual init loss\n",
    "\n",
    "expected_loss = -torch.log(torch.tensor(1.0 / vocab_size)).item()\n",
    "print(\"vocab_size:\", vocab_size)\n",
    "print(\"Expected loss at init (uniform):\", expected_loss)\n",
    "\n",
    "# take a fresh init and compute first-batch loss\n",
    "p_bad = init_baseline_params(vocab_size=vocab_size, device=device)\n",
    "logits0, loss0 = forward_mlp(Xb, Yb, p_bad, block_size=block_size)\n",
    "print(\"Actual first-batch loss:\", loss0.item())\n",
    "\n",
    "# inspect the scale of logits (first row)\n",
    "print(\"logits0[0] min/max:\", logits0[0].min().item(), logits0[0].max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 — Toy demo: extreme logits cause “confidently wrong” loss explosions\n",
    "\n",
    "### Motivation / concept\n",
    "Softmax converts logits to probabilities:\n",
    "\n",
    "- If all logits are equal (e.g. all zeros) → uniform probabilities → loss ≈ `log(vocab_size)`.\n",
    "- If logits are extreme → one class gets probability ≈ 1, others ≈ 0.\n",
    "  - If that class is incorrect → loss becomes huge.\n",
    "\n",
    "This is why **logit scale** matters for stable initialization.\n",
    "\n",
    "### Exercise\n",
    "For a toy 4-class problem:\n",
    "1) Sample random logits.\n",
    "2) Compute loss for different scaling factors: `1x`, `10x`, `50x`.\n",
    "3) Observe how loss behaves.\n",
    "\n",
    "### Solution\n",
    "Run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: toy softmax loss explosion with logit scaling\n",
    "\n",
    "def toy_loss(logits, y):\n",
    "    # logits: (K,), y integer label\n",
    "    return F.cross_entropy(logits.view(1,-1), torch.tensor([y], device=logits.device)).item()\n",
    "\n",
    "g_local = torch.Generator().manual_seed(GLOBAL_SEED + 123)\n",
    "K = 4\n",
    "y = 2\n",
    "\n",
    "base_logits = torch.randn(K, generator=g_local, device=device)\n",
    "for scale in [1.0, 10.0, 50.0]:\n",
    "    L = toy_loss(base_logits * scale, y)\n",
    "    probs = F.softmax(base_logits * scale, dim=0)\n",
    "    print(f\"scale={scale:>4}: loss={L:8.4f}, probs={probs.detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 — Fix #1: make the output layer less overconfident at init\n",
    "\n",
    "### Motivation / concept\n",
    "In the transcript, the first fix is:\n",
    "- set the output bias `b2` to (near) zero,\n",
    "- shrink `W2` so logits are near zero.\n",
    "\n",
    "This drives initial probabilities toward uniform → initial loss near `log(vocab_size)`.\n",
    "\n",
    "Important subtlety:\n",
    "- Setting weights exactly to zero can cause **symmetry problems** in many settings.\n",
    "- Here, the output layer can be *tiny* (e.g. `0.01`) instead of 0.\n",
    "\n",
    "### Exercise\n",
    "Modify initialization so:\n",
    "- `b2 = 0`,\n",
    "- `W2 *= 0.01` (or similar).\n",
    "\n",
    "Verify:\n",
    "- initial loss is near expected,\n",
    "- logits range is much smaller.\n",
    "\n",
    "### Solution\n",
    "Run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: output layer fix\n",
    "\n",
    "def init_fixed_output_params(vocab_size, n_embd=10, n_hidden=200, block_size=3, *,\n",
    "                             seed=GLOBAL_SEED, W2_scale=0.01, device=device):\n",
    "    g_local = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "    C  = torch.randn((vocab_size, n_embd), generator=g_local, device=device)\n",
    "    W1 = torch.randn((n_embd*block_size, n_hidden), generator=g_local, device=device)\n",
    "    b1 = torch.randn((n_hidden,), generator=g_local, device=device)\n",
    "    W2 = torch.randn((n_hidden, vocab_size), generator=g_local, device=device) * W2_scale\n",
    "    b2 = torch.zeros((vocab_size,), device=device)\n",
    "    return MLPParams(C=C, W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "\n",
    "p_fix_out = init_fixed_output_params(vocab_size=vocab_size, device=device, W2_scale=0.01)\n",
    "logits1, loss1 = forward_mlp(Xb, Yb, p_fix_out, block_size=block_size)\n",
    "print(\"Expected loss:\", expected_loss)\n",
    "print(\"New first-batch loss:\", loss1.item())\n",
    "print(\"logits1[0] min/max:\", logits1[0].min().item(), logits1[0].max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7 — Diagnose tanh saturation (hidden activations squashed to ±1)\n",
    "\n",
    "### Motivation / concept\n",
    "`tanh` outputs values in `[-1, 1]`.  \n",
    "If *pre-activations* are too large in magnitude, most `tanh` outputs become very close to `-1` or `+1`.\n",
    "\n",
    "Why is that bad?\n",
    "\n",
    "Backprop through `tanh` multiplies upstream gradient by:\n",
    "\n",
    "\\[\n",
    "\\frac{d}{dx}\\tanh(x) = 1 - \\tanh(x)^2\n",
    "\\]\n",
    "\n",
    "If `tanh(x) ≈ ±1`, then `1 - tanh(x)^2 ≈ 0`, and gradients **vanish** through that neuron.\n",
    "\n",
    "A nice diagnostic is:\n",
    "- histogram of `hpre` and `h`,\n",
    "- percent of activations with `|h| > 0.99` (near saturation).\n",
    "\n",
    "### Exercise\n",
    "Compute:\n",
    "- histogram of `hpre` and `h`,\n",
    "- saturation percentage,\n",
    "for the first minibatch.\n",
    "\n",
    "### Solution\n",
    "Run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: tanh saturation diagnostics\n",
    "\n",
    "def diagnose_tanh_saturation(Xb, p: MLPParams):\n",
    "    with torch.no_grad():\n",
    "        emb = p.C[Xb]\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        hpre = embcat @ p.W1 + (p.b1 if p.b1 is not None else 0.0)\n",
    "        h = torch.tanh(hpre)\n",
    "\n",
    "        sat = (h.abs() > 0.99).float().mean().item() * 100\n",
    "\n",
    "        print(\"hpre mean/std:\", hpre.mean().item(), hpre.std().item())\n",
    "        print(\"h   mean/std:\", h.mean().item(), h.std().item())\n",
    "        print(f\"% saturated (|h|>0.99): {sat:.2f}%\")\n",
    "\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.hist(hpre.cpu().view(-1).tolist(), bins=50)\n",
    "        plt.title(\"hpre (pre-activation) histogram\")\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.hist(h.cpu().view(-1).tolist(), bins=50)\n",
    "        plt.title(\"tanh(hpre) histogram\")\n",
    "        plt.show()\n",
    "\n",
    "# Use the 'fixed output' params, so logits are sane, then diagnose hidden activations\n",
    "diagnose_tanh_saturation(Xb, p_fix_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8 — Fix #2: scale the first layer to prevent tanh saturation\n",
    "\n",
    "### Motivation / concept\n",
    "If `hpre = embcat @ W1 + b1` has too-large variance, `tanh(hpre)` saturates.\n",
    "\n",
    "We can fix this by scaling down:\n",
    "- `W1` (and typically `b1` too)\n",
    "\n",
    "In the original notebook/video, you saw “magic numbers” like `0.2`.  \n",
    "Next we’ll replace that with principled scaling, but first let’s see the direct effect.\n",
    "\n",
    "### Exercise\n",
    "Try W1 scales: `1.0`, `0.5`, `0.2`, `0.1` and see:\n",
    "- pre-activation std\n",
    "- saturation percentage\n",
    "\n",
    "### Solution\n",
    "Run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: scan W1 scaling factor and see saturation\n",
    "\n",
    "def clone_with_scaled_W1(p: MLPParams, scale):\n",
    "    return MLPParams(C=p.C, W1=p.W1 * scale, b1=p.b1, W2=p.W2, b2=p.b2)\n",
    "\n",
    "for s in [1.0, 0.5, 0.2, 0.1]:\n",
    "    p_tmp = clone_with_scaled_W1(p_fix_out, s)\n",
    "    with torch.no_grad():\n",
    "        emb = p_tmp.C[Xb]\n",
    "        embcat = emb.view(emb.shape[0], -1)\n",
    "        hpre = embcat @ p_tmp.W1 + p_tmp.b1\n",
    "        h = torch.tanh(hpre)\n",
    "        sat = (h.abs() > 0.99).float().mean().item() * 100\n",
    "        print(f\"W1 scale={s:>4}: hpre std={hpre.std().item():.3f}, tanh sat%={sat:6.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9 — A principled way: fan-in scaling (variance preservation)\n",
    "\n",
    "### Motivation / concept\n",
    "If `x ~ N(0,1)` and `W ~ N(0,1)`, then `y = x @ W` tends to have **larger variance** as the input dimension grows.\n",
    "\n",
    "A classic approach is to scale weights by `1/sqrt(fan_in)` to keep activations roughly unit variance.\n",
    "\n",
    "We’ll reproduce the idea:\n",
    "- draw random inputs `x` with std ≈ 1\n",
    "- draw weights `W`\n",
    "- compare std of `y = xW` with/without scaling\n",
    "\n",
    "### Exercise\n",
    "Implement a small experiment:\n",
    "- `x` shape `(1000, fan_in)`\n",
    "- `W` shape `(fan_in, 200)`\n",
    "- compare output std for scaling 1.0 vs `1/sqrt(fan_in)`.\n",
    "\n",
    "### Solution\n",
    "Run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: fan-in scaling experiment\n",
    "\n",
    "fan_in = 10\n",
    "n = 1000\n",
    "m = 200\n",
    "g_local = torch.Generator().manual_seed(GLOBAL_SEED + 999)\n",
    "\n",
    "x = torch.randn((n, fan_in), generator=g_local, device=device)\n",
    "W = torch.randn((fan_in, m), generator=g_local, device=device)\n",
    "\n",
    "y1 = x @ W\n",
    "y2 = x @ (W / math.sqrt(fan_in))\n",
    "\n",
    "print(\"x std:\", x.std().item())\n",
    "print(\"W std:\", W.std().item())\n",
    "print(\"y std (no scaling):      \", y1.std().item())\n",
    "print(\"y std (1/sqrt(fan_in)):  \", y2.std().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10 — “Kaiming-ish” init for tanh: add a gain factor\n",
    "\n",
    "### Motivation / concept\n",
    "`tanh` is **contractive**: it tends to shrink variance (squash tails).\n",
    "So for tanh networks, people often use a multiplicative **gain**.\n",
    "\n",
    "In the transcript (and original notebook), a commonly used gain for tanh is:\n",
    "\n",
    "\\[\n",
    "\\text{gain} = 5/3\n",
    "\\]\n",
    "\n",
    "A simple recipe:\n",
    "\\[\n",
    "W \\sim \\mathcal{N}(0, \\sigma^2) \\quad\\text{with}\\quad \\sigma = \\frac{\\text{gain}}{\\sqrt{fan\\_in}}\n",
    "\\]\n",
    "\n",
    "For our first layer:\n",
    "- input dimension is `fan_in = n_embd * block_size`\n",
    "\n",
    "### Exercise\n",
    "Initialize `W1` with:\n",
    "- std = `(5/3)/sqrt(n_embd*block_size)`\n",
    "and compare saturation vs an unscaled W1.\n",
    "\n",
    "### Solution\n",
    "Run the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: tanh-friendly init using std = gain/sqrt(fan_in)\n",
    "\n",
    "def init_kaiming_tanh_params(vocab_size, n_embd=10, n_hidden=200, block_size=3, *,\n",
    "                             seed=GLOBAL_SEED, W2_scale=0.01, gain=5/3, device=device):\n",
    "    fan_in = n_embd * block_size\n",
    "    g_local = torch.Generator(device=\"cpu\").manual_seed(seed)\n",
    "\n",
    "    C  = torch.randn((vocab_size, n_embd), generator=g_local, device=device)\n",
    "    W1 = torch.randn((fan_in, n_hidden), generator=g_local, device=device) * (gain / math.sqrt(fan_in))\n",
    "    b1 = torch.randn((n_hidden,), generator=g_local, device=device) * 0.01  # tiny bias (optional)\n",
    "    W2 = torch.randn((n_hidden, vocab_size), generator=g_local, device=device) * W2_scale\n",
    "    b2 = torch.zeros((vocab_size,), device=device)\n",
    "    return MLPParams(C=C, W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "\n",
    "p_kaiming = init_kaiming_tanh_params(vocab_size=vocab_size, device=device)\n",
    "print(\"First-batch loss (kaiming-ish tanh init):\", forward_mlp(Xb, Yb, p_kaiming, block_size=block_size)[1].item())\n",
    "diagnose_tanh_saturation(Xb, p_kaiming)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 11 — Implement BatchNorm on hidden pre-activations (single-layer MLP)\n",
    "\n",
    "### Motivation / concept\n",
    "BatchNorm (BN) normalizes activations using the current minibatch statistics:\n",
    "\n",
    "For each hidden neuron dimension (over batch dimension):\n",
    "- compute mean and variance\n",
    "- normalize: \\(\\hat{x}=(x-\\mu) / \\sqrt{\\sigma^2 + \\epsilon}\\)\n",
    "- then apply learnable scale/shift: \\(y = \\gamma \\hat{x} + \\beta\\)\n",
    "\n",
    "Important: During **training**, BN uses batch statistics and updates running estimates.  \n",
    "During **inference**, BN uses running estimates (or calibrated estimates).\n",
    "\n",
    "### Exercise\n",
    "Add BN to the forward pass:\n",
    "- between `hpre` and `tanh`\n",
    "- with parameters: `bngain (gamma)`, `bnbias (beta)`\n",
    "- keep running mean/std estimates with EMA (exponential moving average)\n",
    "\n",
    "We will start with a **manual BN** (like the original notebook).\n",
    "\n",
    "### Solution\n",
    "Run the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: single-hidden-layer model with manual BatchNorm\n",
    "\n",
    "@dataclass\n",
    "class BNState:\n",
    "    gain: torch.Tensor        # (1, n_hidden)\n",
    "    bias: torch.Tensor        # (1, n_hidden)\n",
    "    mean_running: torch.Tensor  # (1, n_hidden)\n",
    "    std_running: torch.Tensor   # (1, n_hidden)\n",
    "    momentum: float = 0.001\n",
    "    eps: float = 1e-5\n",
    "\n",
    "def init_bn_state(n_hidden, device=device, momentum=0.001, eps=1e-5):\n",
    "    return BNState(\n",
    "        gain=torch.ones((1, n_hidden), device=device),\n",
    "        bias=torch.zeros((1, n_hidden), device=device),\n",
    "        mean_running=torch.zeros((1, n_hidden), device=device),\n",
    "        std_running=torch.ones((1, n_hidden), device=device),\n",
    "        momentum=momentum,\n",
    "        eps=eps\n",
    "    )\n",
    "\n",
    "def forward_mlp_with_bn(Xb, Yb, p: MLPParams, bn: BNState, *, training=True):\n",
    "    emb = p.C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpre = embcat @ p.W1  # NOTE: we omit b1 here because BN would subtract it out anyway (we'll discuss this)\n",
    "    # BN\n",
    "    if training:\n",
    "        bnmean = hpre.mean(0, keepdim=True)\n",
    "        bnstd = hpre.std(0, keepdim=True)\n",
    "        hpre_bn = bn.gain * (hpre - bnmean) / (bnstd + bn.eps) + bn.bias\n",
    "        with torch.no_grad():\n",
    "            bn.mean_running = (1 - bn.momentum) * bn.mean_running + bn.momentum * bnmean\n",
    "            bn.std_running  = (1 - bn.momentum) * bn.std_running  + bn.momentum * bnstd\n",
    "    else:\n",
    "        hpre_bn = bn.gain * (hpre - bn.mean_running) / (bn.std_running + bn.eps) + bn.bias\n",
    "\n",
    "    h = torch.tanh(hpre_bn)\n",
    "    logits = h @ p.W2 + p.b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    return logits, loss\n",
    "\n",
    "# quick check: forward pass works\n",
    "bn_state = init_bn_state(n_hidden=p_kaiming.W1.shape[1], device=device)\n",
    "logits_bn, loss_bn = forward_mlp_with_bn(Xb, Yb, p_kaiming, bn_state, training=True)\n",
    "loss_bn.item(), bn_state.mean_running.mean().item(), bn_state.std_running.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 12 — Training loop (single-layer MLP + BatchNorm)\n",
    "\n",
    "### Motivation / concept\n",
    "We’ll now train:\n",
    "- embedding table `C`\n",
    "- first layer `W1`\n",
    "- output layer `W2`, `b2`\n",
    "- BN parameters `gain`, `bias`\n",
    "Running stats are **not** trained by gradient descent.\n",
    "\n",
    "We’ll use:\n",
    "- minibatch training\n",
    "- SGD with learning rate decay (like the original notebook)\n",
    "\n",
    "### Exercise\n",
    "Train for a moderate number of steps and plot the loss curve.\n",
    "\n",
    "### Solution\n",
    "Run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: training loop for single-layer MLP + BN\n",
    "\n",
    "def sgd_train_single_layer_bn(\n",
    "    Xtr, Ytr, Xdev, Ydev, vocab_size,\n",
    "    *, n_embd=10, n_hidden=200, block_size=3,\n",
    "    steps=20_000, batch_size=32,\n",
    "    lr1=0.1, lr2=0.01, lr_decay_step=10_000,\n",
    "    seed=GLOBAL_SEED, device=device\n",
    "):\n",
    "    # init model params (tanh-friendly + sane output layer)\n",
    "    p = init_kaiming_tanh_params(vocab_size=vocab_size, n_embd=n_embd, n_hidden=n_hidden,\n",
    "                                 block_size=block_size, seed=seed, device=device, W2_scale=0.01)\n",
    "    bn = init_bn_state(n_hidden=n_hidden, device=device, momentum=0.001)\n",
    "\n",
    "    # learnable parameters list\n",
    "    params = [p.C, p.W1, p.W2, p.b2, bn.gain, bn.bias]\n",
    "    for t in params:\n",
    "        t.requires_grad = True\n",
    "\n",
    "    lossi = []\n",
    "    for i in range(steps):\n",
    "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "        Xb, Yb = Xtr[ix].to(device), Ytr[ix].to(device)\n",
    "\n",
    "        logits, loss = forward_mlp_with_bn(Xb, Yb, p, bn, training=True)\n",
    "\n",
    "        for t in params:\n",
    "            t.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        lr = lr1 if i < lr_decay_step else lr2\n",
    "        with torch.no_grad():\n",
    "            for t in params:\n",
    "                t -= lr * t.grad\n",
    "\n",
    "        lossi.append(loss.item())\n",
    "\n",
    "        if i % (steps // 5) == 0:\n",
    "            print(f\"step {i:>6}/{steps}: loss={loss.item():.4f}\")\n",
    "\n",
    "    # eval helper (BN in inference mode)\n",
    "    @torch.no_grad()\n",
    "    def split_loss(X, Y):\n",
    "        logits, loss = forward_mlp_with_bn(X.to(device), Y.to(device), p, bn, training=False)\n",
    "        return loss.item()\n",
    "\n",
    "    train_loss = split_loss(Xtr, Ytr)\n",
    "    dev_loss = split_loss(Xdev, Ydev)\n",
    "\n",
    "    return p, bn, lossi, train_loss, dev_loss\n",
    "\n",
    "p_trained, bn_trained, lossi, trL, devL = sgd_train_single_layer_bn(\n",
    "    Xtr, Ytr, Xdev, Ydev, vocab_size,\n",
    "    steps=10_000,  # increase if you want (e.g. 200_000 like the original notebook)\n",
    ")\n",
    "\n",
    "plt.plot(lossi)\n",
    "plt.title(\"Training loss (single-layer MLP + BN)\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Final train loss:\", trL)\n",
    "print(\"Final dev loss:  \", devL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 13 — Why we often remove the bias before BatchNorm\n",
    "\n",
    "### Motivation / concept\n",
    "If you compute:\n",
    "- `hpre = x @ W + b`,\n",
    "then BatchNorm immediately subtracts the batch mean:\n",
    "- `hpre - mean(hpre)`.\n",
    "\n",
    "The constant bias `b` shifts all examples equally, so it becomes part of the mean and gets subtracted away.  \n",
    "Result: that bias is **redundant** (often learns nothing).\n",
    "\n",
    "This is why you see `bias=False` in linear/conv layers right before BatchNorm.\n",
    "\n",
    "### Exercise\n",
    "Confirm empirically: try including a bias `b1` pre-BN and check whether its gradient becomes (near) zero.\n",
    "\n",
    "### Solution\n",
    "Run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: show that bias before BatchNorm is (usually) redundant\n",
    "\n",
    "# Make a tiny run and record gradients\n",
    "p_test = init_kaiming_tanh_params(vocab_size=vocab_size, device=device)\n",
    "# add a bias explicitly\n",
    "p_test = MLPParams(C=p_test.C, W1=p_test.W1, b1=torch.zeros(p_test.W1.shape[1], device=device, requires_grad=True),\n",
    "                   W2=p_test.W2, b2=p_test.b2)\n",
    "bn_test = init_bn_state(n_hidden=p_test.W1.shape[1], device=device)\n",
    "\n",
    "# make W1, W2 require grad too\n",
    "for t in [p_test.C, p_test.W1, p_test.W2, p_test.b2, bn_test.gain, bn_test.bias]:\n",
    "    t.requires_grad = True\n",
    "\n",
    "# forward with \"bias before BN\"\n",
    "def forward_with_bias_before_bn(Xb, Yb, p, bn):\n",
    "    emb = p.C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpre = embcat @ p.W1 + p.b1\n",
    "    bnmean = hpre.mean(0, keepdim=True)\n",
    "    bnstd = hpre.std(0, keepdim=True)\n",
    "    hpre_bn = bn.gain * (hpre - bnmean) / (bnstd + bn.eps) + bn.bias\n",
    "    h = torch.tanh(hpre_bn)\n",
    "    logits = h @ p.W2 + p.b2\n",
    "    return logits, F.cross_entropy(logits, Yb)\n",
    "\n",
    "logits, loss = forward_with_bias_before_bn(Xb, Yb, p_test, bn_test)\n",
    "for t in [p_test.C, p_test.W1, p_test.W2, p_test.b2, bn_test.gain, bn_test.bias, p_test.b1]:\n",
    "    if t.grad is not None:\n",
    "        t.grad = None\n",
    "loss.backward()\n",
    "print(\"||grad b1|| (mean abs):\", p_test.b1.grad.abs().mean().item())\n",
    "print(\"Explanation: BN subtracts the batch mean, so constant shifts typically cancel out.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 14 — “Calibrate” BatchNorm vs running stats (inference behavior)\n",
    "\n",
    "### Motivation / concept\n",
    "BatchNorm needs a strategy at inference time:\n",
    "- You can compute mean/std over the entire training set (“calibration”),\n",
    "- or use the running mean/std accumulated during training.\n",
    "\n",
    "Ideally, running stats approximate calibrated stats.\n",
    "\n",
    "### Exercise\n",
    "Compute calibrated `bnmean`, `bnstd` over the full train set and compare to `bn_trained.mean_running/std_running`.\n",
    "\n",
    "### Solution\n",
    "Run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: compare calibrated vs running stats\n",
    "\n",
    "@torch.no_grad()\n",
    "def calibrate_bn_stats_full_train(Xtr, p, bn_like, block_size=3):\n",
    "    emb = p.C[Xtr.to(device)]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hpre = embcat @ p.W1\n",
    "    mean = hpre.mean(0, keepdim=True)\n",
    "    std = hpre.std(0, keepdim=True)\n",
    "    return mean, std\n",
    "\n",
    "bnmean_cal, bnstd_cal = calibrate_bn_stats_full_train(Xtr, p_trained, bn_trained)\n",
    "\n",
    "# compare summary statistics\n",
    "print(\"Running mean (avg):     \", bn_trained.mean_running.mean().item())\n",
    "print(\"Calibrated mean (avg):  \", bnmean_cal.mean().item())\n",
    "print(\"Running std (avg):      \", bn_trained.std_running.mean().item())\n",
    "print(\"Calibrated std (avg):   \", bnstd_cal.mean().item())\n",
    "\n",
    "# compare typical elementwise error magnitudes\n",
    "print(\"Mean abs diff (mean):   \", (bn_trained.mean_running - bnmean_cal).abs().mean().item())\n",
    "print(\"Std abs diff (mean):    \", (bn_trained.std_running - bnstd_cal).abs().mean().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B — “PyTorchify”: build layers (Linear, BatchNorm1d, Tanh) and a deeper network\n",
    "\n",
    "We now switch to the **bonus section** of the transcript + companion notebook:\n",
    "- implement small “modules” like `nn.Module`,\n",
    "- stack them as Lego blocks,\n",
    "- inspect activation and gradient statistics through depth,\n",
    "- inspect update-to-data ratios.\n",
    "\n",
    "This is a great set of tools to debug training stability.\n",
    "\n",
    "---\n",
    "\n",
    "## Exercise 15 — Implement tiny `Linear`, `BatchNorm1d`, and `Tanh` modules\n",
    "\n",
    "### Motivation / concept\n",
    "We want a tiny version of the PyTorch module API:\n",
    "- `__call__` does forward pass and stores `self.out`\n",
    "- `parameters()` returns trainable tensors\n",
    "\n",
    "BatchNorm needs:\n",
    "- `training` flag\n",
    "- running mean/var buffers updated with exponential moving average\n",
    "- (gamma, beta) as trainable parameters\n",
    "\n",
    "### Exercise\n",
    "Implement the three tiny module classes and verify:\n",
    "- forward pass runs,\n",
    "- `parameters()` returns correct tensors.\n",
    "\n",
    "### Solution\n",
    "Run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: tiny modules\n",
    "\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, *, bias=True, generator=None, device=device):\n",
    "        if generator is None:\n",
    "            generator = torch.Generator().manual_seed(GLOBAL_SEED)\n",
    "        # default: variance-preserving init (std ~ 1/sqrt(fan_in))\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=generator, device=device) / math.sqrt(fan_in)\n",
    "        self.bias = torch.zeros((fan_out,), device=device) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out = self.out + self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1, *, device=device):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        # parameters\n",
    "        self.gamma = torch.ones((dim,), device=device)\n",
    "        self.beta = torch.zeros((dim,), device=device)\n",
    "        # buffers\n",
    "        self.running_mean = torch.zeros((dim,), device=device)\n",
    "        self.running_var = torch.ones((dim,), device=device)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True, unbiased=False)\n",
    "        else:\n",
    "            xmean = self.running_mean.view(1, -1)\n",
    "            xvar = self.running_var.view(1, -1)\n",
    "\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                # update buffers (EMA)\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean.view(-1)\n",
    "                self.running_var  = (1 - self.momentum) * self.running_var  + self.momentum * xvar.view(-1)\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "# quick test\n",
    "g_local = torch.Generator().manual_seed(GLOBAL_SEED + 2025)\n",
    "lin = Linear(5, 3, bias=False, generator=g_local)\n",
    "bn = BatchNorm1d(3, momentum=0.1)\n",
    "nonlin = Tanh()\n",
    "\n",
    "x = torch.randn((4,5), generator=g_local, device=device)\n",
    "y = nonlin(bn(lin(x)))\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"num params:\", sum(p.numel() for p in (lin.parameters()+bn.parameters()+nonlin.parameters())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 16 — Build a deep MLP and add diagnostic plots\n",
    "\n",
    "### Motivation / concept\n",
    "Deep MLPs can suffer from:\n",
    "- vanishing/exploding activations\n",
    "- vanishing/exploding gradients\n",
    "- uneven update magnitudes across layers\n",
    "\n",
    "We will track:\n",
    "1) **Activation distributions** at tanh layers:\n",
    "   - mean, std, and % saturation\n",
    "2) **Gradient distributions** at tanh layers:\n",
    "   - mean, std\n",
    "3) Weight gradient distributions\n",
    "4) **Update-to-data ratio** (log10), ideally around ~ `-3` as a rough heuristic\n",
    "\n",
    "### Exercise\n",
    "Build a deep model:\n",
    "- Embedding table `C`\n",
    "- 5× blocks of (Linear → BatchNorm → Tanh)\n",
    "- Final (Linear → BatchNorm) to vocab logits\n",
    "\n",
    "Run ~1000 steps and produce the plots.\n",
    "\n",
    "### Solution\n",
    "Run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: deep net + diagnostics (short run)\n",
    "\n",
    "# hyperparams (feel free to tweak)\n",
    "n_embd = 10\n",
    "n_hidden = 100\n",
    "gain_tanh = 1.0  # try 1.0, 5/3, 2.0, etc. BatchNorm makes this less critical.\n",
    "\n",
    "# re-init generator for determinism\n",
    "g_deep = torch.Generator().manual_seed(GLOBAL_SEED)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g_deep, device=device)\n",
    "\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False, generator=g_deep), BatchNorm1d(n_hidden, momentum=0.1), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False, generator=g_deep),           BatchNorm1d(n_hidden, momentum=0.1), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False, generator=g_deep),           BatchNorm1d(n_hidden, momentum=0.1), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False, generator=g_deep),           BatchNorm1d(n_hidden, momentum=0.1), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False, generator=g_deep),           BatchNorm1d(n_hidden, momentum=0.1), Tanh(),\n",
    "    Linear(n_hidden, vocab_size, bias=False, generator=g_deep),         BatchNorm1d(vocab_size, momentum=0.1)\n",
    "]\n",
    "\n",
    "# optional: scale weights by a \"gain\"\n",
    "with torch.no_grad():\n",
    "    for layer in layers:\n",
    "        if isinstance(layer, Linear):\n",
    "            layer.weight *= gain_tanh\n",
    "    # make last layer less confident at init: shrink gamma (BN scale) for final BN\n",
    "    layers[-1].gamma *= 0.1\n",
    "\n",
    "# collect parameters\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "def forward_deep(Xb):\n",
    "    emb = C[Xb]                   # (B, block_size, n_embd)\n",
    "    x = emb.view(emb.shape[0], -1) # (B, block_size*n_embd)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    return x  # logits\n",
    "\n",
    "# training (short debug run)\n",
    "max_steps = 1000\n",
    "batch_size = 32\n",
    "ud = []      # update/data ratio traces\n",
    "lossi = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g, device=device)\n",
    "    Xb, Yb = Xtr[ix].to(device), Ytr[ix].to(device)\n",
    "\n",
    "    logits = forward_deep(Xb)\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "    # retain grads on layer outputs for plotting\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, \"out\"):\n",
    "            layer.out.retain_grad()\n",
    "\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    lr = 0.1\n",
    "    with torch.no_grad():\n",
    "        # update-to-data ratio logs\n",
    "        ud.append([((lr * p.grad).std() / (p.data.std() + 1e-12)).log10().item() for p in parameters])\n",
    "        for p in parameters:\n",
    "            p.data -= lr * p.grad\n",
    "\n",
    "    lossi.append(loss.item())\n",
    "    if i % 200 == 0:\n",
    "        print(f\"step {i:>4}/{max_steps}: loss={loss.item():.4f}\")\n",
    "\n",
    "# ----- Diagnostics plots -----\n",
    "\n",
    "# 1) Activation distributions at tanh layers\n",
    "plt.figure(figsize=(16, 3))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]):  # ignore final BN\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out.detach()\n",
    "        sat = (t.abs() > 0.97).float().mean().item() * 100\n",
    "        print(f\"layer {i:2d} (Tanh): mean {t.mean():+.3f}, std {t.std():.3f}, saturated {sat:.2f}%\")\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].cpu(), hy.cpu())\n",
    "        legends.append(f\"layer {i}\")\n",
    "plt.legend(legends)\n",
    "plt.title(\"Activation distributions at tanh layers\")\n",
    "plt.show()\n",
    "\n",
    "# 2) Gradient distributions at tanh layers\n",
    "plt.figure(figsize=(16, 3))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers[:-1]):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out.grad.detach()\n",
    "        print(f\"layer {i:2d} (Tanh grad): mean {t.mean():+.3e}, std {t.std():.3e}\")\n",
    "        hy, hx = torch.histogram(t, density=True)\n",
    "        plt.plot(hx[:-1].cpu(), hy.cpu())\n",
    "        legends.append(f\"layer {i}\")\n",
    "plt.legend(legends)\n",
    "plt.title(\"Gradient distributions at tanh layers\")\n",
    "plt.show()\n",
    "\n",
    "# 3) Update-to-data ratio traces (weights only are most interesting, but we'll plot all 2D params)\n",
    "plt.figure(figsize=(16, 3))\n",
    "legends = []\n",
    "for pi, p in enumerate(parameters):\n",
    "    if p.ndim == 2:\n",
    "        plt.plot([ud_step[pi] for ud_step in ud])\n",
    "        legends.append(f\"param {pi} {tuple(p.shape)}\")\n",
    "plt.plot([0, len(ud)], [-3, -3], \"k\")  # heuristic line\n",
    "plt.legend(legends, fontsize=8)\n",
    "plt.title(\"log10(update std / weight std) over time (2D params only)\")\n",
    "plt.show()\n",
    "\n",
    "# loss curve\n",
    "plt.plot(lossi)\n",
    "plt.title(\"Debug training loss (1000 steps)\")\n",
    "plt.xlabel(\"step\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 17 — Sampling: generate names from the trained model\n",
    "\n",
    "### Motivation / concept\n",
    "Sampling does:\n",
    "1) start with context `...` (all dots)\n",
    "2) repeatedly:\n",
    "   - forward pass → logits\n",
    "   - softmax to probabilities\n",
    "   - sample next character index\n",
    "   - shift context window\n",
    "   - stop when `'.'` is sampled\n",
    "\n",
    "### Exercise\n",
    "Implement `sample_name(num_samples=20)` for the deep model above.\n",
    "Make sure to set BatchNorm layers to inference mode (`training=False`).\n",
    "\n",
    "### Solution\n",
    "Run the cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Solution: sampling from deep net\n",
    "\n",
    "@torch.no_grad()\n",
    "def set_eval_mode(layers):\n",
    "    for layer in layers:\n",
    "        if hasattr(layer, \"training\"):\n",
    "            layer.training = False\n",
    "\n",
    "@torch.no_grad()\n",
    "def sample_names(num_samples=20, *, block_size=3, generator_seed=GLOBAL_SEED+10):\n",
    "    set_eval_mode(layers)\n",
    "    g_samp = torch.Generator(device=\"cpu\").manual_seed(generator_seed)\n",
    "    for _ in range(num_samples):\n",
    "        out = []\n",
    "        context = [0] * block_size\n",
    "        while True:\n",
    "            X = torch.tensor([context], dtype=torch.long, device=device)\n",
    "            logits = forward_deep(X)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            ix = torch.multinomial(probs, num_samples=1, generator=g_samp).item()\n",
    "            context = context[1:] + [ix]\n",
    "            out.append(ix)\n",
    "            if ix == 0:\n",
    "                break\n",
    "        print(\"\".join(itos[i] for i in out))\n",
    "\n",
    "sample_names(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-up\n",
    "\n",
    "If you worked through the exercises, you should now understand:\n",
    "\n",
    "- Why **expected init loss** is about `log(vocab_size)` and why huge loss at step 0 signals overconfident logits\n",
    "- How to diagnose & fix:\n",
    "  - **logit scale** (output layer init)\n",
    "  - **tanh saturation** (hidden layer init)\n",
    "- The intuition behind **fan-in scaling** and **gain**\n",
    "- BatchNorm mechanics:\n",
    "  - per-batch mean/variance\n",
    "  - learnable `gamma`/`beta`\n",
    "  - running mean/var and inference behavior\n",
    "  - why bias before BN is typically redundant\n",
    "- How to “PyTorchify” a network into modules\n",
    "- How to use diagnostic plots for:\n",
    "  - activations, gradients, weight gradients\n",
    "  - update-to-data ratios over time\n",
    "\n",
    "Next conceptual step (as the transcript hints): RNNs/GRUs/LSTMs become *very deep* when unrolled in time, so all of these stability ideas matter even more.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}