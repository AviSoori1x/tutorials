{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Initialization and Training Diagnostics: A Step-by-Step Tutorial\n",
    "\n",
    "This tutorial will guide you through understanding:\n",
    "- Why proper initialization matters\n",
    "- How to diagnose activation and gradient problems\n",
    "- What batch normalization does and how it helps\n",
    "- How to build modular neural networks\n",
    "\n",
    "**Learning Approach**: Each section follows the pattern:\n",
    "1. **Motivation** - Why does this matter?\n",
    "2. **Exercise** - Try it yourself\n",
    "3. **Solution** - Check your understanding\n",
    "\n",
    "Let's start from the basics and build up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the words\n",
    "words = open('names.txt', 'r').read().splitlines()\n",
    "print(f\"Total words: {len(words)}\")\n",
    "print(f\"First 8 words: {words[:8]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabulary\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Characters: {''.join(itos[i] for i in range(1, vocab_size))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build dataset\n",
    "block_size = 3  # context length\n",
    "\n",
    "def build_dataset(words):  \n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n",
    "print(f\"Training set: {Xtr.shape[0]} examples\")\n",
    "print(f\"Dev set: {Xdev.shape[0]} examples\")\n",
    "print(f\"Test set: {Xte.shape[0]} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Understanding Loss at Initialization\n",
    "\n",
    "### üéØ Motivation\n",
    "\n",
    "When we start training a neural network, we should have a rough idea of what loss to expect. This is like checking if your car starts before driving - if something is wrong at the start, it will cause problems later.\n",
    "\n",
    "For a character-level language model:\n",
    "- We have 27 possible characters (a-z + '.')\n",
    "- At initialization, the network knows nothing\n",
    "- It should predict each character with equal probability: 1/27\n",
    "- The loss (negative log likelihood) should be: -log(1/27) ‚âà 3.29\n",
    "\n",
    "**If your initial loss is much higher (like 27!), something is very wrong.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 1.1: Calculate Expected Initial Loss\n",
    "\n",
    "**Task**: Calculate what the loss should be at initialization if the model predicts uniformly.\n",
    "\n",
    "Hints:\n",
    "- Probability for any character: 1/27\n",
    "- Loss = -log(probability)\n",
    "- Use `torch.tensor()` and `.log()` for calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Calculate the expected loss at initialization\n",
    "prob = # ?\n",
    "expected_loss = # ?\n",
    "print(f\"Expected loss at initialization: {expected_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "prob = torch.tensor(1/27)\n",
    "expected_loss = -prob.log()\n",
    "print(f\"Expected loss at initialization: {expected_loss:.4f}\")\n",
    "print(f\"\\nExplanation: If the model assigns equal probability (1/27 = 0.037) to each character,\")\n",
    "print(f\"then the negative log probability is {expected_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 1.2: Create a Poorly Initialized Network\n",
    "\n",
    "**Task**: Create a simple MLP and observe what happens with naive initialization.\n",
    "\n",
    "Network architecture:\n",
    "- Embedding: 27 chars ‚Üí 10 dimensions\n",
    "- Hidden layer: 30 inputs (10 * 3 context) ‚Üí 200 neurons\n",
    "- Output layer: 200 ‚Üí 27 logits\n",
    "\n",
    "Use `torch.randn()` for all weights/biases (this is WRONG on purpose!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# Initialize parameters (poorly!)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = # ?\n",
    "b1 = # ?\n",
    "W2 = # ?\n",
    "b2 = # ?\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f\"Total parameters: {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "n_embd = 10\n",
    "n_hidden = 200\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden, generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g)\n",
    "b2 = torch.randn(vocab_size, generator=g)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f\"Total parameters: {sum(p.nelement() for p in parameters)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 1.3: Observe the Initial Loss Problem\n",
    "\n",
    "**Task**: Do a forward pass on a single batch and check the initial loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "batch_size = 32\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "# Forward pass\n",
    "emb = # ?\n",
    "embcat = # ?\n",
    "h = # ?\n",
    "logits = # ?\n",
    "loss = # ?\n",
    "\n",
    "print(f\"Initial loss: {loss.item():.4f}\")\n",
    "print(f\"Expected loss: ~3.29\")\n",
    "print(f\"\\nProblem: Loss is way too {'high' if loss > 10 else 'close'}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "batch_size = 32\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "h = torch.tanh(embcat @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "print(f\"Initial loss: {loss.item():.4f}\")\n",
    "print(f\"Expected loss: ~3.29\")\n",
    "print(f\"\\nüîç Analysis:\")\n",
    "print(f\"Sample logits: {logits[0, :5]}\")\n",
    "print(f\"\\nThe logits have extreme values! This makes the softmax very confident (and wrong).\")\n",
    "print(f\"This is like a student who confidently gives wrong answers on every question.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 1.4: Fix the Output Layer\n",
    "\n",
    "**Task**: Fix the initialization of W2 and b2 so logits are close to zero at initialization.\n",
    "\n",
    "Hints:\n",
    "- Multiply W2 by a small number (try 0.01)\n",
    "- Set b2 to zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden, generator=g)\n",
    "W2 = # ? Fix this\n",
    "b2 = # ? Fix this\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Test it\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "h = torch.tanh(embcat @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "print(f\"Initial loss: {loss.item():.4f}\")\n",
    "print(f\"Expected: ~3.29\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden, generator=g)\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01  # Scale down!\n",
    "b2 = torch.zeros(vocab_size)  # No bias needed\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "h = torch.tanh(embcat @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "print(f\"Initial loss: {loss.item():.4f}\")\n",
    "print(f\"Expected: ~3.29\")\n",
    "print(f\"\\n‚úÖ Much better! Now the network starts from a reasonable place.\")\n",
    "print(f\"Sample logits: {logits[0, :5]}\")\n",
    "print(f\"These are close to zero, so softmax gives ~uniform probabilities.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Understanding Tanh Saturation\n",
    "\n",
    "### üéØ Motivation\n",
    "\n",
    "The tanh activation function squashes inputs to the range [-1, 1].\n",
    "\n",
    "**The Problem:**\n",
    "- If inputs to tanh are very large (positive or negative), outputs get stuck at ¬±1\n",
    "- This is called \"saturation\"\n",
    "- Saturated neurons have zero gradient ‚Üí they don't learn!\n",
    "\n",
    "**Why it matters:**\n",
    "- Gradient of tanh: (1 - tanh¬≤(x))\n",
    "- If tanh(x) = 1, then gradient = 0\n",
    "- If tanh(x) = -1, then gradient = 0\n",
    "- No gradient = no learning = \"dead neuron\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 2.1: Visualize Tanh and Its Gradient\n",
    "\n",
    "**Task**: Plot tanh(x) and its gradient to understand saturation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "y = # ? Apply tanh\n",
    "grad = # ? Calculate gradient: 1 - y^2\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "# Plot tanh\n",
    "plt.subplot(1, 2, 2)\n",
    "# Plot gradient\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "x = torch.linspace(-5, 5, 100)\n",
    "y = torch.tanh(x)\n",
    "grad = 1 - y**2\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, y)\n",
    "plt.axhline(y=1, color='r', linestyle='--', alpha=0.3)\n",
    "plt.axhline(y=-1, color='r', linestyle='--', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('tanh(x) - The Activation')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('tanh(x)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, grad)\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.title('Gradient: 1 - tanh¬≤(x)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('gradient')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Key Observations:\")\n",
    "print(\"1. When |x| > 3, tanh(x) ‚âà ¬±1 (saturated)\")\n",
    "print(\"2. When saturated, gradient ‚âà 0 (no learning!)\")\n",
    "print(\"3. Maximum gradient is at x=0 (gradient = 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 2.2: Check Tanh Saturation in Our Network\n",
    "\n",
    "**Task**: Calculate the pre-activations (before tanh) and check how many neurons are saturated.\n",
    "\n",
    "A neuron is \"saturated\" if |tanh(x)| > 0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Use the poorly initialized network from before\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden, generator=g)\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "h_preact = # ? Calculate pre-activation\n",
    "h = # ? Apply tanh\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(h_preact.detach().flatten().numpy(), bins=50)\n",
    "plt.title('Pre-activation distribution')\n",
    "plt.xlabel('Value')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(h.detach().flatten().numpy(), bins=50)\n",
    "plt.title('Post-activation (tanh) distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.show()\n",
    "\n",
    "# Calculate saturation\n",
    "saturated = # ?\n",
    "print(f\"Percentage of saturated neurons: {saturated * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g)\n",
    "b1 = torch.randn(n_hidden, generator=g)\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "h_preact = embcat @ W1 + b1\n",
    "h = torch.tanh(h_preact)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(h_preact.detach().flatten().numpy(), bins=50)\n",
    "plt.title('Pre-activation distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.axvline(x=-3, color='r', linestyle='--', label='Saturation zone')\n",
    "plt.axvline(x=3, color='r', linestyle='--')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(h.detach().flatten().numpy(), bins=50, range=(-1, 1))\n",
    "plt.title('Post-activation (tanh) distribution')\n",
    "plt.xlabel('Value')\n",
    "plt.axvline(x=-0.97, color='r', linestyle='--', label='Saturated')\n",
    "plt.axvline(x=0.97, color='r', linestyle='--')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "saturated = (h.abs() > 0.97).float().mean()\n",
    "print(f\"Percentage of saturated neurons: {saturated * 100:.2f}%\")\n",
    "print(f\"\\n‚ö†Ô∏è Problem: Many neurons are saturated!\")\n",
    "print(f\"Pre-activations range: [{h_preact.min():.2f}, {h_preact.max():.2f}]\")\n",
    "print(f\"These are too extreme ‚Üí tanh saturates ‚Üí gradients vanish\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 2.3: Fix Tanh Saturation\n",
    "\n",
    "**Task**: Scale down W1 and b1 so pre-activations are closer to zero.\n",
    "\n",
    "Try multiplying W1 and b1 by 0.2 (or experiment with other values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = # ? Fix this\n",
    "b1 = # ? Fix this\n",
    "\n",
    "# Test it\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "h_preact = embcat @ W1 + b1\n",
    "h = torch.tanh(h_preact)\n",
    "\n",
    "saturated = (h.abs() > 0.97).float().mean()\n",
    "print(f\"Percentage of saturated neurons: {saturated * 100:.2f}%\")\n",
    "print(f\"Pre-activations range: [{h_preact.min():.2f}, {h_preact.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * 0.2\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.01\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "h_preact = embcat @ W1 + b1\n",
    "h = torch.tanh(h_preact)\n",
    "\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(h_preact.detach().flatten().numpy(), bins=50)\n",
    "plt.title('Pre-activation distribution (Fixed!)')\n",
    "plt.xlabel('Value')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(h.detach().flatten().numpy(), bins=50, range=(-1, 1))\n",
    "plt.title('Post-activation distribution (Fixed!)')\n",
    "plt.xlabel('Value')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "saturated = (h.abs() > 0.97).float().mean()\n",
    "print(f\"Percentage of saturated neurons: {saturated * 100:.2f}%\")\n",
    "print(f\"Pre-activations range: [{h_preact.min():.2f}, {h_preact.max():.2f}]\")\n",
    "print(f\"\\n‚úÖ Much better! Neurons are now active and can learn.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Kaiming Initialization\n",
    "\n",
    "### üéØ Motivation\n",
    "\n",
    "We've been guessing numbers (0.2, 0.01, etc.) to fix initialization. This is not scalable!\n",
    "\n",
    "**Kaiming Initialization** (He et al., 2015) provides a principled way to initialize weights:\n",
    "- Goal: Keep activations at unit Gaussian (mean=0, std=1) throughout the network\n",
    "- Key insight: If inputs have std=1, outputs will too if we scale weights by 1/‚àö(fan_in)\n",
    "\n",
    "**For different activations:**\n",
    "- Linear (no activation): gain = 1.0\n",
    "- ReLU: gain = ‚àö2 (because ReLU kills half the neurons)\n",
    "- Tanh: gain = 5/3 (because tanh is contractive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 3.1: Understand Fan-in Normalization\n",
    "\n",
    "**Task**: Demonstrate why we need to divide by ‚àö(fan_in)\n",
    "\n",
    "Create:\n",
    "1. Random input x with 1000 examples, 10 features (mean=0, std=1)\n",
    "2. Random weight W with 10 inputs, 200 outputs\n",
    "3. Calculate y = x @ W\n",
    "4. Check the std of y with different weight scalings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "fan_in = 10\n",
    "fan_out = 200\n",
    "\n",
    "x = torch.randn(1000, fan_in)\n",
    "print(f\"Input x: mean={x.mean():.4f}, std={x.std():.4f}\")\n",
    "\n",
    "# Test 1: No scaling\n",
    "W = torch.randn(fan_in, fan_out)\n",
    "y = x @ W\n",
    "print(f\"\\nNo scaling: output std = {y.std():.4f}\")\n",
    "\n",
    "# Test 2: Scale by fan_in\n",
    "W = torch.randn(fan_in, fan_out) / fan_in\n",
    "y = x @ W\n",
    "print(f\"Scale by fan_in: output std = {y.std():.4f}\")\n",
    "\n",
    "# Test 3: Scale by sqrt(fan_in) - CORRECT!\n",
    "W = # ?\n",
    "y = # ?\n",
    "print(f\"Scale by sqrt(fan_in): output std = {y.std():.4f} ‚Üê Should be ~1.0!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "fan_in = 10\n",
    "fan_out = 200\n",
    "\n",
    "x = torch.randn(1000, fan_in)\n",
    "print(f\"Input x: mean={x.mean():.4f}, std={x.std():.4f}\")\n",
    "\n",
    "# Test 1: No scaling\n",
    "W = torch.randn(fan_in, fan_out)\n",
    "y = x @ W\n",
    "print(f\"\\nNo scaling: output std = {y.std():.4f} ‚ùå (too large!)\")\n",
    "\n",
    "# Test 2: Scale by fan_in\n",
    "W = torch.randn(fan_in, fan_out) / fan_in\n",
    "y = x @ W\n",
    "print(f\"Scale by fan_in: output std = {y.std():.4f} ‚ùå (too small!)\")\n",
    "\n",
    "# Test 3: Scale by sqrt(fan_in) - CORRECT!\n",
    "W = torch.randn(fan_in, fan_out) / fan_in**0.5\n",
    "y = x @ W\n",
    "print(f\"Scale by sqrt(fan_in): output std = {y.std():.4f} ‚úÖ (perfect!)\")\n",
    "\n",
    "print(f\"\\nüìä Explanation:\")\n",
    "print(f\"When we multiply x by W, we're summing {fan_in} random products.\")\n",
    "print(f\"Variance adds up: var(sum) = sum(var) = {fan_in} * var\")\n",
    "print(f\"So std grows by sqrt({fan_in}) = {fan_in**0.5:.2f}\")\n",
    "print(f\"To compensate, divide by sqrt({fan_in})!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 3.2: Apply Kaiming Initialization\n",
    "\n",
    "**Task**: Initialize the network using Kaiming initialization with the correct gain for tanh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "fan_in = n_embd * block_size  # = 30\n",
    "gain = # ? (for tanh, gain = 5/3)\n",
    "W1 = # ? Apply Kaiming init\n",
    "b1 = # ? Small or zero\n",
    "\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01\n",
    "b2 = torch.zeros(vocab_size)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Test\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "h_preact = embcat @ W1 + b1\n",
    "h = torch.tanh(h_preact)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "print(f\"Pre-activation std: {h_preact.std():.4f} (should be ~1.0)\")\n",
    "print(f\"Activation std: {h.std():.4f} (should be ~0.6-0.7 for tanh)\")\n",
    "print(f\"Saturation: {(h.abs() > 0.97).float().mean() * 100:.2f}%\")\n",
    "print(f\"Initial loss: {loss.item():.4f} (should be ~3.29)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "fan_in = n_embd * block_size\n",
    "gain = 5/3  # For tanh\n",
    "W1 = torch.randn((fan_in, n_hidden), generator=g) * gain / (fan_in ** 0.5)\n",
    "b1 = torch.zeros(n_hidden)  # Can also use small random values\n",
    "\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01\n",
    "b2 = torch.zeros(vocab_size)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "h_preact = embcat @ W1 + b1\n",
    "h = torch.tanh(h_preact)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "print(f\"Pre-activation std: {h_preact.std():.4f} (should be ~1.0) ‚úÖ\")\n",
    "print(f\"Activation std: {h.std():.4f} (should be ~0.6-0.7 for tanh) ‚úÖ\")\n",
    "print(f\"Saturation: {(h.abs() > 0.97).float().mean() * 100:.2f}% ‚úÖ\")\n",
    "print(f\"Initial loss: {loss.item():.4f} (should be ~3.29) ‚úÖ\")\n",
    "\n",
    "print(f\"\\nüéâ Perfect initialization! The network is ready to train.\")\n",
    "print(f\"\\nFormula: W = torch.randn(...) * (gain / sqrt(fan_in))\")\n",
    "print(f\"Where: fan_in = {fan_in}, gain = {gain:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Batch Normalization\n",
    "\n",
    "### üéØ Motivation\n",
    "\n",
    "Even with Kaiming initialization, deep networks are hard to train. Activations can still drift during training.\n",
    "\n",
    "**Batch Normalization** (2015): If we want activations to be Gaussian, why not just... make them Gaussian?\n",
    "\n",
    "**The trick:**\n",
    "1. Normalize: Make activations mean=0, std=1\n",
    "2. Scale & Shift: Let the network learn the best distribution via Œ≥ (gain) and Œ≤ (bias)\n",
    "\n",
    "**Benefits:**\n",
    "- Stabilizes training\n",
    "- Allows higher learning rates\n",
    "- Reduces dependence on initialization\n",
    "\n",
    "**Downside:**\n",
    "- Couples examples in a batch (weird!)\n",
    "- Different behavior in train vs eval mode\n",
    "- Can be source of bugs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 4.1: Implement Basic Batch Normalization\n",
    "\n",
    "**Task**: Implement the forward pass of batch normalization.\n",
    "\n",
    "Steps:\n",
    "1. Calculate mean and std across the batch dimension\n",
    "2. Normalize: (x - mean) / std\n",
    "3. Scale and shift: Œ≥ * x_normalized + Œ≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Create some random pre-activations\n",
    "h_preact = torch.randn(32, 200) * 3  # 32 examples, 200 neurons, large values\n",
    "\n",
    "print(f\"Before BatchNorm:\")\n",
    "print(f\"Mean: {h_preact.mean(0)[:5]}\")\n",
    "print(f\"Std: {h_preact.std(0)[:5]}\")\n",
    "\n",
    "# BatchNorm parameters\n",
    "bn_gain = torch.ones((1, 200))\n",
    "bn_bias = torch.zeros((1, 200))\n",
    "\n",
    "# TODO: Implement BatchNorm\n",
    "bn_mean = # ? Calculate mean\n",
    "bn_std = # ? Calculate std\n",
    "h_normalized = # ? Normalize\n",
    "h_bn = # ? Scale and shift\n",
    "\n",
    "print(f\"\\nAfter BatchNorm:\")\n",
    "print(f\"Mean: {h_bn.mean(0)[:5]}\")\n",
    "print(f\"Std: {h_bn.std(0)[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "h_preact = torch.randn(32, 200) * 3\n",
    "\n",
    "print(f\"Before BatchNorm:\")\n",
    "print(f\"Mean: {h_preact.mean(0)[:5]}\")\n",
    "print(f\"Std: {h_preact.std(0)[:5]}\")\n",
    "\n",
    "bn_gain = torch.ones((1, 200))\n",
    "bn_bias = torch.zeros((1, 200))\n",
    "\n",
    "# BatchNorm forward pass\n",
    "bn_mean = h_preact.mean(0, keepdim=True)\n",
    "bn_std = h_preact.std(0, keepdim=True)\n",
    "h_normalized = (h_preact - bn_mean) / bn_std\n",
    "h_bn = bn_gain * h_normalized + bn_bias\n",
    "\n",
    "print(f\"\\nAfter BatchNorm:\")\n",
    "print(f\"Mean: {h_bn.mean(0)[:5]} ‚Üê Should be ~0\")\n",
    "print(f\"Std: {h_bn.std(0)[:5]} ‚Üê Should be ~1\")\n",
    "\n",
    "print(f\"\\n‚úÖ Each neuron now has mean‚âà0, std‚âà1!\")\n",
    "print(f\"\\nüîë Key insight: We normalized across the BATCH dimension (dim=0)\")\n",
    "print(f\"   Shape: (32 examples, 200 neurons)\")\n",
    "print(f\"   For each neuron, we computed mean/std over all 32 examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 4.2: Add BatchNorm to the Network\n",
    "\n",
    "**Task**: Integrate batch normalization into the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) / (n_embd * block_size)**0.5\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01\n",
    "b2 = torch.zeros(vocab_size)\n",
    "\n",
    "# BatchNorm parameters\n",
    "bn_gain = # ?\n",
    "bn_bias = # ?\n",
    "\n",
    "parameters = [C, W1, W2, b2, bn_gain, bn_bias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f\"Total parameters: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "# Forward pass with BatchNorm\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "h_preact = embcat @ W1\n",
    "\n",
    "# TODO: Apply BatchNorm here\n",
    "bn_mean = # ?\n",
    "bn_std = # ?\n",
    "h_preact = # ? Normalize, scale, shift\n",
    "\n",
    "h = torch.tanh(h_preact)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "print(f\"\\nInitial loss: {loss.item():.4f}\")\n",
    "print(f\"Saturation: {(h.abs() > 0.97).float().mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) / (n_embd * block_size)**0.5\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01\n",
    "b2 = torch.zeros(vocab_size)\n",
    "\n",
    "bn_gain = torch.ones((1, n_hidden))\n",
    "bn_bias = torch.zeros((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bn_gain, bn_bias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "print(f\"Total parameters: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "emb = C[Xb]\n",
    "embcat = emb.view(emb.shape[0], -1)\n",
    "h_preact = embcat @ W1\n",
    "\n",
    "# BatchNorm layer\n",
    "bn_mean = h_preact.mean(0, keepdim=True)\n",
    "bn_std = h_preact.std(0, keepdim=True)\n",
    "h_preact = bn_gain * (h_preact - bn_mean) / bn_std + bn_bias\n",
    "\n",
    "h = torch.tanh(h_preact)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "print(f\"\\nInitial loss: {loss.item():.4f} ‚úÖ\")\n",
    "print(f\"Saturation: {(h.abs() > 0.97).float().mean() * 100:.2f}% ‚úÖ\")\n",
    "print(f\"\\nüéâ BatchNorm ensures good activations regardless of W1 initialization!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 4.3: Running Statistics for Inference\n",
    "\n",
    "**Task**: During training, we normalize using batch statistics. But at inference, we might have just one example!\n",
    "\n",
    "Solution: Keep running estimates of mean and std during training.\n",
    "\n",
    "Implement the running mean update: `running = 0.999 * running + 0.001 * current`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "# Initialize network (same as before)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) / (n_embd * block_size)**0.5\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01\n",
    "b2 = torch.zeros(vocab_size)\n",
    "bn_gain = torch.ones((1, n_hidden))\n",
    "bn_bias = torch.zeros((1, n_hidden))\n",
    "\n",
    "# Running statistics (buffers, not parameters!)\n",
    "bn_mean_running = # ?\n",
    "bn_std_running = # ?\n",
    "\n",
    "parameters = [C, W1, W2, b2, bn_gain, bn_bias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Simulate training for a few steps\n",
    "for i in range(100):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    h_preact = embcat @ W1\n",
    "    \n",
    "    # Batch statistics\n",
    "    bn_mean_i = h_preact.mean(0, keepdim=True)\n",
    "    bn_std_i = h_preact.std(0, keepdim=True)\n",
    "    \n",
    "    # TODO: Update running statistics\n",
    "    with torch.no_grad():\n",
    "        bn_mean_running = # ?\n",
    "        bn_std_running = # ?\n",
    "    \n",
    "    # Use batch statistics for this forward pass\n",
    "    h_preact = bn_gain * (h_preact - bn_mean_i) / bn_std_i + bn_bias\n",
    "    h = torch.tanh(h_preact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "print(f\"Running mean (first 5): {bn_mean_running[0, :5]}\")\n",
    "print(f\"Running std (first 5): {bn_std_running[0, :5]}\")\n",
    "print(f\"\\nThese will be used at inference time!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) / (n_embd * block_size)**0.5\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.01\n",
    "b2 = torch.zeros(vocab_size)\n",
    "bn_gain = torch.ones((1, n_hidden))\n",
    "bn_bias = torch.zeros((1, n_hidden))\n",
    "\n",
    "bn_mean_running = torch.zeros((1, n_hidden))\n",
    "bn_std_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bn_gain, bn_bias]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "momentum = 0.001\n",
    "for i in range(1000):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    emb = C[Xb]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    h_preact = embcat @ W1\n",
    "    \n",
    "    bn_mean_i = h_preact.mean(0, keepdim=True)\n",
    "    bn_std_i = h_preact.std(0, keepdim=True)\n",
    "    \n",
    "    # Update running statistics (exponential moving average)\n",
    "    with torch.no_grad():\n",
    "        bn_mean_running = (1 - momentum) * bn_mean_running + momentum * bn_mean_i\n",
    "        bn_std_running = (1 - momentum) * bn_std_running + momentum * bn_std_i\n",
    "    \n",
    "    h_preact = bn_gain * (h_preact - bn_mean_i) / bn_std_i + bn_bias\n",
    "    h = torch.tanh(h_preact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "\n",
    "print(f\"Running mean (first 5): {bn_mean_running[0, :5]}\")\n",
    "print(f\"Running std (first 5): {bn_std_running[0, :5]}\")\n",
    "print(f\"\\n‚úÖ These running statistics approximate the true mean/std over the entire dataset.\")\n",
    "print(f\"At inference, use these instead of batch statistics!\")\n",
    "print(f\"\\nüîë Key: with torch.no_grad() prevents building gradients for this update\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Building Modular Networks (PyTorch Style)\n",
    "\n",
    "### üéØ Motivation\n",
    "\n",
    "Writing neural networks with explicit matrix multiplications gets messy. Let's create reusable modules!\n",
    "\n",
    "**We'll build:**\n",
    "1. `Linear` - A linear layer (like `nn.Linear`)\n",
    "2. `BatchNorm1d` - Batch normalization (like `nn.BatchNorm1d`)\n",
    "3. `Tanh` - Tanh activation (like `nn.Tanh`)\n",
    "\n",
    "Then stack them like Lego blocks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 5.1: Create a Linear Layer Class\n",
    "\n",
    "**Task**: Implement a `Linear` class that:\n",
    "- Takes fan_in and fan_out in `__init__`\n",
    "- Initializes weights using Kaiming init\n",
    "- Implements forward pass in `__call__`\n",
    "- Returns parameters in `parameters()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        # TODO: Initialize weight and bias\n",
    "        self.weight = # ?\n",
    "        self.bias = # ?\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        self.out = # ?\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        # TODO: Return list of parameters\n",
    "        return # ?\n",
    "\n",
    "# Test it\n",
    "layer = Linear(10, 20)\n",
    "x = torch.randn(5, 10)\n",
    "y = layer(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.nelement() for p in layer.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "class Linear:\n",
    "    def __init__(self, fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "# Test\n",
    "layer = Linear(10, 20)\n",
    "x = torch.randn(5, 10)\n",
    "y = layer(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {y.shape}\")\n",
    "print(f\"Number of parameters: {sum(p.nelement() for p in layer.parameters())}\")\n",
    "print(f\"Expected: 10*20 + 20 = {10*20 + 20} ‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 5.2: Create BatchNorm1d Class\n",
    "\n",
    "**Task**: Implement `BatchNorm1d` with training and eval modes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        \n",
    "        # TODO: Initialize parameters and buffers\n",
    "        self.gamma = # ? (trainable)\n",
    "        self.beta = # ? (trainable)\n",
    "        self.running_mean = # ? (buffer)\n",
    "        self.running_var = # ? (buffer)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # TODO: Implement forward pass\n",
    "        if self.training:\n",
    "            xmean = # ?\n",
    "            xvar = # ?\n",
    "        else:\n",
    "            xmean = # ?\n",
    "            xvar = # ?\n",
    "        \n",
    "        xhat = # ? Normalize\n",
    "        self.out = # ? Scale and shift\n",
    "        \n",
    "        # TODO: Update running statistics\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = # ?\n",
    "                self.running_var = # ?\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "# Test it\n",
    "bn = BatchNorm1d(10)\n",
    "x = torch.randn(32, 10) * 3\n",
    "y = bn(x)\n",
    "print(f\"Input mean: {x.mean(0)[0]:.4f}, std: {x.std(0)[0]:.4f}\")\n",
    "print(f\"Output mean: {y.mean(0)[0]:.4f}, std: {y.std(0)[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        \n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * xvar\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "bn = BatchNorm1d(10)\n",
    "x = torch.randn(32, 10) * 3\n",
    "y = bn(x)\n",
    "print(f\"Input mean: {x.mean(0)[0]:.4f}, std: {x.std(0)[0]:.4f}\")\n",
    "print(f\"Output mean: {y.mean(0)[0]:.4f}, std: {y.std(0)[0]:.4f}\")\n",
    "print(f\"\\n‚úÖ Output is normalized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 5.3: Build a Deep Network\n",
    "\n",
    "**Task**: Stack multiple layers to create a deep network.\n",
    "\n",
    "Architecture:\n",
    "- Linear(30, 100) ‚Üí BatchNorm ‚Üí Tanh\n",
    "- Linear(100, 100) ‚Üí BatchNorm ‚Üí Tanh\n",
    "- Linear(100, 27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "layers = [\n",
    "    # TODO: Build the layer stack\n",
    "]\n",
    "\n",
    "# Initialize last layer to be less confident\n",
    "with torch.no_grad():\n",
    "    layers[-1].weight *= 0.1\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(f\"Total parameters: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Test forward pass\n",
    "ix = torch.randint(0, Xtr.shape[0], (32,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "emb = C[Xb]\n",
    "x = emb.view(emb.shape[0], -1)\n",
    "for layer in layers:\n",
    "    x = layer(x)\n",
    "loss = F.cross_entropy(x, Yb)\n",
    "\n",
    "print(f\"Initial loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 5.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "n_embd = 10\n",
    "n_hidden = 100\n",
    "\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    layers[-1].gamma *= 0.1\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "print(f\"Total parameters: {sum(p.nelement() for p in parameters)}\")\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "ix = torch.randint(0, Xtr.shape[0], (32,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "emb = C[Xb]\n",
    "x = emb.view(emb.shape[0], -1)\n",
    "for layer in layers:\n",
    "    x = layer(x)\n",
    "loss = F.cross_entropy(x, Yb)\n",
    "\n",
    "print(f\"Initial loss: {loss.item():.4f}\")\n",
    "print(f\"\\n‚úÖ We've built a modular, deep neural network!\")\n",
    "print(f\"üéâ This is how PyTorch works under the hood!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Diagnostic Visualizations\n",
    "\n",
    "### üéØ Motivation\n",
    "\n",
    "How do you know if your network is training well? Look at the statistics!\n",
    "\n",
    "**What to monitor:**\n",
    "1. **Activation distributions** - Are neurons saturated?\n",
    "2. **Gradient distributions** - Are gradients flowing?\n",
    "3. **Update-to-parameter ratios** - Is learning rate correct?\n",
    "\n",
    "**Rules of thumb:**\n",
    "- Activations should have std ‚âà 0.5-1.0\n",
    "- Gradients should be roughly equal across layers\n",
    "- Update/parameter ratio should be ‚âà 10‚Åª¬≥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 6.1: Visualize Activation Distributions\n",
    "\n",
    "**Task**: Plot histograms of activations after each Tanh layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Do a forward pass\n",
    "ix = torch.randint(0, Xtr.shape[0], (32,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "emb = C[Xb]\n",
    "x = emb.view(emb.shape[0], -1)\n",
    "for layer in layers:\n",
    "    x = layer(x)\n",
    "\n",
    "# TODO: Create visualization\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out\n",
    "        # Calculate stats\n",
    "        mean = # ?\n",
    "        std = # ?\n",
    "        saturated = # ? (|t| > 0.97)\n",
    "        \n",
    "        print(f\"Layer {i}: mean={mean:.2f}, std={std:.2f}, saturated={saturated*100:.2f}%\")\n",
    "        \n",
    "        # Plot histogram\n",
    "        # ?\n",
    "\n",
    "plt.legend(legends)\n",
    "plt.title('Activation Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 6.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "ix = torch.randint(0, Xtr.shape[0], (32,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "\n",
    "emb = C[Xb]\n",
    "x = emb.view(emb.shape[0], -1)\n",
    "for layer in layers:\n",
    "    x = layer(x)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "legends = []\n",
    "for i, layer in enumerate(layers):\n",
    "    if isinstance(layer, Tanh):\n",
    "        t = layer.out\n",
    "        mean = t.mean()\n",
    "        std = t.std()\n",
    "        saturated = (t.abs() > 0.97).float().mean()\n",
    "        \n",
    "        print(f\"Layer {i} ({layer.__class__.__name__}): mean={mean:.2f}, std={std:.2f}, saturated={saturated*100:.2f}%\")\n",
    "        \n",
    "        hy, hx = torch.histogram(t.detach(), bins=50, density=True)\n",
    "        plt.plot(hx[:-1], hy)\n",
    "        legends.append(f\"Layer {i}\")\n",
    "\n",
    "plt.legend(legends)\n",
    "plt.title('Activation Distribution')\n",
    "plt.xlabel('Activation value')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ All layers have similar, healthy distributions!\")\n",
    "print(f\"This is thanks to BatchNorm keeping activations normalized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Exercise 6.2: Monitor Training\n",
    "\n",
    "**Task**: Train for a few steps and track update-to-parameter ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "max_steps = 1000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []  # update to data ratios\n",
    "\n",
    "for i in range(max_steps):\n",
    "    # Minibatch\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    # Forward pass\n",
    "    emb = C[Xb]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "    \n",
    "    # Backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update\n",
    "    lr = 0.1\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    # Track stats\n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        # TODO: Calculate update to data ratio\n",
    "        ud.append([((lr * p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lossi)\n",
    "plt.title('Training Loss')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for i, p in enumerate(parameters):\n",
    "    if p.ndim == 2:\n",
    "        plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k', label='target: 10‚Åª¬≥')\n",
    "plt.legend()\n",
    "plt.title('Update/Parameter Ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ Solution 6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "# Reinitialize to start fresh\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "layers = [\n",
    "    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\n",
    "    Linear(n_hidden, vocab_size, bias=False), BatchNorm1d(vocab_size),\n",
    "]\n",
    "with torch.no_grad():\n",
    "    layers[-1].gamma *= 0.1\n",
    "\n",
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "max_steps = 1000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "ud = []\n",
    "\n",
    "for i in range(max_steps):\n",
    "    ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = Xtr[ix], Ytr[ix]\n",
    "    \n",
    "    emb = C[Xb]\n",
    "    x = emb.view(emb.shape[0], -1)\n",
    "    for layer in layers:\n",
    "        x = layer(x)\n",
    "    loss = F.cross_entropy(x, Yb)\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = 0.1\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    lossi.append(loss.log10().item())\n",
    "    with torch.no_grad():\n",
    "        ud.append([((lr * p.grad).std() / p.data.std()).log10().item() for p in parameters])\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(lossi)\n",
    "plt.title('Training Loss (log scale)')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('log10(loss)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "legends = []\n",
    "for i, p in enumerate(parameters):\n",
    "    if p.ndim == 2:\n",
    "        plt.plot([ud[j][i] for j in range(len(ud))])\n",
    "        legends.append(f'param {i}')\n",
    "plt.plot([0, len(ud)], [-3, -3], 'k', linewidth=2)\n",
    "legends.append('target: 10‚Åª¬≥')\n",
    "plt.legend(legends)\n",
    "plt.title('Update/Parameter Ratio (log scale)')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('log10(update/param)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Key observations:\")\n",
    "print(f\"1. Loss is decreasing smoothly (learning is happening!)\")\n",
    "print(f\"2. Update ratios are near 10‚Åª¬≥ (learning rate is good!)\")\n",
    "print(f\"3. All layers updating at similar rates (balanced training!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## üéì Final Summary\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "**1. Initialization Matters**\n",
    "- Initial loss should match theoretical expectation\n",
    "- Bad init ‚Üí wasted training time or failure to learn\n",
    "- Use Kaiming init: `weight = randn(...) * gain / sqrt(fan_in)`\n",
    "\n",
    "**2. Activation Statistics**\n",
    "- Monitor saturation (too many values at extremes)\n",
    "- Keep activations in \"active\" range of nonlinearity\n",
    "- Tanh: keep std ‚âà 0.6, avoid |tanh| > 0.97\n",
    "\n",
    "**3. Batch Normalization**\n",
    "- Forces activations to be Gaussian (mean=0, std=1)\n",
    "- Makes training more stable and less sensitive to init\n",
    "- Requires running statistics for inference\n",
    "- Different behavior in train vs eval mode!\n",
    "\n",
    "**4. Modular Design**\n",
    "- Build reusable layer classes\n",
    "- Stack them like Lego blocks\n",
    "- This is how PyTorch works!\n",
    "\n",
    "**5. Training Diagnostics**\n",
    "- Plot activation distributions\n",
    "- Plot gradient distributions\n",
    "- Monitor update/parameter ratios\n",
    "- Target: ratio ‚âà 10‚Åª¬≥\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "```python\n",
    "# Good initialization template\n",
    "W = torch.randn(fan_in, fan_out) * gain / fan_in**0.5\n",
    "b = torch.zeros(fan_out)  # or small random\n",
    "\n",
    "# Monitor these during training:\n",
    "- activation.mean(), activation.std()\n",
    "- (activation.abs() > threshold).mean()  # saturation\n",
    "- (lr * grad).std() / param.std()  # update ratio\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Try training to convergence\n",
    "2. Experiment with different architectures (deeper, wider)\n",
    "3. Try different activation functions (ReLU, GELU)\n",
    "4. Explore other normalization techniques (LayerNorm, GroupNorm)\n",
    "5. Move on to RNNs, LSTMs, and Transformers!\n",
    "\n",
    "**You now understand the foundations of modern deep learning! üéâ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
