{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "920a895a",
   "metadata": {},
   "source": [
    "\n",
    "# Building **micrograd** from scratch – exercise notebook\n",
    "\n",
    "This is a **self‑contained** walkthrough of a tiny automatic‑differentiation engine and a tiny neural‑network library, inspired by Andrej Karpathy’s *micrograd*.\n",
    "\n",
    "You do **not** need to watch the video to follow this. We’ll build everything from scratch, step by step:\n",
    "\n",
    "- a `Value` class that wraps scalars and tracks gradients,\n",
    "- backpropagation (`.backward()`) over a computation graph,\n",
    "- `Neuron`, `Layer`, and `MLP` classes on top of `Value`,\n",
    "- a small training loop that fits a toy dataset.\n",
    "\n",
    "Each section has:\n",
    "\n",
    "1. **Concept / motivation** – what we’re about to build and why.\n",
    "2. **Exercise cell** – with `### YOUR CODE HERE` and `raise NotImplementedError(...)`.\n",
    "3. **Solution cell** – a reference implementation.\n",
    "\n",
    "Try the exercise first, then check the solution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ba0abf",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 0 – Warm‑up: derivatives as \"sensitivity\"\n",
    "\n",
    "Key idea: a derivative measures **how sensitive** a function’s output is to its input.\n",
    "\n",
    "For a function\n",
    "\n",
    "\\[\n",
    "f(x) = 3x^2 - 4x + 5\n",
    "\\]\n",
    "\n",
    "the derivative at a point \\(x\\) answers:\n",
    "\n",
    "> If I nudge \\(x\\) by a tiny amount, how much does \\(f(x)\\) change?\n",
    "\n",
    "On a graph of \\(y = f(x)\\), the derivative is the **slope** of the curve at that point: rise over run.\n",
    "\n",
    "On a computer we can approximate this slope using **finite differences**:\n",
    "\n",
    "\\[\n",
    "f'(x) \\approx \\frac{f(x + h) - f(x)}{h}\n",
    "\\]\n",
    "\n",
    "for a small step size \\(h\\) (for example `1e-6`). Smaller `h` makes the approximation closer to the true derivative (up to floating‑point limits).\n",
    "\n",
    "This is exactly the sort of information gradients will give us later for giant neural‑network expressions.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Implement `numerical_derivative(f, x)` using the finite‑difference formula.\n",
    "2. Test it on \\(f(x) = 3x^2 - 4x + 5\\), and compare with the exact derivative \\(f'(x) = 6x - 4\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c657e189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "def f(x):\n",
    "    # f(x) = 3x^2 - 4x + 5\n",
    "    return 3 * x**2 - 4 * x + 5\n",
    "\n",
    "def numerical_derivative(f, x, h=1e-6):\n",
    "    \"\"\"\n",
    "    Approximate f'(x) at point x using finite differences.\n",
    "\n",
    "    Replace the 'raise' line with your implementation.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    raise NotImplementedError(\"Exercise 0: implement numerical_derivative\")  # noqa: TRY003\n",
    "\n",
    "\n",
    "# After implementing, you can test like this:\n",
    "# x0 = 3.0\n",
    "# approx = numerical_derivative(f, x0)\n",
    "# exact = 6 * x0 - 4\n",
    "# print(\"approx:\", approx, \" exact:\", exact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d8c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 0 – numerical derivative\n",
    "\n",
    "import math\n",
    "\n",
    "def f(x):\n",
    "    return 3 * x**2 - 4 * x + 5\n",
    "\n",
    "def numerical_derivative(f, x, h=1e-6):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "x0 = 3.0\n",
    "approx = numerical_derivative(f, x0)\n",
    "exact = 6 * x0 - 4\n",
    "\n",
    "print(f\"x = {x0}\")\n",
    "print(f\"numerical derivative ~ {approx:.6f}\")\n",
    "print(f\"exact derivative     = {exact:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c519f5",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 1 – Smart numbers: the `Value` class\n",
    "\n",
    "Plain Python numbers like `3.0` or `-2.5` know nothing about:\n",
    "\n",
    "- where they came from (which computation),\n",
    "- how a final output depends on them (their gradient).\n",
    "\n",
    "We want a **smart scalar** that will eventually:\n",
    "\n",
    "- store its numeric value,\n",
    "- store its gradient (derivative of some final loss w.r.t. this value),\n",
    "- remember how it was computed (parents + operation).\n",
    "\n",
    "We’ll call this smart scalar `Value`.\n",
    "\n",
    "For now, we just need it to:\n",
    "\n",
    "- hold `data` (a float),\n",
    "- hold `grad` (initially `0.0`),\n",
    "- print nicely so we can debug easily.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdfef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 1 – your turn: minimal Value\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Create a Value object that wraps a Python float.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 1: implement __init__\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return a helpful string representation, e.g.\n",
    "        Value(data=2.0, grad=0.0)\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 1: implement __repr__\")\n",
    "\n",
    "\n",
    "# After implementing, try:\n",
    "# v = Value(2.5)\n",
    "# v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 1 – minimal Value\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data):\n",
    "        # numeric value\n",
    "        self.data = float(data)\n",
    "        # gradient of some final scalar output w.r.t. this Value\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "\n",
    "Value(2.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed31b6",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 2 – Using `+` and `*` with `Value` (operator overloading)\n",
    "\n",
    "We’d like to write natural math with our `Value` objects:\n",
    "\n",
    "```python\n",
    "a = Value(-4.0)\n",
    "b = Value(2.0)\n",
    "c = a + b      # should be -2.0\n",
    "d = a * b      # should be -8.0\n",
    "```\n",
    "\n",
    "In Python, `a + b` calls the method `a.__add__(b)`, and `a * b` calls `a.__mul__(b)`.\n",
    "\n",
    "We will:\n",
    "\n",
    "- implement `__add__` and `__mul__` so they return new `Value` objects,\n",
    "- implement `__radd__` and `__rmul__` so `2 + a` and `3 * a` also work.\n",
    "\n",
    "For now we **only** care about computing the correct numeric result. We’ll track graph structure in the next step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76353a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 2 – your turn: add + and *\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data):\n",
    "        self.data = float(data)\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"\n",
    "        Return a new Value with data = self.data + other.data.\n",
    "        If other is a plain number, wrap it as Value(other).\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 2: implement __add__\")\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        \"\"\"\n",
    "        Called when Python evaluates: number + Value.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 2: implement __radd__\")\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"\n",
    "        Return a new Value with data = self.data * other.data.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 2: implement __mul__\")\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        \"\"\"\n",
    "        Called when Python evaluates: number * Value.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 2: implement __rmul__\")\n",
    "\n",
    "\n",
    "# After implementing, try:\n",
    "# a = Value(-4.0)\n",
    "# b = Value(2.0)\n",
    "# print(\"a + b =\", a + b)\n",
    "# print(\"a * b =\", a * b)\n",
    "# print(\"2 + a =\", 2 + a)\n",
    "# print(\"3 * a =\", 3 * a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 2 – addition and multiplication\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data):\n",
    "        self.data = float(data)\n",
    "        self.grad = 0.0\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _lift(x):\n",
    "        # Make sure x is a Value\n",
    "        return x if isinstance(x, Value) else Value(x)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        return Value(self.data + other.data)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        # addition is commutative\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        return Value(self.data * other.data)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        # multiplication is commutative\n",
    "        return self * other\n",
    "\n",
    "\n",
    "a = Value(-4.0)\n",
    "b = Value(2.0)\n",
    "print(\"a + b =\", a + b)\n",
    "print(\"a * b =\", a * b)\n",
    "print(\"2 + a =\", 2 + a)\n",
    "print(\"3 * a =\", 3 * a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d4307d",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 3 – Building a computation graph\n",
    "\n",
    "Backpropagation needs to know **how each value was computed**.\n",
    "\n",
    "We’ll treat each `Value` as a node in a **computation graph**:\n",
    "\n",
    "- Inputs you create directly are leaf nodes.\n",
    "- Results of operations like `+` or `*` are internal nodes.\n",
    "- If `z = x + y`, then `x` and `y` are **parents** of `z`.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "a = Value(-4.0)\n",
    "b = Value(2.0)\n",
    "c = a + b\n",
    "d = a * b\n",
    "e = c + d\n",
    "```\n",
    "\n",
    "Graph sketch:\n",
    "\n",
    "```\n",
    "   a      b\n",
    "   | \\  / |\n",
    "   |  \\/  |\n",
    "   |  /\\  |\n",
    "   c    d\n",
    "     \\ /\n",
    "      e\n",
    "```\n",
    "\n",
    "We’ll give each `Value` three extra fields:\n",
    "\n",
    "- `self._prev`: the set of parent nodes (inputs to this operation),\n",
    "- `self._op`: a string describing the operation (`\"+\"`, `\"*\"`, etc.),\n",
    "- `self._backward`: a function that later will know how to push gradients to the parents.\n",
    "\n",
    "### Task\n",
    "\n",
    "Extend `Value` to:\n",
    "\n",
    "- accept `_children` and `_op` in the constructor,\n",
    "- update `__add__` and `__mul__` so the **output** node remembers its parents and operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 3 – your turn: add graph info\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\"):\n",
    "        self.data = float(data)\n",
    "        self.grad = 0.0\n",
    "\n",
    "        # graph information\n",
    "        self._prev = set(_children)  # parents in the graph\n",
    "        self._op = _op               # operation that produced this node\n",
    "        self._backward = lambda: None\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _lift(x):\n",
    "        return x if isinstance(x, Value) else Value(x)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        # TODO: create out, remember parents and op \"+\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 3: implement __add__ with graph info\")\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        # TODO: create out, remember parents and op \"*\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 3: implement __mul__ with graph info\")\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "\n",
    "# After implementing, try:\n",
    "# a = Value(-4.0)\n",
    "# b = Value(2.0)\n",
    "# c = a + b\n",
    "# d = a * b + b\n",
    "# print(\"d:\", d)\n",
    "# print(\"d._op:\", d._op)\n",
    "# print(\"parents of d:\", d._prev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e88af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 3 – Value as a graph node\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = float(data)\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self._backward = lambda: None\n",
    "        self.label = label  # optional name for debugging\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _lift(x):\n",
    "        return x if isinstance(x, Value) else Value(x)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "\n",
    "a = Value(-4.0, label=\"a\")\n",
    "b = Value(2.0, label=\"b\")\n",
    "c = a + b\n",
    "d = a * b\n",
    "e = c + d\n",
    "\n",
    "print(\"e:\", e)\n",
    "print(\"e._op:\", e._op)\n",
    "print(\"e._prev:\", {p.label for p in e._prev})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd49d68",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 4 – Backpropagation: local gradients and `.backward()`\n",
    "\n",
    "Now we want to automatically fill `grad` for all nodes when we call:\n",
    "\n",
    "```python\n",
    "loss.backward()\n",
    "```\n",
    "\n",
    "where `loss` is some final scalar `Value` in the graph. After this call, each node `v` should hold:\n",
    "\n",
    "\\[\n",
    "v.grad = \\frac{\\partial \\text{loss}}{\\partial v}\n",
    "\\]\n",
    "\n",
    "So `v.grad` tells us how sensitive the final loss is to small changes in `v`.\n",
    "\n",
    "### Local gradients\n",
    "\n",
    "Take two basic operations:\n",
    "\n",
    "- Addition: `out = x + y`\n",
    "  - `dout/dx = 1`\n",
    "  - `dout/dy = 1`\n",
    "\n",
    "- Multiplication: `out = x * y`\n",
    "  - `dout/dx = y`\n",
    "  - `dout/dy = x`\n",
    "\n",
    "If we already know `out.grad = d(loss)/d(out)`, then by the chain rule:\n",
    "\n",
    "```text\n",
    "d(loss)/d(x) = d(loss)/d(out) * d(out)/d(x)\n",
    "```\n",
    "\n",
    "So in code:\n",
    "\n",
    "- For addition:\n",
    "\n",
    "```python\n",
    "x.grad += 1.0 * out.grad\n",
    "y.grad += 1.0 * out.grad\n",
    "```\n",
    "\n",
    "- For multiplication:\n",
    "\n",
    "```python\n",
    "x.grad += y.data * out.grad\n",
    "y.grad += x.data * out.grad\n",
    "```\n",
    "\n",
    "We’ll store this logic in `out._backward()`, a tiny function that knows how to **send gradients to the parents**.\n",
    "\n",
    "### Global backward\n",
    "\n",
    "To propagate gradients through the whole graph, `backward()` will:\n",
    "\n",
    "1. Build a list of all nodes reachable from `self` in **topological order** (parents before children).\n",
    "2. Set `self.grad = 1.0` (because d(loss)/d(loss) = 1).\n",
    "3. Go through the list **in reverse order**, calling `v._backward()` at each node.\n",
    "\n",
    "That reverse order ensures that when we process a node, all nodes that depend on it have already passed their gradients back.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. In `__add__` and `__mul__`, define `out._backward()` using the local rules above.\n",
    "2. Implement `backward(self)` that:\n",
    "   - builds a topological list of nodes,\n",
    "   - seeds `self.grad = 1.0`,\n",
    "   - walks the list in reverse and calls each node’s `_backward()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace0eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 4 – your turn: implement backprop for + and *\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = float(data)\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self._backward = lambda: None\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _lift(x):\n",
    "        return x if isinstance(x, Value) else Value(x)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            # TODO: use local derivative for addition\n",
    "            ### YOUR CODE HERE\n",
    "            raise NotImplementedError(\"Exercise 4: implement _backward for +\")\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            # TODO: use local derivative for multiplication\n",
    "            ### YOUR CODE HERE\n",
    "            raise NotImplementedError(\"Exercise 4: implement _backward for *\")\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Backpropagate gradients from this node through the graph.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 4: implement backward()\")\n",
    "\n",
    "\n",
    "# After implementing, test with:\n",
    "# a = Value(2.0, label=\"a\")\n",
    "# b = Value(3.0, label=\"b\")\n",
    "# c = a * b\n",
    "# c.backward()\n",
    "# print(\"dc/da (should be 3):\", a.grad)\n",
    "# print(\"dc/db (should be 2):\", b.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6392be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 4 – backprop for + and *\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = float(data)\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self._backward = lambda: None\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _lift(x):\n",
    "        return x if isinstance(x, Value) else Value(x)\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            # d(out)/d(self) = 1, d(out)/d(other) = 1\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            # d(out)/d(self) = other.data, d(out)/d(other) = self.data\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def backward(self):\n",
    "        # 1) build topological order\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build(self)\n",
    "\n",
    "        # 2) zero all grads\n",
    "        for v in topo:\n",
    "            v.grad = 0.0\n",
    "\n",
    "        # 3) seed gradient at this node\n",
    "        self.grad = 1.0\n",
    "\n",
    "        # 4) backprop in reverse topological order\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "\n",
    "# Test: simple multiplication\n",
    "a = Value(2.0, label=\"a\")\n",
    "b = Value(3.0, label=\"b\")\n",
    "c = a * b\n",
    "c.backward()\n",
    "print(\"c =\", c)\n",
    "print(\"dc/da (should be 3):\", a.grad)\n",
    "print(\"dc/db (should be 2):\", b.grad)\n",
    "\n",
    "# Test: node used twice, b = a + a\n",
    "a = Value(3.0, label=\"a\")\n",
    "b = a + a\n",
    "b.backward()\n",
    "print(\"\\nTest b = a + a\")\n",
    "print(\"b =\", b)\n",
    "print(\"db/da (should be 2):\", a.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1b4da",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 5 – More math ops on `Value`\n",
    "\n",
    "Real neural nets use more than `+` and `*`. We’d like to support:\n",
    "\n",
    "- unary minus: `-x`\n",
    "- subtraction: `x - y`, `y - x`\n",
    "- division: `x / y`, `y / x`\n",
    "- powers: `x ** k` for scalar `k`\n",
    "- nonlinearities: `tanh()` and `relu()`\n",
    "- `exp()`\n",
    "\n",
    "The pattern is always the same:\n",
    "\n",
    "1. Implement the **forward** computation.\n",
    "2. Figure out the **local derivative** (from calculus or intuition).\n",
    "3. In `_backward`, multiply `out.grad` by this local derivative and add to `self.grad`.\n",
    "\n",
    "Some handy derivatives:\n",
    "\n",
    "- `d(-x)/dx = -1`\n",
    "- `d(x - y)/dx = 1`, `d(x - y)/dy = -1`\n",
    "- `d(x**k)/dx = k * x**(k-1)`\n",
    "- `d(tanh(x))/dx = 1 - tanh(x)**2`\n",
    "- `d(ReLU(x))/dx = 1 if x > 0 else 0`\n",
    "- `d(exp(x))/dx = exp(x)`\n",
    "\n",
    "Division can be rewritten using powers:\n",
    "\n",
    "\\[\n",
    "\\frac{x}{y} = x \\cdot y^{-1}\n",
    "\\]\n",
    "\n",
    "Below is the full `Value` implementation with these ops. Read through slowly and make sure each `_backward` matches the derivative rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a0f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 5 – full Value engine with many scalar operations\n",
    "\n",
    "import math\n",
    "\n",
    "class Value:\n",
    "    def __init__(self, data, _children=(), _op=\"\", label=\"\"):\n",
    "        self.data = float(data)\n",
    "        self.grad = 0.0\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self._backward = lambda: None\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _lift(x):\n",
    "        return x if isinstance(x, Value) else Value(x)\n",
    "\n",
    "    # ----- basic arithmetic -----\n",
    "    def __add__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        out = Value(self.data + other.data, (self, other), \"+\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += 1.0 * out.grad\n",
    "            other.grad += 1.0 * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        out = Value(self.data * other.data, (self, other), \"*\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    # ----- unary minus and subtraction -----\n",
    "    def __neg__(self):\n",
    "        # -x is just (-1) * x\n",
    "        return self * -1.0\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        return other + (-self)\n",
    "\n",
    "    # ----- division -----\n",
    "    def __truediv__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        return self * (other ** -1.0)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        other = Value._lift(other)\n",
    "        return other * (self ** -1.0)\n",
    "\n",
    "    # ----- powers -----\n",
    "    def __pow__(self, exponent):\n",
    "        assert isinstance(exponent, (int, float)), \"only supports int/float powers\"\n",
    "        out = Value(self.data ** exponent, (self,), f\"**{exponent}\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += exponent * (self.data ** (exponent - 1)) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # ----- nonlinearities -----\n",
    "    def tanh(self):\n",
    "        x = self.data\n",
    "        t = (math.exp(2 * x) - 1) / (math.exp(2 * x) + 1)\n",
    "        out = Value(t, (self,), \"tanh\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1.0 - t**2) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        x = self.data\n",
    "        out = Value(x if x > 0 else 0.0, (self,), \"ReLU\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1.0 if x > 0 else 0.0) * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    def exp(self):\n",
    "        x = self.data\n",
    "        e = math.exp(x)\n",
    "        out = Value(e, (self,), \"exp\")\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.data * out.grad\n",
    "\n",
    "        out._backward = _backward\n",
    "        return out\n",
    "\n",
    "    # ----- backward engine -----\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build(self)\n",
    "\n",
    "        # zero grads\n",
    "        for v in topo:\n",
    "            v.grad = 0.0\n",
    "\n",
    "        # seed gradient\n",
    "        self.grad = 1.0\n",
    "\n",
    "        # backprop\n",
    "        for v in reversed(topo):\n",
    "            v._backward()\n",
    "\n",
    "\n",
    "# Small stress test (adapted from micrograd README)\n",
    "a = Value(-4.0, label=\"a\")\n",
    "b = Value(2.0, label=\"b\")\n",
    "c = a + b\n",
    "d = a * b + b**3\n",
    "c = c + c + 1\n",
    "c = c + 1 + c + (-a)\n",
    "d = d + d * 2 + (b + a).relu()\n",
    "d = d + 3 * d + (b - a).relu()\n",
    "e = c - d\n",
    "f = e**2\n",
    "g = f / 2.0\n",
    "g = g + 10.0 / f\n",
    "\n",
    "print(\"g.data =\", g.data)\n",
    "g.backward()\n",
    "print(\"a.grad =\", a.grad)\n",
    "print(\"b.grad =\", b.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad9f6b",
   "metadata": {},
   "source": [
    "\n",
    "### (Optional) Inspecting the computation graph\n",
    "\n",
    "This helper prints a simple textual view of the graph starting from a root node. It can help you see how everything is wired together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeac26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dump_graph(root):\n",
    "    nodes = []\n",
    "    visited = set()\n",
    "\n",
    "    def build(v):\n",
    "        if v not in visited:\n",
    "            visited.add(v)\n",
    "            for child in v._prev:\n",
    "                build(child)\n",
    "            nodes.append(v)\n",
    "\n",
    "    build(root)\n",
    "\n",
    "    for v in nodes:\n",
    "        parent_labels = [p.label or \"?\" for p in v._prev]\n",
    "        print(f\"node {v.label or '?'}: op={v._op!r}, data={v.data}, grad={v.grad}, parents={parent_labels}\")\n",
    "\n",
    "\n",
    "dump_graph(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347a88be",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 6 – From scalars to a single neuron\n",
    "\n",
    "Now we use our scalar engine to build neural‑network parts.\n",
    "\n",
    "A single fully connected neuron computes\n",
    "\n",
    "\\[\n",
    "\\text{out} = \\phi(w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b)\n",
    "\\]\n",
    "\n",
    "- \\(x_i\\): inputs (we’ll use `Value` objects)\n",
    "- \\(w_i\\): weights (trainable `Value` parameters)\n",
    "- \\(b\\): bias (trainable `Value`)\n",
    "- \\(\\phi\\): nonlinearity (ReLU here)\n",
    "\n",
    "Since all operations are on `Value`s (`*`, `+`, `relu`), the graph + gradients “just work”.\n",
    "\n",
    "### `Module`\n",
    "\n",
    "We also define a tiny base class `Module` so all neural‑network objects can:\n",
    "\n",
    "- return their parameters via `parameters()`,\n",
    "- zero their gradients via `zero_grad()`.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Implement `Module.parameters` (base version can just return `[]`).\n",
    "2. Implement `Neuron.__call__`:\n",
    "   - compute `act = sum(w_i * x_i) + b`,\n",
    "   - if `self.nonlin` is `True`, return `act.relu()`, else return `act`.\n",
    "3. Implement `Neuron.parameters` to return all weights plus bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b80c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 6 – your turn: Module and Neuron\n",
    "\n",
    "import random\n",
    "\n",
    "class Module:\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return a flat list of all Value parameters.\n",
    "\n",
    "        Base class: no parameters.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 6: implement Module.parameters\")\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "\n",
    "class Neuron(Module):\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        \"\"\"\n",
    "        nin : number of inputs\n",
    "        nonlin : whether to apply ReLU nonlinearity\n",
    "        \"\"\"\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(0.0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        x : list of Value inputs\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 6: implement Neuron.__call__\")\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"\n",
    "        Return all parameters (weights and bias) as a list.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 6: implement Neuron.parameters\")\n",
    "\n",
    "\n",
    "# After implementing, test with:\n",
    "# n = Neuron(3)\n",
    "# x = [Value(1.0), Value(-2.0), Value(0.5)]\n",
    "# out = n(x)\n",
    "# print(\"neuron output:\", out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 6 – Module and Neuron\n",
    "\n",
    "import random\n",
    "\n",
    "class Module:\n",
    "    def parameters(self):\n",
    "        # Base module: no parameters\n",
    "        return []\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p in self.parameters():\n",
    "            p.grad = 0.0\n",
    "\n",
    "\n",
    "class Neuron(Module):\n",
    "    def __init__(self, nin, nonlin=True):\n",
    "        self.w = [Value(random.uniform(-1, 1)) for _ in range(nin)]\n",
    "        self.b = Value(0.0)\n",
    "        self.nonlin = nonlin\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # weighted sum: w·x + b\n",
    "        act = sum((wi * xi for wi, xi in zip(self.w, x)), self.b)\n",
    "        # nonlinearity\n",
    "        return act.relu() if self.nonlin else act\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.w + [self.b]\n",
    "\n",
    "\n",
    "# Test a neuron\n",
    "n = Neuron(3)\n",
    "x = [Value(1.0), Value(-2.0), Value(0.5)]\n",
    "out = n(x)\n",
    "print(\"neuron output:\", out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6ed255",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 7 – Layers and multi‑layer perceptrons (MLPs)\n",
    "\n",
    "Neural networks stack many neurons in **layers**, and then stack layers into a **multi‑layer perceptron** (MLP).\n",
    "\n",
    "### Layer\n",
    "\n",
    "A `Layer`:\n",
    "\n",
    "- holds a list of `Neuron`s,\n",
    "- on `__call__`, applies each neuron to the same input vector,\n",
    "- returns a list of outputs (or a single `Value` if there is only one neuron),\n",
    "- exposes all its parameters via `parameters()`.\n",
    "\n",
    "### MLP\n",
    "\n",
    "An `MLP` is a sequence of layers. For example:\n",
    "\n",
    "```python\n",
    "mlp = MLP(3, [4, 4, 1])\n",
    "```\n",
    "\n",
    "- input dimension is 3,\n",
    "- first hidden layer has 4 neurons,\n",
    "- second hidden layer has 4 neurons,\n",
    "- output layer has 1 neuron.\n",
    "\n",
    "On `__call__`, the MLP just feeds the output of one layer into the next.\n",
    "\n",
    "### Task\n",
    "\n",
    "1. Implement `Layer.__call__` and `Layer.parameters()`.\n",
    "2. Implement `MLP.__call__` (looping through layers) and `MLP.parameters()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7cc106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Exercise 7 – your turn: Layer and MLP\n",
    "\n",
    "class Layer(Module):\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        \"\"\"\n",
    "        nin : number of inputs\n",
    "        nout : number of neurons in this layer\n",
    "        kwargs : forwarded to Neuron (e.g. nonlin=True/False)\n",
    "        \"\"\"\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # apply each neuron to the same input vector\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        # if there is only 1 neuron, return its single output\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 7: implement Layer.__call__\")\n",
    "\n",
    "    def parameters(self):\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 7: implement Layer.parameters\")\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts):\n",
    "        \"\"\"\n",
    "        nin : input dimension\n",
    "        nouts : list of layer sizes, e.g. [4, 4, 1]\n",
    "        \"\"\"\n",
    "        sizes = [nin] + nouts\n",
    "        self.layers = [\n",
    "            Layer(sizes[i], sizes[i+1], nonlin=(i != len(nouts) - 1))\n",
    "            for i in range(len(nouts))\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 7: implement MLP.__call__\")\n",
    "\n",
    "    def parameters(self):\n",
    "        ### YOUR CODE HERE\n",
    "        raise NotImplementedError(\"Exercise 7: implement MLP.parameters\")\n",
    "\n",
    "\n",
    "# After implementing, test with:\n",
    "# mlp = MLP(3, [4, 4, 1])\n",
    "# x = [Value(1.0), Value(-2.0), Value(0.5)]\n",
    "# y = mlp(x)\n",
    "# print(\"MLP output:\", y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796067f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Solution 7 – Layer and MLP\n",
    "\n",
    "class Layer(Module):\n",
    "    def __init__(self, nin, nout, **kwargs):\n",
    "        self.neurons = [Neuron(nin, **kwargs) for _ in range(nout)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outs = [n(x) for n in self.neurons]\n",
    "        return outs[0] if len(outs) == 1 else outs\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for n in self.neurons for p in n.parameters()]\n",
    "\n",
    "\n",
    "class MLP(Module):\n",
    "    def __init__(self, nin, nouts):\n",
    "        sizes = [nin] + nouts\n",
    "        self.layers = [\n",
    "            Layer(sizes[i], sizes[i+1], nonlin=(i != len(nouts) - 1))\n",
    "            for i in range(len(nouts))\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "\n",
    "\n",
    "mlp = MLP(3, [4, 4, 1])\n",
    "x = [Value(1.0), Value(-2.0), Value(0.5)]\n",
    "y = mlp(x)\n",
    "print(\"MLP output:\", y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55fa92d",
   "metadata": {},
   "source": [
    "\n",
    "## Final example – training a tiny neural network\n",
    "\n",
    "We now have:\n",
    "\n",
    "- a scalar autograd engine (`Value`),\n",
    "- neural‑network building blocks (`Neuron`, `Layer`, `MLP`).\n",
    "\n",
    "Let’s put them together and **train** a small MLP on a toy classification problem.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We’ll create 4 points in 2D with labels in {+1, −1}. The goal is to map each point to its correct label.\n",
    "\n",
    "### Model\n",
    "\n",
    "Use an MLP:\n",
    "\n",
    "```python\n",
    "model = MLP(2, [4, 4, 1])\n",
    "```\n",
    "\n",
    "- input dimension = 2,\n",
    "- two hidden layers with 4 neurons each,\n",
    "- 1 output neuron (scalar score).\n",
    "\n",
    "### Loss – mean squared error (MSE)\n",
    "\n",
    "For predictions `y_pred` and targets `y_true`:\n",
    "\n",
    "\\[\n",
    "\\text{loss} = \\frac{1}{N} \\sum_i (y_{\\text{pred}, i} - y_{\\text{true}, i})^2\n",
    "\\]\n",
    "\n",
    "Loss is small when predictions are close to targets.\n",
    "\n",
    "### Training loop – vanilla gradient descent\n",
    "\n",
    "Each step:\n",
    "\n",
    "1. Forward pass: compute predictions and the loss.\n",
    "2. `model.zero_grad()` – clear old gradients.\n",
    "3. `loss.backward()` – fill in `p.grad` for all parameters.\n",
    "4. For each parameter `p`, update `p.data += -learning_rate * p.grad`.\n",
    "\n",
    "If everything is wired correctly, the loss should go down and predictions should approach the desired labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feb83c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tiny 2D dataset\n",
    "xs = [\n",
    "    [Value(2.0),  Value(3.0)],   # want +1\n",
    "    [Value(1.0),  Value(-1.0)],  # want -1\n",
    "    [Value(-1.0), Value(-2.0)],  # want -1\n",
    "    [Value(-2.0), Value(2.0)],   # want +1\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]\n",
    "\n",
    "model = MLP(2, [4, 4, 1])\n",
    "print(\"Number of parameters:\", len(model.parameters()))\n",
    "\n",
    "learning_rate = 0.1\n",
    "steps = 50\n",
    "\n",
    "for step in range(steps):\n",
    "    # Forward pass: compute predictions\n",
    "    ypred = [model(x) for x in xs]\n",
    "\n",
    "    # Mean squared error loss\n",
    "    losses = [(yout - ytrue)**2 for yout, ytrue in zip(ypred, ys)]\n",
    "    loss = sum(losses) * (1.0 / len(losses))\n",
    "\n",
    "    # Backpropagation\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Parameter update\n",
    "    for p in model.parameters():\n",
    "        p.data += -learning_rate * p.grad\n",
    "\n",
    "    if step % 5 == 0 or step == steps - 1:\n",
    "        print(f\"step {step:03d}  loss = {loss.data:.4f}\")\n",
    "\n",
    "print(\"\\nFinal predictions:\")\n",
    "for x, ytrue, yout in zip(xs, ys, ypred):\n",
    "    print(f\"target={ytrue:+.1f}, pred={yout.data:+.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c804266",
   "metadata": {},
   "source": [
    "\n",
    "## Wrap‑up\n",
    "\n",
    "You’ve built, from scratch:\n",
    "\n",
    "- a scalar automatic‑differentiation engine:\n",
    "  - `Value` nodes that store data, gradients, parents, and local `_backward` functions,\n",
    "  - a `.backward()` method that walks the computation graph in reverse to apply the chain rule;\n",
    "- a tiny neural‑network library:\n",
    "  - `Module` with `parameters()` and `zero_grad()`,\n",
    "  - `Neuron`, `Layer`, and `MLP` that are built purely from `Value` operations;\n",
    "- a small training loop that uses gradient descent to minimise a loss.\n",
    "\n",
    "This is the conceptual core of what big libraries like **PyTorch**, **TensorFlow**, or **JAX** do under the hood. They operate on tensors (multi‑dimensional arrays) instead of scalars, and they are highly optimised and feature‑rich, but the **math and ideas are the same**.\n",
    "\n",
    "From here you can experiment with:\n",
    "\n",
    "- different activation functions (`tanh`, sigmoid, ...),\n",
    "- different network architectures (more layers, more neurons),\n",
    "- different loss functions (e.g. cross‑entropy),\n",
    "- visualising the computation graph with tools like `graphviz`.\n",
    "\n",
    "The key idea:\n",
    "\n",
    "> Every operation knows how its **output** depends on its **inputs** (local derivative).  \n",
    "> Backpropagation stitches all these pieces together (chain rule) to tell you how a final loss depends on **every parameter**.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
