{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ“š Table of Contents\n",
        "\n",
        "**Total Estimated Time: 6-8 hours** (can be done in multiple sessions)\n",
        "\n",
        "---\n",
        "\n",
        "## Quick Navigation\n",
        "\n",
        "| Part | Topic | Time | Description |\n",
        "|------|-------|------|-------------|\n",
        "| **[Part 0](#part-0)** | Environment Setup | â±ï¸ 5 min | GPU detection, library imports |\n",
        "| **[Part 1](#part-1)** | Foundations: Dot Products | â±ï¸ 15 min | Math basics, cosine similarity, scaling |\n",
        "| **[Part 2](#part-2)** | Self-Attention | â±ï¸ 45 min | Q/K/V projections, attention weights, masking |\n",
        "| **[Part 3](#part-3)** | Multi-Head Self-Attention | â±ï¸ 30 min | Multiple heads, parallel attention |\n",
        "| **[Part 4](#part-4)** | Grouped Query Attention (GQA) | â±ï¸ 25 min | Memory efficiency, KV sharing |\n",
        "| **[Part 5](#part-5)** | Sliding Window Attention (SWA) | â±ï¸ 20 min | Local attention, sparse patterns |\n",
        "| **[Part 6](#part-6)** | Multi-head Latent Attention (MLA) | â±ï¸ 35 min | DeepSeek's advanced technique |\n",
        "| **[Part 7](#part-7)** | Cross-Attention | â±ï¸ 25 min | Encoder-decoder attention |\n",
        "| **[Part 8](#part-8)** | Modern Optimizations | â±ï¸ 30 min | Flash Attention, KV Cache, RoPE |\n",
        "| **[Part 9](#part-9)** | Transformer Block | â±ï¸ 25 min | LayerNorm, FFN, residuals |\n",
        "| **[Part 10](#part-10)** | End-to-End Project | â±ï¸ 40 min | Sentiment classifier with training |\n",
        "| **[Part 11](#part-11)** | Summary & Resources | â±ï¸ 10 min | Comparison tables, glossary |\n",
        "| **[Part 12](#part-12)** | Advanced Topics | â±ï¸ 30 min | Sparse, Linear, MoE, Speculative |\n",
        "| **[Cheat Sheet](#cheat-sheet)** | Quick Reference | ğŸ“‹ | All formulas & code patterns |\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ Learning Path Recommendations\n",
        "\n",
        "### ğŸŸ¢ **Beginner Path** (3-4 hours)\n",
        "Complete Parts 0-3, then jump to Part 9-10\n",
        "- Focus on understanding basic attention\n",
        "- Build a working sentiment classifier\n",
        "\n",
        "### ğŸŸ¡ **Intermediate Path** (5-6 hours)\n",
        "Complete Parts 0-7, then Part 9-10\n",
        "- Learn all attention variants\n",
        "- Understand real-world trade-offs\n",
        "\n",
        "### ğŸ”´ **Complete Path** (6-8 hours)\n",
        "Complete all parts in order\n",
        "- Master every attention mechanism\n",
        "- Understand modern optimizations\n",
        "\n",
        "---\n",
        "\n",
        "## âœ… Prerequisites Checklist\n",
        "\n",
        "Before starting, make sure you have:\n",
        "- [ ] Python 3.8+ installed\n",
        "- [ ] PyTorch 1.10+ (`pip install torch`)\n",
        "- [ ] Basic understanding of neural networks\n",
        "- [ ] Familiarity with matrix operations\n",
        "- [ ] (Optional) GPU with CUDA for faster execution\n",
        "\n",
        "---\n",
        "\n",
        "<a name=\"part-0\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ§  Complete Beginner's Guide to Attention Mechanisms\n",
        "\n",
        "## From Absolute Zero to Expert Understanding\n",
        "\n",
        "**Source**: [Attention, Visualized](https://www.adaptive-ml.com/post/attention-visualized) by Adaptive ML\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“‹ Complete Table of Contents\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                          ATTENTION MECHANISMS ROADMAP                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 0: Setup & Fundamentals â±ï¸ ~20 min                                   â”‚\n",
        "â”‚   â”œâ”€ 0.1: Environment Setup (GPU/CPU)                                       â”‚\n",
        "â”‚   â”œâ”€ 0.2: Understanding Tensors                                             â”‚\n",
        "â”‚   â”œâ”€ 0.3: Word Embeddings Explained                                         â”‚\n",
        "â”‚   â”œâ”€ 0.4: Creating Your First Tensor                                        â”‚\n",
        "â”‚   â””â”€ ğŸŸ¢ Exercises 0.1-0.3                                                    â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 1: The Dot Product â±ï¸ ~25 min                                        â”‚\n",
        "â”‚   â”œâ”€ 1.1: Mathematical Definition                                           â”‚\n",
        "â”‚   â”œâ”€ 1.2: Geometric Interpretation                                          â”‚\n",
        "â”‚   â”œâ”€ 1.3: Dot Product as Similarity                                         â”‚\n",
        "â”‚   â”œâ”€ 1.4: Dot Product vs Cosine Similarity  [NEW!]                         â”‚\n",
        "â”‚   â”œâ”€ 1.5: Matrix Multiplication                                             â”‚\n",
        "â”‚   â”œâ”€ 1.6: Scaled Dot Product                                                â”‚\n",
        "â”‚   â”œâ”€ 1.7: Numerical Stability  [NEW!]                                       â”‚\n",
        "â”‚   â””â”€ ğŸŸ¢ğŸŸ¡ Exercises 1.1-1.4                                                  â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 2: Self-Attention â±ï¸ ~40 min                                         â”‚\n",
        "â”‚   â”œâ”€ 2.1: The Context Problem                                               â”‚\n",
        "â”‚   â”œâ”€ 2.2: Self-Attention Overview                                           â”‚\n",
        "â”‚   â”œâ”€ 2.3: Step 1 - Creating Q, K, V                                         â”‚\n",
        "â”‚   â”œâ”€ 2.4: Step 2 - Attention Scores                                         â”‚\n",
        "â”‚   â”œâ”€ 2.5: Step 3 - Softmax Normalization                                    â”‚\n",
        "â”‚   â”œâ”€ 2.6: Step 4 - Applying to Values                                       â”‚\n",
        "â”‚   â”œâ”€ 2.7: Full Self-Attention Implementation                                â”‚\n",
        "â”‚   â”œâ”€ 2.8: Causal (Masked) Attention  [NEW!]                                â”‚\n",
        "â”‚   â”œâ”€ 2.9: Positional Encodings  [NEW!]                                     â”‚\n",
        "â”‚   â”œâ”€ 2.10: Padding Masks  [NEW!]                                           â”‚\n",
        "â”‚   â””â”€ ğŸŸ¢ğŸŸ¡ğŸ”´ Exercises 2.1-2.6                                                â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 3: Multi-Head Attention â±ï¸ ~30 min                                   â”‚\n",
        "â”‚   â”œâ”€ 3.1: Why Multiple Heads?                                               â”‚\n",
        "â”‚   â”œâ”€ 3.2: Head Splitting Mathematics                                        â”‚\n",
        "â”‚   â”œâ”€ 3.3: Complete MHSA Implementation                                      â”‚\n",
        "â”‚   â”œâ”€ 3.4: Visualizing Attention Heads  [NEW!]                              â”‚\n",
        "â”‚   â”œâ”€ 3.5: What Different Heads Learn  [NEW!]                               â”‚\n",
        "â”‚   â”œâ”€ 3.6: Attention Dropout  [NEW!]                                        â”‚\n",
        "â”‚   â””â”€ ğŸŸ¡ğŸ”´ Exercises 3.1-3.4                                                  â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 4: Grouped Query Attention â±ï¸ ~20 min                                â”‚\n",
        "â”‚   â”œâ”€ 4.1: Memory Efficiency Problem                                         â”‚\n",
        "â”‚   â”œâ”€ 4.2: GQA Solution                                                      â”‚\n",
        "â”‚   â”œâ”€ 4.3: Benchmarking GQA vs MHSA  [NEW!]                                 â”‚\n",
        "â”‚   â””â”€ ğŸŸ¡ Exercises 4.1-4.2                                                    â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 5: Sliding Window Attention â±ï¸ ~20 min                               â”‚\n",
        "â”‚   â”œâ”€ 5.1: Computational Complexity                                          â”‚\n",
        "â”‚   â”œâ”€ 5.2: Window Masking                                                    â”‚\n",
        "â”‚   â”œâ”€ 5.3: SWA Implementation                                                â”‚\n",
        "â”‚   â””â”€ ğŸŸ¡ Exercises 5.1-5.2                                                    â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 6: Multi-head Latent Attention (MLA) â±ï¸ ~30 min  [NEW - DeepSeek!]  â”‚\n",
        "â”‚   â”œâ”€ 6.1: The KV Cache Problem Revisited                                   â”‚\n",
        "â”‚   â”œâ”€ 6.2: MLA Architecture Deep Dive                                       â”‚\n",
        "â”‚   â”œâ”€ 6.3: Why Decoupled RoPE?                                              â”‚\n",
        "â”‚   â”œâ”€ 6.4: Comparing Attention Mechanisms                                   â”‚\n",
        "â”‚   â”œâ”€ 6.5: Implementing MLA from Scratch                                    â”‚\n",
        "â”‚   â”œâ”€ 6.6: Understanding MLA's Memory Savings                               â”‚\n",
        "â”‚   â”œâ”€ 6.7: The Absorption Trick Explained                                   â”‚\n",
        "â”‚   â”œâ”€ 6.8: MLA Summary and Key Takeaways                                    â”‚\n",
        "â”‚   â””â”€ ğŸŸ¡ğŸ”´ Exercises 6.1-6.2                                                  â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 7: Cross-Attention â±ï¸ ~20 min                                        â”‚\n",
        "â”‚   â”œâ”€ 7.1: Encoder-Decoder Attention                                         â”‚\n",
        "â”‚   â”œâ”€ 7.2: Q from Decoder, K/V from Encoder                                  â”‚\n",
        "â”‚   â”œâ”€ 7.3: Use Cases (Translation, Image Captioning)                         â”‚\n",
        "â”‚   â””â”€ ğŸŸ¡ Exercises 7.1-7.2                                                    â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 8: Modern Optimizations â±ï¸ ~15 min                                   â”‚\n",
        "â”‚   â”œâ”€ 8.1: Flash Attention                                                   â”‚\n",
        "â”‚   â”œâ”€ 8.2: RoPE (Rotary Position Embeddings)                                â”‚\n",
        "â”‚   â”œâ”€ 8.3: ALiBi (Attention with Linear Biases)                             â”‚\n",
        "â”‚   â””â”€ 8.4: KV Caching for Inference                                          â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 9: Building a Transformer Block â±ï¸ ~25 min                           â”‚\n",
        "â”‚   â”œâ”€ 9.1: Layer Normalization                                               â”‚\n",
        "â”‚   â”œâ”€ 9.2: Feed-Forward Network                                              â”‚\n",
        "â”‚   â”œâ”€ 9.3: Residual Connections                                              â”‚\n",
        "â”‚   â””â”€ 9.4: Complete Transformer Block                                        â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 10: End-to-End Project â±ï¸ ~30 min                                    â”‚\n",
        "â”‚   â”œâ”€ 10.1: Simple Sentiment Classifier                                      â”‚\n",
        "â”‚   â”œâ”€ 10.2: Training Loop                                                    â”‚\n",
        "â”‚   â”œâ”€ 10.3: Visualizing Learned Attention                                    â”‚\n",
        "â”‚   â””â”€ ğŸ”´ Final Project                                                        â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”‚   Part 11: Summary & Resources                                               â”‚\n",
        "â”‚   â”œâ”€ 11.1: Complete Comparison Table                                        â”‚\n",
        "â”‚   â”œâ”€ 11.2: Common Mistakes & Gotchas                                        â”‚\n",
        "â”‚   â”œâ”€ 11.3: Glossary                                                         â”‚\n",
        "â”‚   â””â”€ 11.4: Next Steps & Papers to Read                                      â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "Exercise Difficulty: ğŸŸ¢ Easy  ğŸŸ¡ Medium  ğŸ”´ Hard\n",
        "Total Time: ~4-5 hours (can split across sessions)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ¯ Learning Objectives\n",
        "\n",
        "By completing this notebook, you will be able to:\n",
        "\n",
        "| # | Objective | Status |\n",
        "|---|-----------|--------|\n",
        "| 1 | Explain tensors and their shapes in PyTorch | â˜ |\n",
        "| 2 | Compute dot products and understand their geometric meaning | â˜ |\n",
        "| 3 | Implement self-attention from scratch with full understanding | â˜ |\n",
        "| 4 | Create causal masks for autoregressive models | â˜ |\n",
        "| 5 | Explain why positional encodings are needed | â˜ |\n",
        "| 6 | Implement multi-head attention with head visualization | â˜ |\n",
        "| 7 | Compare MHSA, GQA, and SWA with benchmarks | â˜ |\n",
        "| 8 | Implement cross-attention for encoder-decoder models | â˜ |\n",
        "| 9 | Build a complete Transformer block | â˜ |\n",
        "| 10 | Train a simple attention-based classifier | â˜ |\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ’¡ How to Use This Notebook\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    STUDY WORKFLOW                                â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                 â”‚\n",
        "â”‚   1. READ         â†’  Every explanation builds understanding     â”‚\n",
        "â”‚       â†“                                                         â”‚\n",
        "â”‚   2. STUDY        â†’  ASCII diagrams = visual intuition          â”‚\n",
        "â”‚       â†“                                                         â”‚\n",
        "â”‚   3. TRACE        â†’  Math formulas = what's really happening   â”‚\n",
        "â”‚       â†“                                                         â”‚\n",
        "â”‚   4. CODE         â†’  Run every cell, observe outputs            â”‚\n",
        "â”‚       â†“                                                         â”‚\n",
        "â”‚   5. EXERCISE     â†’  Complete without peeking at solutions!     â”‚\n",
        "â”‚       â†“                                                         â”‚\n",
        "â”‚   6. EXPERIMENT   â†’  Change values, break things, learn!        â”‚\n",
        "â”‚                                                                 â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### âš ï¸ Important Tips\n",
        "\n",
        "| Tip | Why It Matters |\n",
        "|-----|----------------|\n",
        "| **Don't skip exercises** | They solidify understanding - solutions are hidden |\n",
        "| **Run cells in order** | Each builds on previous ones |\n",
        "| **Read error messages** | They're educational, not just annoying |\n",
        "| **Take breaks** | Split across 2-3 sessions for better retention |\n",
        "| **Experiment freely** | The best learning comes from breaking things |\n",
        "\n",
        "---\n",
        "\n",
        "## ğŸ“Š Prerequisites Check\n",
        "\n",
        "Before starting, you should be comfortable with:\n",
        "\n",
        "```python\n",
        "# Python Basics\n",
        "âœ… Variables, functions, classes\n",
        "âœ… List comprehensions\n",
        "âœ… Basic NumPy operations\n",
        "\n",
        "# Math Basics  \n",
        "âœ… Vectors and matrices\n",
        "âœ… Basic linear algebra (multiplication)\n",
        "âœ… Exponents and logarithms\n",
        "\n",
        "# Not Required (we'll teach these!)\n",
        "âŒ PyTorch experience\n",
        "âŒ Deep learning background\n",
        "âŒ Transformer knowledge\n",
        "```\n",
        "\n",
        "Let's begin your journey! ğŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-0\"></a>\n",
        "# Part 0: Setup and Fundamental Concepts â±ï¸ ~5 min\n",
        "\n",
        "## Section 0.1: Installing Required Libraries\n",
        "\n",
        "### ğŸ¯ What We're Installing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# PART 0: ENVIRONMENT SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Install packages if needed (uncomment the line below)\n",
        "# !pip install torch numpy matplotlib\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import time\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# ============================================================================\n",
        "# GPU/CPU DETECTION (Important for performance!)\n",
        "# ============================================================================\n",
        "\n",
        "print('='*70)\n",
        "print('ğŸ–¥ï¸  ENVIRONMENT SETUP')\n",
        "print('='*70)\n",
        "\n",
        "# Detect and configure device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f'âœ… GPU Detected: {gpu_name}')\n",
        "    print(f'   Memory: {gpu_memory:.1f} GB')\n",
        "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "    device = torch.device('mps')\n",
        "    print('âœ… Apple Silicon GPU (MPS) Detected')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('âš ï¸  No GPU detected - using CPU')\n",
        "    print('   (This is fine for learning, but slower for large models)')\n",
        "\n",
        "print(f'\\nğŸ“ Using device: {device}')\n",
        "\n",
        "print('\\n' + '-'*70)\n",
        "print('ğŸ“¦ LIBRARY VERSIONS')\n",
        "print('-'*70)\n",
        "print(f'   PyTorch: {torch.__version__}')\n",
        "print(f'   NumPy:   {np.__version__}')\n",
        "\n",
        "# Check for important PyTorch features\n",
        "print('\\n' + '-'*70)\n",
        "print('ğŸ”§ PYTORCH FEATURES')\n",
        "print('-'*70)\n",
        "print(f'   CUDA available:     {torch.cuda.is_available()}')\n",
        "print(f'   cuDNN enabled:      {torch.backends.cudnn.enabled if torch.cuda.is_available() else \"N/A\"}')\n",
        "\n",
        "# Check for Flash Attention (PyTorch 2.0+)\n",
        "has_flash_attention = hasattr(F, 'scaled_dot_product_attention')\n",
        "print(f'   Flash Attention:    {has_flash_attention} (PyTorch 2.0+)')\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('âœ… SETUP COMPLETE - Ready to learn attention mechanisms! ğŸš€')\n",
        "print('='*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 0.2: Understanding Tensors\n",
        "\n",
        "### ğŸ§® What is a Tensor?\n",
        "\n",
        "A **tensor** is a multi-dimensional array - a container for numbers arranged in a grid.\n",
        "\n",
        "Think of it as a generalization:\n",
        "- **0D tensor** = single number (scalar)\n",
        "- **1D tensor** = list of numbers (vector)\n",
        "- **2D tensor** = table of numbers (matrix)\n",
        "- **3D+ tensor** = cube/hypercube of numbers\n",
        "\n",
        "### ğŸ“Š Visual Guide to Tensor Dimensions\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                                                              â”‚\n",
        "â”‚  0D TENSOR (Scalar)                                          â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚\n",
        "â”‚  Just a single number:                                       â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”‚      5                                                       â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”‚  Shape: ()                                                   â”‚\n",
        "â”‚  Example: temperature = 72.5                                 â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                              â”‚\n",
        "â”‚  1D TENSOR (Vector)                                          â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚\n",
        "â”‚  A row or column of numbers:                                 â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”‚      [1, 2, 3, 4]                                            â”‚\n",
        "â”‚       â†‘  â†‘  â†‘  â†‘                                             â”‚\n",
        "â”‚      idx 0,1,2,3                                             â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”‚  Shape: (4,)                                                 â”‚\n",
        "â”‚  Example: word embedding = [0.2, 0.5, 0.8, 0.1]             â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                              â”‚\n",
        "â”‚  2D TENSOR (Matrix)                                          â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚\n",
        "â”‚  A table/grid of numbers:                                    â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”‚          Col 0  Col 1  Col 2  Col 3                          â”‚\n",
        "â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                           â”‚\n",
        "â”‚  Row 0 â”‚  1      2      3      4  â”‚                          â”‚\n",
        "â”‚  Row 1 â”‚  5      6      7      8  â”‚                          â”‚\n",
        "â”‚  Row 2 â”‚  9     10     11     12  â”‚                          â”‚\n",
        "â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                           â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”‚  Shape: (3, 4) = 3 rows Ã— 4 columns                          â”‚\n",
        "â”‚  Example: 3-word sentence, each word has 4 features          â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                              â”‚\n",
        "â”‚  3D TENSOR (Cube)                                            â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚\n",
        "â”‚  A stack of matrices:                                        â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                       â”‚\n",
        "â”‚       â•±â”‚    Matrix   â”‚â•±                                      â”‚\n",
        "â”‚      â•± â”‚      1      â”‚â•±                                      â”‚\n",
        "â”‚     â”Œâ”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”                                    â”‚\n",
        "â”‚     â”‚  â”‚   Matrix    â”‚  â”‚                                    â”‚\n",
        "â”‚     â”‚  â”‚      2      â”‚  â”‚                                    â”‚\n",
        "â”‚     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                                    â”‚\n",
        "â”‚     â”‚     Matrix 3      â”‚                                    â”‚\n",
        "â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚\n",
        "â”‚                                                              â”‚\n",
        "â”‚  Shape: (2, 3, 4) = 2 matrices of size 3Ã—4                   â”‚\n",
        "â”‚  Example: 2 sentences, each with 3 words of 4 dimensions     â”‚\n",
        "â”‚                                                              â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ”‘ Key Concept: Shape\n",
        "\n",
        "The **shape** tells us the size in each dimension:\n",
        "- `(5,)` = vector with 5 elements\n",
        "- `(3, 4)` = matrix with 3 rows and 4 columns\n",
        "- `(2, 3, 4)` = 2 matrices, each 3Ã—4\n",
        "\n",
        "### ğŸ’¡ Why Tensors Matter for NLP\n",
        "\n",
        "In language processing:\n",
        "- **Words** â†’ 1D tensors (vectors)\n",
        "- **Sentences** â†’ 2D tensors (matrices)\n",
        "- **Batches of sentences** â†’ 3D tensors\n",
        "\n",
        "Understanding tensor shapes is CRITICAL for attention mechanisms!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 0.3: How Words Become Numbers\n",
        "\n",
        "### ğŸ—£ï¸ The Fundamental Problem\n",
        "\n",
        "Computers can't understand words directly. They only understand numbers!\n",
        "\n",
        "**Question**: How do we convert \"dog\" into something a computer can process?\n",
        "\n",
        "**Answer**: Word Embeddings!\n",
        "\n",
        "### ğŸ“ Word Embeddings Explained\n",
        "\n",
        "A **word embedding** is a dense vector representation of a word where:\n",
        "1. Each word gets a unique vector of numbers\n",
        "2. Similar words get similar vectors\n",
        "3. The vector captures meaning\n",
        "\n",
        "### ğŸ¨ Visualizing Word Embeddings\n",
        "\n",
        "```\n",
        "Word â†’ Vector Mapping (3-dimensional example)\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "\"dog\"  â†’  [0.5, 0.6, 0.7]\n",
        "           â†‘    â†‘    â†‘\n",
        "         Dim0  Dim1 Dim2\n",
        "\n",
        "\"cat\"  â†’  [0.4, 0.5, 0.6]  â† Similar to dog!\n",
        "                              (both are animals)\n",
        "\n",
        "\"car\"  â†’  [0.1, 0.2, 0.1]  â† Very different!\n",
        "                              (not an animal)\n",
        "```\n",
        "\n",
        "**Key Insight**: The numbers encode semantic meaning!\n",
        "- Dogs and cats have similar embeddings (both animals)\n",
        "- Cars have different embeddings (vehicles, not animals)\n",
        "\n",
        "### ğŸ“– From Words to Sentences\n",
        "\n",
        "A sentence is represented by **stacking word embeddings**:\n",
        "\n",
        "```\n",
        "Sentence: \"the dog was\"\n",
        "\n",
        "Step 1: Get each word's embedding\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\"the\" â†’ [0.1, 0.2, 0.3, 0.4]\n",
        "\"dog\" â†’ [0.5, 0.6, 0.7, 0.8]\n",
        "\"was\" â†’ [0.9, 1.0, 1.1, 1.2]\n",
        "\n",
        "Step 2: Stack them into a matrix\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "       Feature Dimensions\n",
        "        â†“    â†“    â†“    â†“\n",
        "       Dim0 Dim1 Dim2 Dim3\n",
        "     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "the  â”‚ 0.1  0.2  0.3  0.4   â”‚  â† Word 0 (row 0)\n",
        "dog  â”‚ 0.5  0.6  0.7  0.8   â”‚  â† Word 1 (row 1)\n",
        "was  â”‚ 0.9  1.0  1.1  1.2   â”‚  â† Word 2 (row 2)\n",
        "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "      â†‘\n",
        "    Words (rows)\n",
        "\n",
        "Shape: (3, 4)\n",
        "  â†‘    â†‘\n",
        "  â”‚    â””â”€ embedding_dim (features per word)\n",
        "  â””â”€â”€â”€â”€â”€â”€ sequence_length (number of words)\n",
        "```\n",
        "\n",
        "### ğŸŒŸ Real-World Embedding Sizes\n",
        "\n",
        "In our examples: **4 dimensions** (for simplicity)\n",
        "\n",
        "In real models:\n",
        "- **Word2Vec**: 300 dimensions\n",
        "- **GloVe**: 50-300 dimensions\n",
        "- **GPT-2**: 768 dimensions\n",
        "- **BERT**: 768 or 1024 dimensions\n",
        "- **GPT-3**: 12,288 dimensions\n",
        "- **GPT-4**: Even larger!\n",
        "\n",
        "More dimensions = more capacity to capture subtle meanings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 0.4: Creating Your First Tensor\n",
        "\n",
        "### ğŸ”¨ Building a Sentence Tensor\n",
        "\n",
        "Let's create a tensor representing **\"the dog was\"** with 4-dimensional embeddings.\n",
        "\n",
        "### ğŸ“ Code Explanation\n",
        "\n",
        "```python\n",
        "torch.tensor([...])  # Creates a PyTorch tensor\n",
        "    â”‚\n",
        "    â””â”€ List of lists creates 2D tensor\n",
        "           â”‚\n",
        "           â””â”€ Each inner list = one word's embedding\n",
        "```\n",
        "\n",
        "### ğŸ¯ What Each Line Does\n",
        "\n",
        "```python\n",
        "[0.1, 0.2, 0.3, 0.4]  # Embedding for \"the\"\n",
        " â†‘                      (usually a function word,\n",
        " â”‚                       less semantic content)\n",
        " â””â”€ These are learned during training!\n",
        "\n",
        "[0.5, 0.6, 0.7, 0.8]  # Embedding for \"dog\"\n",
        "                        (content word, animal)\n",
        "\n",
        "[0.9, 1.0, 1.1, 1.2]  # Embedding for \"was\"\n",
        "                        (verb, past tense)\n",
        "```\n",
        "\n",
        "In real models, these numbers are **learned** from data, not hand-picked!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a tensor for the sentence \"the dog was\"\n",
        "# Each word is represented by a 4-dimensional vector\n",
        "\n",
        "sentence = torch.tensor([\n",
        "    [0.1, 0.2, 0.3, 0.4],  # 'the' - first word\n",
        "    [0.5, 0.6, 0.7, 0.8],  # 'dog' - second word\n",
        "    [0.9, 1.0, 1.1, 1.2],  # 'was' - third word\n",
        "])\n",
        "\n",
        "print('='*70)\n",
        "print('SENTENCE TENSOR')\n",
        "print('='*70)\n",
        "print('\\nTensor values:')\n",
        "print(sentence)\n",
        "print(f'\\nShape: {sentence.shape}')\n",
        "print(f'\\nInterpretation:')\n",
        "print(f'  â€¢ {sentence.shape[0]} words in the sentence (rows)')\n",
        "print(f'  â€¢ {sentence.shape[1]} dimensions per word (columns)')\n",
        "print(f'  â€¢ Total numbers stored: {sentence.numel()}')\n",
        "print(f'\\nData type: {sentence.dtype}')\n",
        "print(f'Device: {sentence.device} (CPU or CUDA for GPU)')\n",
        "\n",
        "# Access individual elements\n",
        "print('\\n' + '='*70)\n",
        "print('ACCESSING ELEMENTS')\n",
        "print('='*70)\n",
        "print(f'\\nFirst word embedding (\"the\"): {sentence[0]}')\n",
        "print(f'Second word embedding (\"dog\"): {sentence[1]}')\n",
        "print(f'Third word embedding (\"was\"): {sentence[2]}')\n",
        "print(f'\\nFirst feature of \"dog\": {sentence[1, 0]}')\n",
        "print(f'All features of \"dog\": {sentence[1, :]}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 0.1: Create Your Own Tensor\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Practice creating tensors by making one for the sentence **\"I love AI\"**\n",
        "\n",
        "### ğŸ“‹ Requirements\n",
        "- **3 words** â†’ 3 rows\n",
        "- **4 dimensions** â†’ 4 columns per row\n",
        "- Final shape must be `torch.Size([3, 4])`\n",
        "\n",
        "### ğŸ’¡ Instructions\n",
        "1. Replace the zeros below with any numbers you want\n",
        "2. Use values between 0.0 and 1.0 (easier to work with)\n",
        "3. Run the cell\n",
        "4. Check that the shape is correct\n",
        "\n",
        "### ğŸ¨ Creative Freedom\n",
        "The actual numbers don't matter for this exercise - we're just learning the syntax!\n",
        "\n",
        "**Hint**: Copy the structure from the example above, but change the values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Replace the 0.0 values with any numbers (try 0.0 to 1.0)\n",
        "\n",
        "my_sentence = torch.tensor([\n",
        "    [0.0, 0.0, 0.0, 0.0],  # 'I' - your numbers here\n",
        "    [0.0, 0.0, 0.0, 0.0],  # 'love' - your numbers here\n",
        "    [0.0, 0.0, 0.0, 0.0],  # 'AI' - your numbers here\n",
        "])\n",
        "\n",
        "# Verification code (don't modify)\n",
        "print('Your tensor:')\n",
        "print(my_sentence)\n",
        "print(f'\\nShape: {my_sentence.shape}')\n",
        "\n",
        "# Automatic check\n",
        "try:\n",
        "    assert my_sentence.shape == torch.Size([3, 4]), f'Wrong shape! Expected (3, 4), got {my_sentence.shape}'\n",
        "    print('\\nâœ… CORRECT! Shape is (3, 4)')\n",
        "    print('   You successfully created a sentence tensor!')\n",
        "except AssertionError as e:\n",
        "    print(f'\\nâŒ {e}')\n",
        "    print('   Fix the shape and try again!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 0.2: Tensor Indexing\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Learn how to access specific elements in tensors\n",
        "\n",
        "### ğŸ“š Indexing Syntax\n",
        "```python\n",
        "tensor[row, column]  # Access specific element\n",
        "tensor[row, :]       # Access entire row\n",
        "tensor[:, column]    # Access entire column\n",
        "```\n",
        "\n",
        "### ğŸ“‹ Tasks\n",
        "Using the `sentence` tensor created earlier:\n",
        "1. Get the embedding for the word \"dog\" (row 1)\n",
        "2. Get the 3rd feature (index 2) of \"was\"\n",
        "3. Get all 2nd features (index 1) across all words\n",
        "\n",
        "### ğŸ’¡ Hints\n",
        "- Python uses 0-based indexing!\n",
        "- Row 0 = \"the\", Row 1 = \"dog\", Row 2 = \"was\"\n",
        "- Use `:` to select all items in a dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Get embedding for 'dog' (row 1)\n",
        "dog_embedding = None  # YOUR CODE: sentence[?, ?]\n",
        "\n",
        "# Task 2: Get 3rd feature (index 2) of 'was' (row 2)\n",
        "was_feature_3 = None  # YOUR CODE: sentence[?, ?]\n",
        "\n",
        "# Task 3: Get all 2nd features (index 1) from all words\n",
        "all_feature_2 = None  # YOUR CODE: sentence[?, ?]\n",
        "\n",
        "# Uncomment to check your answers:\n",
        "# print('Task 1 - Dog embedding:', dog_embedding)\n",
        "# print('Expected:', torch.tensor([0.5, 0.6, 0.7, 0.8]))\n",
        "# print()\n",
        "# print('Task 2 - Was feature 3:', was_feature_3)\n",
        "# print('Expected:', 1.1)\n",
        "# print()\n",
        "# print('Task 3 - All feature 2:', all_feature_2)\n",
        "# print('Expected:', torch.tensor([0.2, 0.6, 1.0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 0.3: Basic Tensor Operations\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Practice fundamental tensor operations\n",
        "\n",
        "### ğŸ“š Common Operations\n",
        "```python\n",
        "tensor.shape        # Get dimensions\n",
        "tensor.numel()      # Count total elements\n",
        "tensor.mean()       # Calculate average\n",
        "tensor.sum()        # Sum all elements\n",
        "tensor.max()        # Find maximum\n",
        "tensor.min()        # Find minimum\n",
        "```\n",
        "\n",
        "### ğŸ“‹ Tasks\n",
        "Using the `sentence` tensor:\n",
        "1. Calculate the mean of all values\n",
        "2. Find the maximum value\n",
        "3. Sum all values in the first word (\"the\")\n",
        "4. Count total elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Mean of all values\n",
        "mean_val = None  # YOUR CODE: sentence.???()\n",
        "\n",
        "# Task 2: Maximum value\n",
        "max_val = None  # YOUR CODE: sentence.???()\n",
        "\n",
        "# Task 3: Sum of first word\n",
        "first_word_sum = None  # YOUR CODE: sentence[?].???()\n",
        "\n",
        "# Task 4: Total elements\n",
        "total_elements = None  # YOUR CODE: sentence.???()\n",
        "\n",
        "# Uncomment to check:\n",
        "# print(f'Mean: {mean_val:.3f} (expected: 0.650)')\n",
        "# print(f'Max: {max_val:.3f} (expected: 1.200)')\n",
        "# print(f'First word sum: {first_word_sum:.3f} (expected: 1.000)')\n",
        "# print(f'Total elements: {total_elements} (expected: 12)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-1\"></a>\n",
        "# Part 1: The Dot Product - Foundation of Attention â±ï¸ ~15 min\n",
        "\n",
        "## Section 1.1: Mathematical Definition\n",
        "\n",
        "### ğŸ¯ Why This Matters\n",
        "\n",
        "The dot product is **THE MOST IMPORTANT OPERATION** in attention mechanisms!\n",
        "\n",
        "**Every single attention score** is computed using a dot product.\n",
        "\n",
        "Master this, and you'll understand 80% of how attention works!\n",
        "\n",
        "### ğŸ“ Formal Mathematical Definition\n",
        "\n",
        "Given two vectors $\\mathbf{a}$ and $\\mathbf{b}$, each containing $d$ numbers:\n",
        "\n",
        "$$\n",
        "\\mathbf{a} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3 \\\\ \\vdots \\\\ a_d \\end{bmatrix}\n",
        "\\quad \\quad\n",
        "\\mathbf{b} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3 \\\\ \\vdots \\\\ b_d \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "The **dot product** (also called inner product or scalar product) is defined as:\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{d} a_i b_i = a_1 b_1 + a_2 b_2 + a_3 b_3 + \\cdots + a_d b_d\n",
        "$$\n",
        "\n",
        "**Alternative notation** using matrix transpose:\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = \\mathbf{a}^\\top \\mathbf{b}\n",
        "$$\n",
        "\n",
        "where $\\mathbf{a}^\\top$ is the **transpose** of $\\mathbf{a}$ (turns column into row).\n",
        "\n",
        "### ğŸ§® Step-by-Step Example\n",
        "\n",
        "Let's compute the dot product of:\n",
        "\n",
        "$$\n",
        "\\mathbf{a} = [1, 2, 3] \\quad \\text{and} \\quad \\mathbf{b} = [4, 5, 6]\n",
        "$$\n",
        "\n",
        "**Step 1**: Multiply corresponding elements\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "a_1 \\times b_1 &= 1 \\times 4 = 4 \\\\\n",
        "a_2 \\times b_2 &= 2 \\times 5 = 10 \\\\\n",
        "a_3 \\times b_3 &= 3 \\times 6 = 18\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Step 2**: Add them all together\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = 4 + 10 + 18 = 32\n",
        "$$\n",
        "\n",
        "**Result**: A single number! (The dot product always gives a scalar.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1.2: Geometric Interpretation\n",
        "\n",
        "### ğŸ” The Beautiful Geometry Behind Dot Products\n",
        "\n",
        "The dot product has a powerful geometric meaning that explains why it measures similarity!\n",
        "\n",
        "### ğŸ“ The Geometric Formula\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\|\\mathbf{a}\\| = \\sqrt{a_1^2 + a_2^2 + \\cdots + a_d^2}$ is the **magnitude** (length) of vector $\\mathbf{a}$\n",
        "- $\\|\\mathbf{b}\\|$ is the magnitude of vector $\\mathbf{b}$\n",
        "- $\\theta$ is the **angle** between the two vectors\n",
        "- $\\cos(\\theta)$ is the cosine of that angle\n",
        "\n",
        "### ğŸ“Š Visual Understanding of Angles\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  CASE 1: Î¸ = 0Â° (Vectors Point Same Direction)        â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚\n",
        "â”‚                                                        â”‚\n",
        "â”‚      a  â†’â†’â†’â†’â†’â†’â†’â†’â†’                                     â”‚\n",
        "â”‚      b  â†’â†’â†’â†’â†’â†’â†’â†’â†’                                     â”‚\n",
        "â”‚                                                        â”‚\n",
        "â”‚      cos(0Â°) = 1                                       â”‚\n",
        "â”‚      Result: MAXIMUM POSITIVE dot product              â”‚\n",
        "â”‚      Meaning: Vectors are PERFECTLY ALIGNED            â”‚\n",
        "â”‚      Example: 'dog' and 'puppy' (very similar!)        â”‚\n",
        "â”‚                                                        â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  CASE 2: Î¸ = 45Â° (Vectors Point Somewhat Together)    â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚\n",
        "â”‚                                                        â”‚\n",
        "â”‚      a  â†’â†’â†’â†’â†’â†’â†’â†’â†’                                     â”‚\n",
        "â”‚      b    â†—â†—â†—â†—â†—                                        â”‚\n",
        "â”‚                                                        â”‚\n",
        "â”‚      cos(45Â°) â‰ˆ 0.707                                  â”‚\n",
        "â”‚      Result: MODERATE POSITIVE dot product             â”‚\n",
        "â”‚      Meaning: Vectors are SOMEWHAT ALIGNED             â”‚\n",
        "â”‚      Example: 'dog' and 'animal' (related)             â”‚\n",
        "â”‚                                                        â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  CASE 3: Î¸ = 90Â° (Vectors Are Perpendicular)          â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚\n",
        "â”‚                                                        â”‚\n",
        "â”‚      a  â†’â†’â†’â†’â†’â†’â†’â†’â†’                                     â”‚\n",
        "â”‚      b      â†‘                                          â”‚\n",
        "â”‚             â†‘                                          â”‚\n",
        "â”‚             â†‘                                          â”‚\n",
        "â”‚                                                        â”‚\n",
        "â”‚      cos(90Â°) = 0                                      â”‚\n",
        "â”‚      Result: ZERO dot product                          â”‚\n",
        "â”‚      Meaning: Vectors are ORTHOGONAL/INDEPENDENT       â”‚\n",
        "â”‚      Example: 'dog' and 'mathematics' (unrelated)      â”‚\n",
        "â”‚                                                        â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  CASE 4: Î¸ = 135Â° (Vectors Point Somewhat Apart)      â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚\n",
        "â”‚                                                        â”‚\n",
        "â”‚      a  â†’â†’â†’â†’â†’â†’â†’â†’â†’                                     â”‚\n",
        "â”‚      b  â†™â†™â†™â†™â†™                                          â”‚\n",
        "â”‚                                                        â”‚\n",
        "â”‚      cos(135Â°) â‰ˆ -0.707                                â”‚\n",
        "â”‚      Result: MODERATE NEGATIVE dot product             â”‚\n",
        "â”‚      Meaning: Vectors POINT AWAY from each other       â”‚\n",
        "â”‚      Example: 'love' and 'hate' (opposites)            â”‚\n",
        "â”‚                                                        â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  CASE 5: Î¸ = 180Â° (Vectors Point Opposite)            â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”‚\n",
        "â”‚                                                        â”‚\n",
        "â”‚      a  â†’â†’â†’â†’â†’â†’â†’â†’â†’                                     â”‚\n",
        "â”‚      b  â†â†â†â†â†â†â†â†â†                                     â”‚\n",
        "â”‚                                                        â”‚\n",
        "â”‚      cos(180Â°) = -1                                    â”‚\n",
        "â”‚      Result: MAXIMUM NEGATIVE dot product              â”‚\n",
        "â”‚      Meaning: Vectors are PERFECTLY OPPOSITE           â”‚\n",
        "â”‚      Example: 'hot' and 'cold' (antonyms)              â”‚\n",
        "â”‚                                                        â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ’¡ Key Insight\n",
        "\n",
        "The cosine function maps angles to similarity:\n",
        "- $\\cos(\\theta) = 1$ â†’ same direction â†’ highly similar\n",
        "- $\\cos(\\theta) = 0$ â†’ perpendicular â†’ unrelated\n",
        "- $\\cos(\\theta) = -1$ â†’ opposite direction â†’ opposites\n",
        "\n",
        "**This is why dot product measures similarity!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1.3: Dot Product as Similarity Measure\n",
        "\n",
        "### ğŸ’¡ Using Dot Products in NLP\n",
        "\n",
        "In natural language processing, we use dot products to measure **how similar two words are**.\n",
        "\n",
        "### ğŸ“Š Similarity Scale\n",
        "\n",
        "| Dot Product Value | Interpretation | Word Example |\n",
        "|------------------|----------------|---------------|\n",
        "| **Large Positive** (e.g., 8.5) | Very similar meanings | 'dog' Â· 'puppy' |\n",
        "| **Medium Positive** (e.g., 4.2) | Somewhat related | 'dog' Â· 'cat' |\n",
        "| **Small Positive** (e.g., 0.8) | Weakly related | 'dog' Â· 'animal' |\n",
        "| **Near Zero** (e.g., 0.1) | Unrelated concepts | 'dog' Â· 'mathematics' |\n",
        "| **Negative** (e.g., -2.5) | Opposite meanings | 'hot' Â· 'cold' |\n",
        "\n",
        "### ğŸ¯ Why This Matters for Attention\n",
        "\n",
        "In attention mechanisms:\n",
        "1. We compute dot products between **every pair of words**\n",
        "2. High dot product â†’ words should attend to each other\n",
        "3. Low dot product â†’ words can ignore each other\n",
        "\n",
        "### ğŸ”‘ Real Example\n",
        "\n",
        "For sentence \"the hot dog was delicious\":\n",
        "- `dog Â· hot` = HIGH â†’ \"dog\" should attend to \"hot\"\n",
        "- `dog Â· the` = LOW â†’ \"dog\" can ignore \"the\"\n",
        "- This helps the model understand \"hot dog\" is food!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrating dot product as similarity\n",
        "\n",
        "# Create 3D word embeddings (simplified for visualization)\n",
        "dog = torch.tensor([1.0, 0.5, 0.8])\n",
        "cat = torch.tensor([0.9, 0.6, 0.7])  # Similar to dog (both animals)\n",
        "car = torch.tensor([0.1, 0.2, 0.1])  # Different from dog (vehicle)\n",
        "\n",
        "# Compute similarities using dot product\n",
        "sim_dog_cat = torch.dot(dog, cat)\n",
        "sim_dog_car = torch.dot(dog, car)\n",
        "\n",
        "print('='*70)\n",
        "print('DOT PRODUCT AS SIMILARITY MEASURE')\n",
        "print('='*70)\n",
        "print('\\nWord Embeddings (3D vectors):')\n",
        "print(f'  dog = {dog.numpy()}')\n",
        "print(f'  cat = {cat.numpy()}')\n",
        "print(f'  car = {car.numpy()}')\n",
        "\n",
        "print('\\n' + '-'*70)\n",
        "print('Similarity Scores (Dot Products):')\n",
        "print('-'*70)\n",
        "print(f'  dog Â· cat = {sim_dog_cat:.4f}  â† HIGHER (both are animals!)')\n",
        "print(f'  dog Â· car = {sim_dog_car:.4f}  â† LOWER (unrelated concepts)')\n",
        "\n",
        "print('\\n' + '-'*70)\n",
        "print('Analysis:')\n",
        "print('-'*70)\n",
        "ratio = sim_dog_cat / sim_dog_car\n",
        "print(f'  â€¢ dog-cat is {ratio:.2f}x more similar than dog-car')\n",
        "print(f'  â€¢ This makes sense: dogs and cats are both animals!')\n",
        "print(f'  â€¢ Cars are vehicles - completely different category')\n",
        "\n",
        "# Visualize with bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(\n",
        "    ['dog Â· cat\\n(animals)', 'dog Â· car\\n(unrelated)'],\n",
        "    [sim_dog_cat, sim_dog_car],\n",
        "    color=['#2ecc71', '#e74c3c'],\n",
        "    alpha=0.8,\n",
        "    edgecolor='black',\n",
        "    linewidth=2.5\n",
        ")\n",
        "\n",
        "plt.ylabel('Similarity Score (Dot Product)', fontsize=13, fontweight='bold')\n",
        "plt.title('Dot Product Measures Semantic Similarity', \n",
        "          fontsize=15, fontweight='bold', pad=20)\n",
        "plt.axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
        "plt.grid(axis='y', alpha=0.3, linestyle=':')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar in bars:\n",
        "    height = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "            f'{height:.4f}',\n",
        "            ha='center', va='bottom',\n",
        "            fontsize=13, fontweight='bold',\n",
        "            bbox=dict(boxstyle='round,pad=0.5', facecolor='white', edgecolor='black'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('âœ… KEY TAKEAWAY')\n",
        "print('='*70)\n",
        "print('Similar words â†’ High dot product')\n",
        "print('Unrelated words â†’ Low dot product')\n",
        "print('This is the foundation of attention!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1.4: Dot Product vs Cosine Similarity\n",
        "\n",
        "### ğŸ¤” Two Related But Different Measures\n",
        "\n",
        "Both measure \"how similar\" two vectors are, but they work differently!\n",
        "\n",
        "### ğŸ“ The Formulas\n",
        "\n",
        "**Dot Product**:\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{d} a_i b_i\n",
        "$$\n",
        "\n",
        "**Cosine Similarity**:\n",
        "$$\n",
        "\\cos(\\theta) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\sqrt{\\sum a_i^2} \\cdot \\sqrt{\\sum b_i^2}}\n",
        "$$\n",
        "\n",
        "### ğŸ” The Key Difference\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    DOT PRODUCT vs COSINE SIMILARITY                     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   DOT PRODUCT: Affected by MAGNITUDE                                   â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚      a = [1, 0]     b = [1, 0]     â†’  aÂ·b = 1                          â”‚\n",
        "â”‚      a = [10, 0]    b = [1, 0]    â†’  aÂ·b = 10   (10Ã— larger!)         â”‚\n",
        "â”‚      a = [100, 0]   b = [1, 0]   â†’  aÂ·b = 100  (100Ã— larger!)        â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚      Same direction, DIFFERENT scores!                                 â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   COSINE SIMILARITY: Only measures ANGLE                               â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚      a = [1, 0]     b = [1, 0]     â†’  cos = 1.0                        â”‚\n",
        "â”‚      a = [10, 0]    b = [1, 0]    â†’  cos = 1.0  (same!)               â”‚\n",
        "â”‚      a = [100, 0]   b = [1, 0]   â†’  cos = 1.0  (same!)               â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚      Same direction = SAME score (always between -1 and 1)            â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ“Š When to Use Which?\n",
        "\n",
        "| Use Case | Best Metric | Why |\n",
        "|----------|-------------|-----|\n",
        "| **Attention scores** | Scaled Dot Product | Magnitude carries information |\n",
        "| **Semantic search** | Cosine Similarity | Only care about direction |\n",
        "| **Contrastive learning** | Cosine Similarity | Want normalized comparisons |\n",
        "| **Nearest neighbors** | Either | Depends on preprocessing |\n",
        "| **RAG retrieval** | Cosine Similarity | Embeddings often pre-normalized |\n",
        "\n",
        "### ğŸ’¡ Why Attention Uses Dot Product\n",
        "\n",
        "In attention:\n",
        "- **Magnitude matters!** A \"louder\" query should have more influence\n",
        "- **Scaling (Ã·âˆšd)** prevents magnitude from exploding\n",
        "- **Softmax** converts to probabilities anyway\n",
        "\n",
        "If we used cosine similarity in attention:\n",
        "- Would lose magnitude information\n",
        "- Often works fine, but dot product is standard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparing Dot Product vs Cosine Similarity\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    \"\"\"Compute cosine similarity between two vectors.\"\"\"\n",
        "    dot = torch.dot(a, b)\n",
        "    norm_a = torch.norm(a)\n",
        "    norm_b = torch.norm(b)\n",
        "    return dot / (norm_a * norm_b)\n",
        "\n",
        "# Create test vectors\n",
        "v1 = torch.tensor([1.0, 0.0, 0.0])\n",
        "v2 = torch.tensor([10.0, 0.0, 0.0])   # Same direction, 10x magnitude\n",
        "v3 = torch.tensor([100.0, 0.0, 0.0])  # Same direction, 100x magnitude\n",
        "v4 = torch.tensor([0.0, 1.0, 0.0])    # Perpendicular\n",
        "\n",
        "print('='*70)\n",
        "print('DOT PRODUCT vs COSINE SIMILARITY')\n",
        "print('='*70)\n",
        "\n",
        "print('\\nğŸ“Š Same Direction, Different Magnitudes:')\n",
        "print('-'*50)\n",
        "print(f'{\"Vectors\":<25} {\"Dot Product\":<15} {\"Cosine Sim\":<15}')\n",
        "print('-'*50)\n",
        "\n",
        "comparisons = [\n",
        "    ('v1=[1,0,0] Â· v1', v1, v1),\n",
        "    ('v1=[1,0,0] Â· v2=[10,0,0]', v1, v2),\n",
        "    ('v1=[1,0,0] Â· v3=[100,0,0]', v1, v3),\n",
        "    ('v1=[1,0,0] Â· v4=[0,1,0]', v1, v4),\n",
        "]\n",
        "\n",
        "for name, a, b in comparisons:\n",
        "    dot = torch.dot(a, b).item()\n",
        "    cos = cosine_similarity(a, b).item()\n",
        "    print(f'{name:<25} {dot:<15.2f} {cos:<15.4f}')\n",
        "\n",
        "print('\\nğŸ’¡ Key Insight:')\n",
        "print('   â€¢ Dot product changes with magnitude (1 â†’ 10 â†’ 100)')\n",
        "print('   â€¢ Cosine similarity stays constant (1.0) for same direction')\n",
        "print('   â€¢ Perpendicular vectors: dot=0, cos=0')\n",
        "\n",
        "# Visualize\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Dot products\n",
        "vectors = ['v1Â·v1', 'v1Â·v2\\n(10x)', 'v1Â·v3\\n(100x)', 'v1Â·v4\\n(perp)']\n",
        "dots = [1, 10, 100, 0]\n",
        "cosines = [1.0, 1.0, 1.0, 0.0]\n",
        "\n",
        "ax1.bar(vectors, dots, color=['#3498db', '#2ecc71', '#e74c3c', '#95a5a6'], alpha=0.8)\n",
        "ax1.set_ylabel('Score', fontweight='bold')\n",
        "ax1.set_title('Dot Product\\n(affected by magnitude)', fontweight='bold')\n",
        "ax1.set_ylim(0, 120)\n",
        "for i, v in enumerate(dots):\n",
        "    ax1.text(i, v + 2, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "ax2.bar(vectors, cosines, color=['#3498db', '#2ecc71', '#e74c3c', '#95a5a6'], alpha=0.8)\n",
        "ax2.set_ylabel('Score', fontweight='bold')\n",
        "ax2.set_title('Cosine Similarity\\n(only measures angle)', fontweight='bold')\n",
        "ax2.set_ylim(-0.1, 1.2)\n",
        "for i, v in enumerate(cosines):\n",
        "    ax2.text(i, v + 0.02, f'{v:.1f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('âœ… TAKEAWAY: Use dot product for attention, cosine for search/retrieval')\n",
        "print('='*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 1.1: Manual Dot Product Calculation\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Practice computing dot products by hand to build intuition\n",
        "\n",
        "### ğŸ“‹ Task\n",
        "Given these two vectors:\n",
        "$$\n",
        "\\mathbf{x} = [2, 3, 4] \\quad \\mathbf{y} = [1, 2, 3]\n",
        "$$\n",
        "\n",
        "Compute $\\mathbf{x} \\cdot \\mathbf{y}$ using the formula:\n",
        "$$\n",
        "\\mathbf{x} \\cdot \\mathbf{y} = x_1 y_1 + x_2 y_2 + x_3 y_3\n",
        "$$\n",
        "\n",
        "### ğŸ’¡ Step-by-Step Instructions\n",
        "1. Multiply: $2 \\times 1 = ?$\n",
        "2. Multiply: $3 \\times 2 = ?$\n",
        "3. Multiply: $4 \\times 3 = ?$\n",
        "4. Add all three results together\n",
        "\n",
        "### âœ… Check Your Work\n",
        "After calculating by hand, verify with PyTorch below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectors from the exercise\n",
        "x = torch.tensor([2.0, 3.0, 4.0])\n",
        "y = torch.tensor([1.0, 2.0, 3.0])\n",
        "\n",
        "# YOUR MANUAL CALCULATION:\n",
        "# Step 1: 2 Ã— 1 = ?\n",
        "# Step 2: 3 Ã— 2 = ?\n",
        "# Step 3: 4 Ã— 3 = ?\n",
        "# Sum: ? + ? + ? = ?\n",
        "\n",
        "manual_result = None  # Put your calculated answer here\n",
        "\n",
        "# Compute with PyTorch to check\n",
        "pytorch_result = torch.dot(x, y)\n",
        "\n",
        "# Uncomment to verify:\n",
        "# print(f'Your manual calculation: {manual_result}')\n",
        "# print(f'PyTorch result: {pytorch_result}')\n",
        "# if manual_result is not None and abs(manual_result - pytorch_result) < 0.001:\n",
        "#     print('\\nâœ… CORRECT! Great job!')\n",
        "# else:\n",
        "#     print('\\nâŒ Not quite. Try again!')\n",
        "#     print('Hint: (2Ã—1) + (3Ã—2) + (4Ã—3) = ?')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 1.2: Word Similarity Exploration\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Explore how dot products capture semantic similarity\n",
        "\n",
        "### ğŸ“‹ Task\n",
        "Create embeddings for three words and compare their similarities:\n",
        "- **apple** (fruit)\n",
        "- **orange** (fruit)  \n",
        "- **computer** (technology)\n",
        "\n",
        "### ğŸ’¡ Instructions\n",
        "1. Create 3D embeddings where apple and orange are similar\n",
        "2. Make computer very different\n",
        "3. Compute dot products\n",
        "4. Verify appleÂ·orange > appleÂ·computer\n",
        "\n",
        "### ğŸ¨ Hint\n",
        "Try making apple and orange have similar values (e.g., both start with 0.8, 0.9...)\n",
        "Make computer have very different values (e.g., 0.1, 0.2...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "# Create embeddings where apple and orange are similar\n",
        "\n",
        "apple = torch.tensor([0.0, 0.0, 0.0])     # Change these!\n",
        "orange = torch.tensor([0.0, 0.0, 0.0])    # Change these!\n",
        "computer = torch.tensor([0.0, 0.0, 0.0])  # Change these!\n",
        "\n",
        "# Compute similarities\n",
        "sim_apple_orange = torch.dot(apple, orange)\n",
        "sim_apple_computer = torch.dot(apple, computer)\n",
        "\n",
        "# Uncomment to check:\n",
        "# print(f'apple Â· orange = {sim_apple_orange:.4f}')\n",
        "# print(f'apple Â· computer = {sim_apple_computer:.4f}')\n",
        "# if sim_apple_orange > sim_apple_computer:\n",
        "#     print('\\nâœ… SUCCESS! Apple and orange are more similar!')\n",
        "#     print(f'   Ratio: {(sim_apple_orange/sim_apple_computer):.2f}x more similar')\n",
        "# else:\n",
        "#     print('\\nâŒ Try again! Make apple and orange more similar to each other.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1.4: Matrix Multiplication - Batch Dot Products\n",
        "\n",
        "### ğŸ”„ The Big Idea\n",
        "\n",
        "Matrix multiplication is just **many dot products computed at once**!\n",
        "\n",
        "This is crucial for efficiency in deep learning.\n",
        "\n",
        "### ğŸ“ Mathematical Definition\n",
        "\n",
        "Given:\n",
        "- Matrix $\\mathbf{A}$ with shape $(m \\times n)$\n",
        "- Matrix $\\mathbf{B}$ with shape $(n \\times p)$\n",
        "\n",
        "The product $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$ has shape $(m \\times p)$ where:\n",
        "\n",
        "$$\n",
        "C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj} = \\text{(row } i \\text{ of } \\mathbf{A}) \\cdot \\text{(column } j \\text{ of } \\mathbf{B})\n",
        "$$\n",
        "\n",
        "**In English**: Element $C_{ij}$ is the dot product of:\n",
        "- Row $i$ from matrix $\\mathbf{A}$\n",
        "- Column $j$ from matrix $\\mathbf{B}$\n",
        "\n",
        "### ğŸ¨ Visual Example\n",
        "\n",
        "```\n",
        "Matrix A (2Ã—3)        Matrix B (3Ã—2)        Result C (2Ã—2)\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 1   2   3 â”‚         â”‚ 1   2     â”‚         â”‚ 22   28   â”‚\n",
        "â”‚ 4   5   6 â”‚    Ã—    â”‚ 3   4     â”‚    =    â”‚ 49   64   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚ 5   6     â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "How we compute C[0,0] = 22:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Row 0 of A: [1, 2, 3]\n",
        "Col 0 of B: [1, 3, 5]\n",
        "\n",
        "Dot product:\n",
        "  (1 Ã— 1) + (2 Ã— 3) + (3 Ã— 5)\n",
        "  = 1 + 6 + 15  \n",
        "  = 22 âœ“\n",
        "\n",
        "How we compute C[0,1] = 28:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Row 0 of A: [1, 2, 3]\n",
        "Col 1 of B: [2, 4, 6]\n",
        "\n",
        "Dot product:\n",
        "  (1 Ã— 2) + (2 Ã— 4) + (3 Ã— 6)\n",
        "  = 2 + 8 + 18\n",
        "  = 28 âœ“\n",
        "\n",
        "How we compute C[1,0] = 49:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Row 1 of A: [4, 5, 6]\n",
        "Col 0 of B: [1, 3, 5]\n",
        "\n",
        "Dot product:\n",
        "  (4 Ã— 1) + (5 Ã— 3) + (6 Ã— 5)\n",
        "  = 4 + 15 + 30\n",
        "  = 49 âœ“\n",
        "\n",
        "How we compute C[1,1] = 64:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Row 1 of A: [4, 5, 6]\n",
        "Col 1 of B: [2, 4, 6]\n",
        "\n",
        "Dot product:\n",
        "  (4 Ã— 2) + (5 Ã— 4) + (6 Ã— 6)\n",
        "  = 8 + 20 + 36\n",
        "  = 64 âœ“\n",
        "```\n",
        "\n",
        "### âš¡ Why This is Powerful\n",
        "\n",
        "Instead of computing 4 dot products one by one, matrix multiplication does them **all at once**!\n",
        "\n",
        "On GPUs, this is **massively faster** than loops."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrix multiplication example with verification\n",
        "\n",
        "A = torch.tensor([\n",
        "    [1., 2., 3.],\n",
        "    [4., 5., 6.]\n",
        "])\n",
        "\n",
        "B = torch.tensor([\n",
        "    [1., 2.],\n",
        "    [3., 4.],\n",
        "    [5., 6.]\n",
        "])\n",
        "\n",
        "# Matrix multiplication using @ operator\n",
        "C = A @ B\n",
        "\n",
        "print('='*70)\n",
        "print('MATRIX MULTIPLICATION DEMONSTRATION')\n",
        "print('='*70)\n",
        "print('\\nMatrix A (2Ã—3):')\n",
        "print(A)\n",
        "print(f'Shape: {A.shape}')\n",
        "\n",
        "print('\\nMatrix B (3Ã—2):')\n",
        "print(B)\n",
        "print(f'Shape: {B.shape}')\n",
        "\n",
        "print('\\nResult C = A @ B (2Ã—2):')\n",
        "print(C)\n",
        "print(f'Shape: {C.shape}')\n",
        "\n",
        "# Manual verification of C[0,0]\n",
        "print('\\n' + '='*70)\n",
        "print('MANUAL VERIFICATION')\n",
        "print('='*70)\n",
        "print('\\nComputing C[0,0] manually:')\n",
        "print(f'  Row 0 of A: {A[0]}')\n",
        "print(f'  Col 0 of B: {B[:, 0]}')\n",
        "print(f'  Dot product: {A[0][0]}Ã—{B[0][0]} + {A[0][1]}Ã—{B[1][0]} + {A[0][2]}Ã—{B[2][0]}')\n",
        "manual = A[0][0]*B[0][0] + A[0][1]*B[1][0] + A[0][2]*B[2][0]\n",
        "print(f'             = {A[0][0]*B[0][0]} + {A[0][1]*B[1][0]} + {A[0][2]*B[2][0]}')\n",
        "print(f'             = {manual}')\n",
        "print(f'  C[0,0] from matrix mult: {C[0,0]}')\n",
        "print(f'  âœ… Match: {abs(manual - C[0,0]) < 0.001}')\n",
        "\n",
        "# Verify C[1,1]\n",
        "print('\\nComputing C[1,1] manually:')\n",
        "print(f'  Row 1 of A: {A[1]}')\n",
        "print(f'  Col 1 of B: {B[:, 1]}')\n",
        "manual2 = torch.dot(A[1], B[:, 1])\n",
        "print(f'  Dot product = {manual2}')\n",
        "print(f'  C[1,1] from matrix mult: {C[1,1]}')\n",
        "print(f'  âœ… Match: {abs(manual2 - C[1,1]) < 0.001}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 1.3: Matrix Multiplication Practice\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Practice matrix multiplication and understand shapes\n",
        "\n",
        "### ğŸ“‹ Task 1: Predict the Shape\n",
        "Given:\n",
        "- Matrix $\\mathbf{P}$ with shape $(4, 3)$\n",
        "- Matrix $\\mathbf{Q}$ with shape $(3, 5)$\n",
        "\n",
        "What will be the shape of $\\mathbf{P} \\mathbf{Q}$?\n",
        "\n",
        "**Rule**: $(m, n) \\times (n, p) = (m, p)$\n",
        "\n",
        "### ğŸ“‹ Task 2: Compute One Element\n",
        "Given:\n",
        "$$\n",
        "\\mathbf{X} = \\begin{bmatrix} 2 & 1 \\\\ 3 & 4 \\end{bmatrix}, \\quad\n",
        "\\mathbf{Y} = \\begin{bmatrix} 1 & 0 \\\\ 2 & 3 \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Compute element $[0,1]$ of $\\mathbf{X} \\mathbf{Y}$ by hand.\n",
        "\n",
        "**Hint**: Use row 0 of $\\mathbf{X}$ and column 1 of $\\mathbf{Y}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Predict the shape\n",
        "P = torch.randn(4, 3)\n",
        "Q = torch.randn(3, 5)\n",
        "\n",
        "predicted_shape = None  # YOUR ANSWER: (?, ?)\n",
        "\n",
        "# Task 2: Manual computation\n",
        "X = torch.tensor([[2., 1.], [3., 4.]])\n",
        "Y = torch.tensor([[1., 0.], [2., 3.]])\n",
        "\n",
        "# Compute XY[0,1] manually\n",
        "# Row 0 of X: [2, 1]\n",
        "# Col 1 of Y: [0, 3]\n",
        "# Dot product: (2Ã—0) + (1Ã—3) = ?\n",
        "\n",
        "manual_answer = None  # YOUR ANSWER\n",
        "\n",
        "# Uncomment to check:\n",
        "# result = P @ Q\n",
        "# print(f'Task 1 - Your prediction: {predicted_shape}')\n",
        "# print(f'Task 1 - Actual shape: {result.shape}')\n",
        "# if predicted_shape == tuple(result.shape):\n",
        "#     print('âœ… Task 1 correct!')\n",
        "# \n",
        "# XY = X @ Y\n",
        "# print(f'\\nTask 2 - Your answer: {manual_answer}')\n",
        "# print(f'Task 2 - Actual XY[0,1]: {XY[0,1]}')\n",
        "# if manual_answer is not None and abs(manual_answer - XY[0,1]) < 0.001:\n",
        "#     print('âœ… Task 2 correct!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1.5: The Scaled Dot Product\n",
        "\n",
        "### âš ï¸ The Problem with Raw Dot Products\n",
        "\n",
        "As the dimension $d$ increases, dot products can become **very large**!\n",
        "\n",
        "### ğŸ“Š Why This Happens\n",
        "\n",
        "The dot product is a sum over $d$ terms:\n",
        "\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^{d} a_i b_i\n",
        "$$\n",
        "\n",
        "If each $a_i, b_i \\approx 1$ and $d = 1000$:\n",
        "$$\n",
        "\\mathbf{a} \\cdot \\mathbf{b} \\approx 1 + 1 + \\cdots + 1 \\text{ (1000 times)} = 1000!\n",
        "$$\n",
        "\n",
        "### ğŸš¨ Why Large Values Are Bad\n",
        "\n",
        "Large values cause problems in the **softmax** function (coming soon):\n",
        "\n",
        "```\n",
        "softmax([1, 1, 1]) = [0.33, 0.33, 0.33]  â† Uniform\n",
        "softmax([10, 1, 1]) = [0.999, 0.0005, 0.0005]  â† Too peaked!\n",
        "softmax([100, 1, 1]) = [1.0, 0.0, 0.0]  â† Completely saturated!\n",
        "```\n",
        "\n",
        "Saturated softmax = vanishing gradients = training fails! âŒ\n",
        "\n",
        "### âœ… The Solution: Scaling\n",
        "\n",
        "Divide by $\\sqrt{d_k}$ to keep values in a reasonable range:\n",
        "\n",
        "$$\n",
        "\\text{Scaled Dot Product} = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\sqrt{d_k}}\n",
        "$$\n",
        "\n",
        "where $d_k$ is the dimension of the vectors.\n",
        "\n",
        "### ğŸ§® Why Specifically $\\sqrt{d_k}$?\n",
        "\n",
        "**Statistical reasoning**:\n",
        "\n",
        "If elements $a_i, b_i$ are independent random variables with:\n",
        "- Mean = 0\n",
        "- Variance = 1\n",
        "\n",
        "Then:\n",
        "$$\n",
        "\\text{Var}(a_i b_i) = 1\n",
        "$$\n",
        "\n",
        "For the sum:\n",
        "$$\n",
        "\\text{Var}\\left(\\sum_{i=1}^{d_k} a_i b_i\\right) = d_k\n",
        "$$\n",
        "\n",
        "Therefore:\n",
        "$$\n",
        "\\text{Std}\\left(\\sum_{i=1}^{d_k} a_i b_i\\right) = \\sqrt{d_k}\n",
        "$$\n",
        "\n",
        "Dividing by $\\sqrt{d_k}$ normalizes the standard deviation back to 1!\n",
        "\n",
        "### ğŸ’¡ Practical Impact\n",
        "\n",
        "```\n",
        "Without scaling (d=1000):\n",
        "  Dot product â‰ˆ 1000 â†’ softmax saturates\n",
        "\n",
        "With scaling (Ã·âˆš1000 â‰ˆ Ã·31.6):\n",
        "  Scaled dot product â‰ˆ 31.6 â†’ softmax works well\n",
        "```\n",
        "\n",
        "### ğŸ¯ This is Core to Attention!\n",
        "\n",
        "The famous attention formula:\n",
        "\n",
        "$$\n",
        "\\text{Attention} = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right) \\mathbf{V}\n",
        "$$\n",
        "\n",
        "That $\\sqrt{d_k}$ is **crucial** for stable training!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrating the effect of scaling\n",
        "\n",
        "# Create high-dimensional random vectors\n",
        "d_small = 10\n",
        "d_large = 1000\n",
        "\n",
        "# Small dimension\n",
        "a_small = torch.randn(d_small)\n",
        "b_small = torch.randn(d_small)\n",
        "dot_small = torch.dot(a_small, b_small)\n",
        "scaled_small = dot_small / math.sqrt(d_small)\n",
        "\n",
        "# Large dimension\n",
        "a_large = torch.randn(d_large)\n",
        "b_large = torch.randn(d_large)\n",
        "dot_large = torch.dot(a_large, b_large)\n",
        "scaled_large = dot_large / math.sqrt(d_large)\n",
        "\n",
        "print('='*70)\n",
        "print('EFFECT OF DIMENSION ON DOT PRODUCTS')\n",
        "print('='*70)\n",
        "\n",
        "print(f'\\nSmall Dimension (d={d_small}):')\n",
        "print(f'  Raw dot product: {dot_small:.2f}')\n",
        "print(f'  Scaled (Ã·âˆš{d_small}): {scaled_small:.2f}')\n",
        "\n",
        "print(f'\\nLarge Dimension (d={d_large}):')\n",
        "print(f'  Raw dot product: {dot_large:.2f}  â† Very large!')\n",
        "print(f'  Scaled (Ã·âˆš{d_large}): {scaled_large:.2f}  â† Reasonable!')\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('WHY SCALING MATTERS')\n",
        "print('='*70)\n",
        "print(f'Without scaling: {dot_large:.1f} â†’ Would saturate softmax!')\n",
        "print(f'With scaling: {scaled_large:.1f} â†’ Softmax works properly')\n",
        "\n",
        "# Visualize\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Bar chart comparing magnitudes\n",
        "dimensions = [d_small, d_large]\n",
        "raw_dots = [abs(dot_small), abs(dot_large)]\n",
        "scaled_dots = [abs(scaled_small), abs(scaled_large)]\n",
        "\n",
        "x = np.arange(len(dimensions))\n",
        "width = 0.35\n",
        "\n",
        "ax1.bar(x - width/2, raw_dots, width, label='Raw', color='red', alpha=0.7)\n",
        "ax1.bar(x + width/2, scaled_dots, width, label='Scaled', color='green', alpha=0.7)\n",
        "ax1.set_ylabel('Magnitude', fontweight='bold')\n",
        "ax1.set_title('Raw vs Scaled Dot Products', fontweight='bold')\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticks(x)\n",
        "ax1.set_xticklabels([f'd={d}' for d in dimensions])\n",
        "ax1.legend()\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Softmax saturation demonstration\n",
        "scores_unscaled = torch.tensor([float(dot_large), 1.0, 1.0])\n",
        "scores_scaled = torch.tensor([float(scaled_large), 1.0, 1.0])\n",
        "soft_unscaled = torch.softmax(scores_unscaled, dim=0)\n",
        "soft_scaled = torch.softmax(scores_scaled, dim=0)\n",
        "\n",
        "labels = ['High\\nScore', 'Low\\nScore 1', 'Low\\nScore 2']\n",
        "x2 = np.arange(len(labels))\n",
        "\n",
        "ax2.bar(x2 - width/2, soft_unscaled, width, label='Unscaledâ†’Saturated', color='red', alpha=0.7)\n",
        "ax2.bar(x2 + width/2, soft_scaled, width, label='Scaledâ†’Balanced', color='green', alpha=0.7)\n",
        "ax2.set_ylabel('Probability', fontweight='bold')\n",
        "ax2.set_title('Softmax Outputs', fontweight='bold')\n",
        "ax2.set_xticks(x2)\n",
        "ax2.set_xticklabels(labels)\n",
        "ax2.legend()\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "ax2.set_ylim([0, 1])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('âœ… KEY TAKEAWAY')\n",
        "print('='*70)\n",
        "print('Always scale by âˆšd_k to prevent softmax saturation!')\n",
        "print('This is essential for stable attention training.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-2\"></a>\n",
        "# Part 2: Self-Attention - The Heart of Transformers â±ï¸ ~45 min\n",
        "\n",
        "## Section 2.1: The Context Problem\n",
        "\n",
        "### ğŸ¤” A Fundamental Challenge in NLP\n",
        "\n",
        "Consider these two sentences:\n",
        "\n",
        "1. \"The **dog** was barking loudly at the mailman\"\n",
        "2. \"The **hot dog** was delicious with mustard\"\n",
        "\n",
        "The word \"dog\" appears in both, but means completely different things!\n",
        "\n",
        "### âŒ The Problem with Static Embeddings\n",
        "\n",
        "```\n",
        "Traditional Approach (Word2Vec, GloVe):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "\"dog\" â†’ [0.5, 0.6, 0.7, 0.8]  (ALWAYS the same)\n",
        "\n",
        "Problem:\n",
        "  Sentence 1: \"dog\" = animal ğŸ•\n",
        "  Sentence 2: \"dog\" = food ğŸŒ­\n",
        "  \n",
        "  Same embedding, different meanings! âŒ\n",
        "```\n",
        "\n",
        "**Static embeddings cannot capture context!**\n",
        "\n",
        "### âœ… The Self-Attention Solution\n",
        "\n",
        "```\n",
        "Self-Attention Approach:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "Sentence 1: \"The dog was barking\"\n",
        "  \"dog\" looks at: [\"the\", \"was\", \"barking\"]\n",
        "  \"dog\" â† \"barking\" (high attention!)\n",
        "  Result: dog_embedding + context_from_barking\n",
        "  â†’ Understands it's an ANIMAL ğŸ•\n",
        "\n",
        "Sentence 2: \"The hot dog was delicious\"\n",
        "  \"dog\" looks at: [\"the\", \"hot\", \"was\", \"delicious\"]\n",
        "  \"dog\" â† \"hot\" (high attention!)\n",
        "  \"dog\" â† \"delicious\" (high attention!)\n",
        "  Result: dog_embedding + context_from_hot + context_from_delicious\n",
        "  â†’ Understands it's FOOD ğŸŒ­\n",
        "```\n",
        "\n",
        "**Self-attention creates context-aware representations!**\n",
        "\n",
        "### ğŸ¯ The Key Insight\n",
        "\n",
        "Instead of fixed embeddings, each word gets a **dynamic** representation that:\n",
        "1. Looks at all other words in the sentence\n",
        "2. Decides which words are important (attention weights)\n",
        "3. Incorporates information from important words\n",
        "4. Creates a context-aware representation\n",
        "\n",
        "### ğŸ’¡ Real-World Impact\n",
        "\n",
        "This simple idea revolutionized NLP and powers:\n",
        "- BERT (Google)\n",
        "- GPT series (OpenAI)\n",
        "- T5 (Google)\n",
        "- Claude (Anthropic) â† You're talking to me!\n",
        "- And virtually all modern LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ§  Check Your Understanding: Before Self-Attention\n",
        "\n",
        "**Before proceeding, make sure you can answer these questions:**\n",
        "\n",
        "### Quick Quiz\n",
        "\n",
        "1. **What does the dot product measure between two vectors?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   The dot product measures how similar/aligned two vectors are. Higher values mean more similar direction.\n",
        "   </details>\n",
        "\n",
        "2. **Why do we scale the dot product by âˆšd_k?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   To prevent the dot products from becoming too large as dimension increases, which would cause softmax to produce near-binary outputs (vanishing gradients).\n",
        "   </details>\n",
        "\n",
        "3. **What does softmax do to attention scores?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   Converts raw scores into a probability distribution that sums to 1, where higher scores get exponentially more weight.\n",
        "   </details>\n",
        "\n",
        "4. **What is a word embedding?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   A dense vector representation of a word where similar words have similar vectors (close in the embedding space).\n",
        "   </details>\n",
        "\n",
        "### âœ… Ready to continue?\n",
        "If you answered 3-4 correctly, you're ready for Self-Attention!\n",
        "If not, consider reviewing Part 1 before continuing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2.2: Self-Attention Overview\n",
        "\n",
        "### ğŸ“Š The Four-Step Process\n",
        "\n",
        "Self-attention consists of exactly **4 steps**:\n",
        "\n",
        "```\n",
        "                    INPUT: X\n",
        "              (seq_len Ã— embed_dim)\n",
        "           \"the hot dog was delicious\"\n",
        "                      â”‚\n",
        "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "          â”‚           â”‚           â”‚\n",
        "   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "   â•‘  STEP 1: Create Q, K, V         â•‘\n",
        "   â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â•‘\n",
        "   â•‘  Q = X @ W_q  (Query: what I seek) â•‘\n",
        "   â•‘  K = X @ W_k  (Key: what I offer)  â•‘\n",
        "   â•‘  V = X @ W_v  (Value: what I give) â•‘\n",
        "   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "          â”‚           â”‚           â”‚\n",
        "          â†“           â†“           â†“\n",
        "       Query        Key        Value\n",
        "      (what I'm   (what I    (what I\n",
        "       seeking)    offer)     provide)\n",
        "          â”‚           â”‚           â”‚\n",
        "          â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚\n",
        "                â”‚                 â”‚\n",
        "   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "   â•‘  STEP 2: Attention Scores     â•‘\n",
        "   â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€      â•‘\n",
        "   â•‘  Scores = (Q @ K^T) / âˆšd_k     â•‘\n",
        "   â•‘                                â•‘\n",
        "   â•‘  Compute similarity between    â•‘\n",
        "   â•‘  every pair of words           â•‘\n",
        "   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "                â”‚\n",
        "                â†“\n",
        "        [score matrix]\n",
        "           (n Ã— n)\n",
        "                â”‚\n",
        "   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "   â•‘  STEP 3: Softmax               â•‘\n",
        "   â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â•‘\n",
        "   â•‘  Weights = softmax(Scores)     â•‘\n",
        "   â•‘                                â•‘\n",
        "   â•‘  Convert scores to             â•‘\n",
        "   â•‘  probabilities (0 to 1)        â•‘\n",
        "   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "                â”‚\n",
        "                â†“\n",
        "      [attention weights]\n",
        "       (probabilities)\n",
        "                â”‚\n",
        "   â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
        "   â•‘  STEP 4: Apply to Values       â•‘\n",
        "   â•‘  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€       â•‘\n",
        "   â•‘  Output = Weights @ V          â•‘\n",
        "   â•‘                                â•‘\n",
        "   â•‘  Weighted sum of all values    â•‘\n",
        "   â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
        "                â”‚\n",
        "                â†“\n",
        "         Context-Aware\n",
        "       Representations!\n",
        "```\n",
        "\n",
        "### ğŸ”‘ What Each Step Does\n",
        "\n",
        "1. **Step 1 (Q, K, V)**: Transform input into three different \"views\"\n",
        "   - Q: What each word is looking for\n",
        "   - K: What each word advertises\n",
        "   - V: What each word will contribute\n",
        "\n",
        "2. **Step 2 (Scores)**: Compute compatibility between queries and keys\n",
        "   - High score = \"these words should attend to each other\"\n",
        "   - Low score = \"these words can ignore each other\"\n",
        "\n",
        "3. **Step 3 (Softmax)**: Normalize scores into probabilities\n",
        "   - Ensures weights sum to 1\n",
        "   - Creates smooth probability distribution\n",
        "\n",
        "4. **Step 4 (Weighted Sum)**: Aggregate value vectors\n",
        "   - Each word becomes a mix of all words\n",
        "   - Mixing proportions determined by attention weights\n",
        "\n",
        "### ğŸ¯ The Complete Formula\n",
        "\n",
        "All four steps in one equation:\n",
        "\n",
        "$$\n",
        "\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right) \\mathbf{V}\n",
        "$$\n",
        "\n",
        "This is the most important equation in modern NLP!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2.3: Step 1 - Creating Query, Key, Value (Q, K, V)\n",
        "\n",
        "### ğŸ¯ The Goal\n",
        "\n",
        "Transform the input into three different representations, each serving a specific purpose.\n",
        "\n",
        "### ğŸ“š The Library Analogy\n",
        "\n",
        "Think of attention like searching a library:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  QUERY (Q): Your Search Question               â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚\n",
        "â”‚                                                 â”‚\n",
        "â”‚  You enter the library looking for:            â”‚\n",
        "â”‚  \"Books about machine learning\"                â”‚\n",
        "â”‚                                                 â”‚\n",
        "â”‚  In NLP: Each word asks \"What am I seeking?\"   â”‚\n",
        "â”‚  Example: \"dog\" might seek \"animal words\"      â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  KEY (K): Book Titles/Keywords                 â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚\n",
        "â”‚                                                 â”‚\n",
        "â”‚  Each book advertises its topic:               â”‚\n",
        "â”‚  \"Neural Networks\", \"Deep Learning\", \"AI\"      â”‚\n",
        "â”‚                                                 â”‚\n",
        "â”‚  In NLP: Each word advertises \"What I offer\"   â”‚\n",
        "â”‚  Example: \"barking\" offers \"animal context\"    â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚  VALUE (V): Actual Book Content                â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”‚\n",
        "â”‚                                                 â”‚\n",
        "â”‚  What you get when you check out the book:     â”‚\n",
        "â”‚  The actual information inside                  â”‚\n",
        "â”‚                                                 â”‚\n",
        "â”‚  In NLP: The information each word contributes â”‚\n",
        "â”‚  Example: \"hot\" contributes \"food context\"     â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ”¢ Mathematical Formulation\n",
        "\n",
        "Given input matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ where:\n",
        "- $n$ = sequence length (number of words)\n",
        "- $d$ = embedding dimension\n",
        "\n",
        "We create three transformations using **learned weight matrices**:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\mathbf{Q} &= \\mathbf{X} \\mathbf{W}_Q \\quad \\text{where } \\mathbf{W}_Q \\in \\mathbb{R}^{d \\times d_k} \\\\\n",
        "\\mathbf{K} &= \\mathbf{X} \\mathbf{W}_K \\quad \\text{where } \\mathbf{W}_K \\in \\mathbb{R}^{d \\times d_k} \\\\\n",
        "\\mathbf{V} &= \\mathbf{X} \\mathbf{W}_V \\quad \\text{where } \\mathbf{W}_V \\in \\mathbb{R}^{d \\times d_v}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Key points**:\n",
        "- $\\mathbf{W}_Q, \\mathbf{W}_K, \\mathbf{W}_V$ are **learned during training** (not hand-designed)\n",
        "- Often $d_k = d_v = d$ (same dimensions)\n",
        "- Each word gets its own Q, K, V vectors\n",
        "\n",
        "### ğŸ“ Shape Transformations\n",
        "\n",
        "```\n",
        "Input:     X          (n Ã— d)      e.g., (3 words Ã— 4 dims)\n",
        "Weights:   W_Q        (d Ã— d_k)    e.g., (4 Ã— 4)\n",
        "           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Output:    Q = X @ W_Q  (n Ã— d_k)  e.g., (3 Ã— 4)\n",
        "\n",
        "Same applies for K and V!\n",
        "\n",
        "Visual:\n",
        "     X (3Ã—4)    @    W_Q (4Ã—4)   =   Q (3Ã—4)\n",
        "  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "  â”‚ . . . . â”‚     â”‚ . . . . â”‚     â”‚ q . . . â”‚  â† Query for word 0\n",
        "  â”‚ . . . . â”‚  @  â”‚ . . . . â”‚  =  â”‚ q . . . â”‚  â† Query for word 1  \n",
        "  â”‚ . . . . â”‚     â”‚ . . . . â”‚     â”‚ q . . . â”‚  â† Query for word 2\n",
        "  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ . . . . â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ¨ Why Three Separate Matrices?\n",
        "\n",
        "Each serves a different role:\n",
        "\n",
        "| Matrix | Represents | Example |\n",
        "|--------|-----------|----------|\n",
        "| **Q** | What this token seeks | \"dog\" seeks animal-related words |\n",
        "| **K** | What this token advertises | \"barking\" advertises animal activity |\n",
        "| **V** | What this token contributes | \"barking\" contributes animal context |\n",
        "\n",
        "A word might:\n",
        "- **Query** for verbs (looking for action words)\n",
        "- **Key** advertise as a noun (I'm an entity)\n",
        "- **Value** provide specific semantic meaning\n",
        "\n",
        "This separation gives the model flexibility to learn complex patterns!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Creating Q, K, V matrices\n",
        "\n",
        "# Input: sentence \"the hot dog\" (3 words, 4 dimensions each)\n",
        "X = torch.tensor([\n",
        "    [1.0, 0.0, 0.0, 0.0],  # 'the' (one-hot style for clarity)\n",
        "    [0.0, 1.0, 0.0, 0.0],  # 'hot'\n",
        "    [0.0, 0.0, 1.0, 0.0],  # 'dog'\n",
        "], dtype=torch.float32)\n",
        "\n",
        "print('='*70)\n",
        "print('STEP 1: CREATING Q, K, V')\n",
        "print('='*70)\n",
        "print('\\nInput X (sentence \"the hot dog\"):')\n",
        "print(X)\n",
        "print(f'Shape: {X.shape} â†’ (3 words, 4 dimensions)\\n')\n",
        "\n",
        "# Define dimensions\n",
        "embed_dim = 4   # Input embedding dimension\n",
        "head_dim = 4    # Q, K, V dimension (keeping same for simplicity)\n",
        "\n",
        "# Initialize weight matrices with small random values\n",
        "# In real models, these are learned during training!\n",
        "torch.manual_seed(42)  # For reproducibility\n",
        "W_q = torch.randn(embed_dim, head_dim) * 0.1\n",
        "W_k = torch.randn(embed_dim, head_dim) * 0.1\n",
        "W_v = torch.randn(embed_dim, head_dim) * 0.1\n",
        "\n",
        "print('='*70)\n",
        "print('WEIGHT MATRICES (learned during training)')\n",
        "print('='*70)\n",
        "print('\\nW_Q (Query weight matrix):')\n",
        "print(W_q)\n",
        "print(f'Shape: {W_q.shape} â†’ ({embed_dim} input dims, {head_dim} output dims)')\n",
        "\n",
        "print('\\nW_K (Key weight matrix):')\n",
        "print(W_k)\n",
        "print(f'Shape: {W_k.shape}')\n",
        "\n",
        "print('\\nW_V (Value weight matrix):')\n",
        "print(W_v)\n",
        "print(f'Shape: {W_v.shape}')\n",
        "\n",
        "# Create Q, K, V through matrix multiplication\n",
        "Q = X @ W_q  # Query: what each word is looking for\n",
        "K = X @ W_k  # Key: what each word offers\n",
        "V = X @ W_v  # Value: what each word will contribute\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('RESULTING Q, K, V MATRICES')\n",
        "print('='*70)\n",
        "\n",
        "print('\\nQuery (Q) - \"What am I looking for?\":')\n",
        "print(Q)\n",
        "print(f'Shape: {Q.shape} â†’ (3 words, 4 query dimensions)')\n",
        "print('Each row = query vector for one word')\n",
        "\n",
        "print('\\nKey (K) - \"What do I offer?\":')\n",
        "print(K)\n",
        "print(f'Shape: {K.shape} â†’ (3 words, 4 key dimensions)')\n",
        "print('Each row = key vector for one word')\n",
        "\n",
        "print('\\nValue (V) - \"What information do I provide?\":')\n",
        "print(V)\n",
        "print(f'Shape: {V.shape} â†’ (3 words, 4 value dimensions)')\n",
        "print('Each row = value vector for one word')\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('âœ¨ KEY INSIGHT')\n",
        "print('='*70)\n",
        "print('Each word now has THREE different representations:')\n",
        "print('  1. Query  â†’ Describes what it wants to know')\n",
        "print('  2. Key    â†’ Describes what it can provide')\n",
        "print('  3. Value  â†’ Contains its actual content')\n",
        "print('\\nThese will be used in the next steps to compute attention!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 2.1: Understanding Q, K, V Dimensions\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Understand how dimensions flow through Q, K, V creation\n",
        "\n",
        "### ğŸ“‹ Task 1: Predict Shapes\n",
        "Given:\n",
        "- Input $\\mathbf{X}$ with shape $(5, 8)$ (5 words, 8 dims each)\n",
        "- Weight $\\mathbf{W}_Q$ with shape $(8, 6)$\n",
        "\n",
        "What will be the shape of $\\mathbf{Q} = \\mathbf{X} \\mathbf{W}_Q$?\n",
        "\n",
        "### ğŸ“‹ Task 2: Create Q, K, V\n",
        "Create Q, K, V for a new 2-word sentence with 3D embeddings.\n",
        "\n",
        "### ğŸ’¡ Hints\n",
        "- Matrix multiplication: $(m, n) \\times (n, p) = (m, p)$\n",
        "- Use `torch.randn()` for weight matrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Predict the shape\n",
        "X_test = torch.randn(5, 8)\n",
        "W_q_test = torch.randn(8, 6)\n",
        "\n",
        "predicted_q_shape = None  # YOUR ANSWER: (?, ?)\n",
        "\n",
        "# Task 2: Create Q, K, V for a 2-word sentence\n",
        "X_new = torch.tensor([\n",
        "    [0.5, 0.3, 0.2],  # Word 1\n",
        "    [0.8, 0.1, 0.4],  # Word 2\n",
        "])\n",
        "\n",
        "# YOUR CODE: Create weight matrices and compute Q, K, V\n",
        "# W_q_new = torch.randn(?, ?)\n",
        "# W_k_new = torch.randn(?, ?)\n",
        "# W_v_new = torch.randn(?, ?)\n",
        "# Q_new = ?\n",
        "# K_new = ?\n",
        "# V_new = ?\n",
        "\n",
        "# Uncomment to check:\n",
        "# Q_actual = X_test @ W_q_test\n",
        "# print(f'Task 1 - Your prediction: {predicted_q_shape}')\n",
        "# print(f'Task 1 - Actual shape: {Q_actual.shape}')\n",
        "# \n",
        "# print(f'\\nTask 2 - Q_new shape: {Q_new.shape}')\n",
        "# print(f'Task 2 - K_new shape: {K_new.shape}')\n",
        "# print(f'Task 2 - V_new shape: {V_new.shape}')\n",
        "# if Q_new.shape == (2, 3) and K_new.shape == (2, 3) and V_new.shape == (2, 3):\n",
        "#     print('\\nâœ… All shapes correct!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2.4: Step 2 - Computing Attention Scores\n",
        "\n",
        "### ğŸ¯ The Goal\n",
        "\n",
        "Compute how much each word should attend to every other word.\n",
        "\n",
        "### ğŸ”¢ The Formula\n",
        "\n",
        "$$\n",
        "\\text{Scores} = \\frac{\\mathbf{Q} \\mathbf{K}^\\top}{\\sqrt{d_k}}\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\mathbf{Q}$ = Query matrix, shape $(n, d_k)$\n",
        "- $\\mathbf{K}^\\top$ = Transposed Key matrix, shape $(d_k, n)$\n",
        "- $d_k$ = dimension of keys/queries\n",
        "- Result shape: $(n, n)$ - **one score for each word pair!**\n",
        "\n",
        "### ğŸ“Š Understanding the Result\n",
        "\n",
        "The scores matrix is $(n \\times n)$:\n",
        "\n",
        "```\n",
        "For \"the hot dog\", Scores matrix:\n",
        "\n",
        "            the      hot      dog     â† Keys (TO)\n",
        "          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    the   â”‚ sâ‚€â‚€     sâ‚€â‚     sâ‚€â‚‚  â”‚  â† \"the\" attending to...\n",
        "    hot   â”‚ sâ‚â‚€     sâ‚â‚     sâ‚â‚‚  â”‚  â† \"hot\" attending to...\n",
        "    dog   â”‚ sâ‚‚â‚€     sâ‚‚â‚     sâ‚‚â‚‚  â”‚  â† \"dog\" attending to...\n",
        "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â†‘\n",
        "         Queries (FROM)\n",
        "\n",
        "Each element:\n",
        "  s_{ij} = similarity between word i (query) and word j (key)\n",
        "\n",
        "Example:\n",
        "  sâ‚‚â‚ = score for \"dog\" attending to \"hot\"\n",
        "  High sâ‚‚â‚ â†’ \"dog\" should pay attention to \"hot\"!\n",
        "```\n",
        "\n",
        "### ğŸ”„ Why Transpose K?\n",
        "\n",
        "To make matrix multiplication work:\n",
        "\n",
        "$$\n",
        "\\mathbf{Q} \\mathbf{K}^\\top: \\quad (n, d_k) \\times (d_k, n) = (n, n) \\quad âœ…\n",
        "$$\n",
        "\n",
        "Without transpose:\n",
        "$$\n",
        "\\mathbf{Q} \\mathbf{K}: \\quad (n, d_k) \\times (n, d_k) \\quad âŒ \\text{ incompatible!}\n",
        "$$\n",
        "\n",
        "### ğŸ“ Why Divide by $\\sqrt{d_k}$?\n",
        "\n",
        "As we learned in Part 1:\n",
        "- Prevents scores from getting too large\n",
        "- Keeps softmax stable\n",
        "- Critical for training!\n",
        "\n",
        "### ğŸ’¡ Interpretation\n",
        "\n",
        "```\n",
        "High score:  Q and K are similar â†’ should attend\n",
        "Low score:   Q and K are different â†’ can ignore\n",
        "\n",
        "Example:\n",
        "  \"dog\"Â·\"hot\" = high â†’ attend (food context!)\n",
        "  \"dog\"Â·\"the\" = low â†’ ignore (just article)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Compute attention scores\n",
        "\n",
        "d_k = K.shape[-1]  # Dimension of key vectors\n",
        "print(f'Key dimension d_k = {d_k}')\n",
        "print(f'Scaling factor âˆšd_k = {math.sqrt(d_k):.4f}\\n')\n",
        "\n",
        "# Compute raw scores: Q @ K^T\n",
        "raw_scores = Q @ K.T  # K.T is the transpose\n",
        "\n",
        "print('='*70)\n",
        "print('RAW ATTENTION SCORES (before scaling)')\n",
        "print('='*70)\n",
        "print('\\nRaw Scores = Q @ K^T:')\n",
        "print(raw_scores)\n",
        "print(f'Shape: {raw_scores.shape} â†’ (3 words Ã— 3 words)')\n",
        "print('\\nEach element [i,j] = dot product of Q[i] and K[j]')\n",
        "print('= how much word i wants to attend to word j\\n')\n",
        "\n",
        "# Scale by âˆšd_k\n",
        "scaled_scores = raw_scores / math.sqrt(d_k)\n",
        "\n",
        "print('='*70)\n",
        "print(f'SCALED ATTENTION SCORES (Ã· âˆš{d_k} = {math.sqrt(d_k):.2f})')\n",
        "print('='*70)\n",
        "print('\\nScaled Scores:')\n",
        "print(scaled_scores)\n",
        "print(f'Shape: {scaled_scores.shape}\\n')\n",
        "\n",
        "# Label words for interpretation\n",
        "tokens = ['the', 'hot', 'dog']\n",
        "\n",
        "print('='*70)\n",
        "print('INTERPRETATION')\n",
        "print('='*70)\n",
        "for i, token_from in enumerate(tokens):\n",
        "    print(f'\\n\"{token_from}\" attending to:')\n",
        "    for j, token_to in enumerate(tokens):\n",
        "        score = scaled_scores[i, j].item()\n",
        "        print(f'  â†’ \"{token_to}\": {score:7.4f}')\n",
        "\n",
        "# Visualize as heatmap\n",
        "def plot_attention_scores(scores, tokens, title):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    im = plt.imshow(scores.detach().numpy(), cmap='YlOrRd', aspect='auto')\n",
        "    cbar = plt.colorbar(im, label='Attention Score')\n",
        "    cbar.ax.tick_params(labelsize=11)\n",
        "    \n",
        "    plt.xticks(range(len(tokens)), tokens, fontsize=13, fontweight='bold')\n",
        "    plt.yticks(range(len(tokens)), tokens, fontsize=13, fontweight='bold')\n",
        "    plt.xlabel('Key (attending TO)', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Query (attending FROM)', fontsize=14, fontweight='bold')\n",
        "    plt.title(title, fontsize=15, fontweight='bold', pad=20)\n",
        "    \n",
        "    # Add values in cells\n",
        "    for i in range(len(tokens)):\n",
        "        for j in range(len(tokens)):\n",
        "            val = scores[i, j].item()\n",
        "            color = 'white' if val > scores.mean() else 'black'\n",
        "            plt.text(j, i, f'{val:.3f}',\n",
        "                    ha='center', va='center',\n",
        "                    color=color, fontsize=12, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_attention_scores(scaled_scores, tokens, 'Attention Scores\\n(Before Softmax)')\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('ğŸ“Š HEATMAP INTERPRETATION')\n",
        "print('='*70)\n",
        "print('â€¢ Darker/Redder = Higher score = More attention likely')\n",
        "print('â€¢ Lighter/Yellow = Lower score = Less attention likely')\n",
        "print('â€¢ Diagonal = Self-attention (word attending to itself)')\n",
        "print('\\nNext: Convert these scores to probabilities with softmax!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 2.2: Computing Attention Scores\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Practice computing attention scores\n",
        "\n",
        "### ğŸ“‹ Task\n",
        "Given Q and K from your earlier exercise, compute scaled attention scores.\n",
        "\n",
        "### ğŸ’¡ Steps\n",
        "1. Compute Q @ K.T\n",
        "2. Get d_k from K.shape[-1]\n",
        "3. Divide by sqrt(d_k)\n",
        "4. Visualize the result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming you have Q_new and K_new from Exercise 2.1\n",
        "# If not, uncomment and create them:\n",
        "# Q_new = torch.randn(2, 3)\n",
        "# K_new = torch.randn(2, 3)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# scores_new = ?\n",
        "# d_k_new = ?\n",
        "# scaled_scores_new = ?\n",
        "\n",
        "# Uncomment to check:\n",
        "# print(f'Scores shape: {scaled_scores_new.shape}')\n",
        "# print(f'Expected: (2, 2) for 2 words')\n",
        "# print(f'\\nScaled scores:\\n{scaled_scores_new}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2.5: Step 3 - Softmax Normalization\n",
        "\n",
        "### ğŸ¯ The Goal\n",
        "\n",
        "Convert attention scores into probabilities that sum to 1.\n",
        "\n",
        "### ğŸ² The Softmax Function\n",
        "\n",
        "Given a vector $\\mathbf{x} = [x_1, x_2, \\ldots, x_n]$:\n",
        "\n",
        "$$\n",
        "\\text{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}\n",
        "$$\n",
        "\n",
        "**Properties**:\n",
        "1. Output values are between 0 and 1\n",
        "2. Outputs sum to 1 (valid probability distribution)\n",
        "3. Preserves ordering (larger input â†’ larger output)\n",
        "4. Differentiable (can train with backprop)\n",
        "\n",
        "### ğŸ“Š Step-by-Step Example\n",
        "\n",
        "Apply softmax to $\\mathbf{x} = [1.0, 2.0, 3.0]$:\n",
        "\n",
        "**Step 1**: Exponentiate\n",
        "$$\n",
        "e^{1.0} \\approx 2.718, \\quad e^{2.0} \\approx 7.389, \\quad e^{3.0} \\approx 20.086\n",
        "$$\n",
        "\n",
        "**Step 2**: Sum\n",
        "$$\n",
        "\\text{sum} = 2.718 + 7.389 + 20.086 = 30.193\n",
        "$$\n",
        "\n",
        "**Step 3**: Divide\n",
        "$$\n",
        "\\begin{align}\n",
        "\\text{softmax}([1, 2, 3]) &= \\left[\\frac{2.718}{30.193}, \\frac{7.389}{30.193}, \\frac{20.086}{30.193}\\right] \\\\\n",
        "&= [0.090, 0.245, 0.665]\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "Verify: $0.090 + 0.245 + 0.665 = 1.000$ âœ…\n",
        "\n",
        "### ğŸ’¡ Why Exponential?\n",
        "\n",
        "The $e^x$ function:\n",
        "1. **Always positive**: $e^x > 0$ for all $x$\n",
        "2. **Amplifies differences**: Larger values become much larger\n",
        "3. **Smooth**: Differentiable everywhere\n",
        "\n",
        "### ğŸ“ˆ Effect on Different Inputs\n",
        "\n",
        "```\n",
        "Input:  [1.0,  1.0,  1.0]  â†’  Output: [0.33, 0.33, 0.33]  (uniform)\n",
        "Input:  [1.0,  2.0,  3.0]  â†’  Output: [0.09, 0.24, 0.67]  (peaked)\n",
        "Input:  [1.0,  5.0,  1.0]  â†’  Output: [0.01, 0.98, 0.01]  (very peaked)\n",
        "```\n",
        "\n",
        "### ğŸ¯ Applying to Attention Scores\n",
        "\n",
        "We apply softmax **row-wise** to the scores matrix:\n",
        "\n",
        "$$\n",
        "\\text{AttentionWeights}_{ij} = \\frac{\\exp(\\text{Scores}_{ij})}{\\sum_{k=1}^{n} \\exp(\\text{Scores}_{ik})}\n",
        "$$\n",
        "\n",
        "Each row becomes a probability distribution:\n",
        "- Row $i$ = how word $i$ distributes its attention\n",
        "- All values in row $i$ sum to 1.0\n",
        "\n",
        "**This creates proper attention weights!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Apply softmax\n",
        "\n",
        "# Apply softmax along last dimension (rows)\n",
        "attention_weights = torch.softmax(scaled_scores, dim=-1)\n",
        "\n",
        "print('='*70)\n",
        "print('ATTENTION WEIGHTS (After Softmax)')\n",
        "print('='*70)\n",
        "print('\\nAttention Weights:')\n",
        "print(attention_weights)\n",
        "print(f'Shape: {attention_weights.shape}\\n')\n",
        "\n",
        "# Verify each row sums to 1\n",
        "print('='*70)\n",
        "print('VERIFICATION: Each Row Sums to 1')\n",
        "print('='*70)\n",
        "for i, token in enumerate(tokens):\n",
        "    row = attention_weights[i]\n",
        "    row_sum = row.sum().item()\n",
        "    print(f'\\nRow {i} (\"{token}\"):')\n",
        "    print(f'  Values: {row.numpy()}')\n",
        "    print(f'  Sum: {row_sum:.10f}  {\"âœ…\" if abs(row_sum - 1.0) < 1e-6 else \"âŒ\"}')\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('INTERPRETATION')\n",
        "print('='*70)\n",
        "print('Each row is a PROBABILITY DISTRIBUTION over all words.')\n",
        "print('Values show how much attention each word pays to others.\\n')\n",
        "\n",
        "for i, token_from in enumerate(tokens):\n",
        "    print(f'\"{token_from}\" attention distribution:')\n",
        "    for j, token_to in enumerate(tokens):\n",
        "        weight = attention_weights[i, j].item()\n",
        "        percentage = weight * 100\n",
        "        print(f'  â†’ \"{token_to}\": {weight:.4f} ({percentage:.1f}%)')\n",
        "    print()\n",
        "\n",
        "# Visualize\n",
        "def plot_attention_weights(weights, tokens):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    im = plt.imshow(weights.detach().numpy(), cmap='Blues',\n",
        "                    aspect='auto', vmin=0, vmax=1)\n",
        "    cbar = plt.colorbar(im, label='Attention Weight (Probability)')\n",
        "    cbar.ax.tick_params(labelsize=11)\n",
        "    \n",
        "    plt.xticks(range(len(tokens)), tokens, fontsize=13, fontweight='bold')\n",
        "    plt.yticks(range(len(tokens)), tokens, fontsize=13, fontweight='bold')\n",
        "    plt.xlabel('Key (attending TO)', fontsize=14, fontweight='bold')\n",
        "    plt.ylabel('Query (attending FROM)', fontsize=14, fontweight='bold')\n",
        "    plt.title('Attention Weights\\n(After Softmax - Probabilities)',\n",
        "             fontsize=15, fontweight='bold', pad=20)\n",
        "    \n",
        "    for i in range(len(tokens)):\n",
        "        for j in range(len(tokens)):\n",
        "            val = weights[i, j].item()\n",
        "            color = 'white' if val > 0.5 else 'black'\n",
        "            plt.text(j, i, f'{val:.3f}',\n",
        "                    ha='center', va='center',\n",
        "                    color=color, fontsize=12, fontweight='bold')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_attention_weights(attention_weights, tokens)\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('ğŸ“Š HEATMAP INTERPRETATION')\n",
        "print('='*70)\n",
        "print('â€¢ Darker blue = Higher probability = More attention')\n",
        "print('â€¢ Lighter blue = Lower probability = Less attention')\n",
        "print('â€¢ Each row sums to 1.0 (probability distribution)')\n",
        "print('â€¢ Each row shows where one word \"looks\" in the sentence')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2.6: Step 4 - Weighted Aggregation\n",
        "\n",
        "### ğŸ¯ The Final Step!\n",
        "\n",
        "Apply attention weights to value vectors to get context-aware representations.\n",
        "\n",
        "### ğŸ”¢ The Formula\n",
        "\n",
        "$$\n",
        "\\text{Output} = \\text{AttentionWeights} \\times \\mathbf{V}\n",
        "$$\n",
        "\n",
        "Shape analysis:\n",
        "$$\n",
        "\\underbrace{\\text{Output}}_{(n, d_v)} = \\underbrace{\\text{Weights}}_{(n, n)} \\times \\underbrace{\\mathbf{V}}_{(n, d_v)}\n",
        "$$\n",
        "\n",
        "### ğŸ’¡ What's Happening?\n",
        "\n",
        "For each word $i$, the output is:\n",
        "\n",
        "$$\n",
        "\\text{Output}_i = \\sum_{j=1}^{n} w_{ij} \\mathbf{V}_j\n",
        "$$\n",
        "\n",
        "where $w_{ij}$ is the attention weight from word $i$ to word $j$.\n",
        "\n",
        "**In English**: Each output word = weighted average of ALL value vectors!\n",
        "\n",
        "### ğŸ¨ Concrete Example\n",
        "\n",
        "For \"the hot dog\", output for \"dog\" (row 2):\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "\\text{Output}_{\\text{dog}} &= w_{\\text{dog}\\to\\text{the}} \\cdot \\mathbf{V}_{\\text{the}} \\\\\n",
        "&+ w_{\\text{dog}\\to\\text{hot}} \\cdot \\mathbf{V}_{\\text{hot}} \\\\\n",
        "&+ w_{\\text{dog}\\to\\text{dog}} \\cdot \\mathbf{V}_{\\text{dog}}\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "If \"dog\" attends strongly to \"hot\":\n",
        "- $w_{\\text{dog}\\to\\text{hot}}$ is large\n",
        "- $\\text{Output}_{\\text{dog}}$ heavily influenced by $\\mathbf{V}_{\\text{hot}}$\n",
        "- Model understands \"dog\" is food!\n",
        "\n",
        "### âœ¨ The Magic\n",
        "\n",
        "```\n",
        "BEFORE attention:  \n",
        "  \"dog\" = [fixed vector]\n",
        "         Same in ALL sentences\n",
        "\n",
        "AFTER attention:\n",
        "  \"dog\" in \"hot dog\" = [vector influenced by \"hot\"]\n",
        "  \"dog\" in \"pet dog\" = [vector influenced by \"pet\"]\n",
        "         CONTEXT-AWARE!\n",
        "```\n",
        "\n",
        "This is why transformers are so powerful!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Apply attention to values\n",
        "\n",
        "output = attention_weights @ V\n",
        "\n",
        "print('='*70)\n",
        "print('STEP 4: APPLYING ATTENTION TO VALUES')\n",
        "print('='*70)\n",
        "\n",
        "print('\\nInputs:')\n",
        "print(f'  Attention Weights shape: {attention_weights.shape} (3Ã—3)')\n",
        "print(f'  Value matrix V shape: {V.shape} (3Ã—4)\\n')\n",
        "\n",
        "print('Attention Weights:')\n",
        "print(attention_weights)\n",
        "\n",
        "print('\\nValue matrix V:')\n",
        "print(V)\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('OUTPUT (Weights @ V)')\n",
        "print('='*70)\n",
        "print(output)\n",
        "print(f'Shape: {output.shape} (3 words Ã— 4 dimensions)\\n')\n",
        "\n",
        "print('='*70)\n",
        "print('DETAILED EXPLANATION')\n",
        "print('='*70)\n",
        "print('\\nEach output row is a WEIGHTED COMBINATION of all value vectors.\\n')\n",
        "\n",
        "for i, token in enumerate(tokens):\n",
        "    print(f'Output for \"{token}\" (row {i}):')\n",
        "    print(f'  = ', end='')\n",
        "    \n",
        "    components = []\n",
        "    for j, token_j in enumerate(tokens):\n",
        "        weight = attention_weights[i, j].item()\n",
        "        components.append(f'{weight:.3f}Ã—V[\"{token_j}\"]')\n",
        "    \n",
        "    print(' + '.join(components))\n",
        "    \n",
        "    # Show which word had most influence\n",
        "    max_idx = attention_weights[i].argmax().item()\n",
        "    max_weight = attention_weights[i, max_idx].item()\n",
        "    print(f'  â†’ Most influenced by \"{tokens[max_idx]}\" ({max_weight:.1%})\\n')\n",
        "\n",
        "print('='*70)\n",
        "print('âœ¨ THE MAGIC OF SELF-ATTENTION')\n",
        "print('='*70)\n",
        "print('\\n\"dog\" now has a representation influenced by \"hot\"!')\n",
        "print('The model can understand \"hot dog\" as FOOD, not an ANIMAL.')\n",
        "print('\\nThis is CONTEXT-AWARE representation - the core of transformers!')\n",
        "\n",
        "# Complete formula visualization\n",
        "print('\\n' + '='*70)\n",
        "print('COMPLETE SELF-ATTENTION FORMULA')\n",
        "print('='*70)\n",
        "print('\\nAll 4 steps in one equation:')\n",
        "print('\\n  Attention(Q, K, V) = softmax(QÂ·K^T / âˆšd_k) Â· V')\n",
        "print('\\nwhere:')\n",
        "print('  Q = X @ W_Q  (queries)')\n",
        "print('  K = X @ W_K  (keys)')\n",
        "print('  V = X @ W_V  (values)')\n",
        "print('\\nThis transforms static embeddings into context-aware representations!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ® Interactive Attention Explorer\n",
        "\n",
        "Use the sliders below to explore how attention changes with different parameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Interactive Attention Explorer with ipywidgets\n",
        "# Note: This requires ipywidgets to be installed: pip install ipywidgets\n",
        "\n",
        "try:\n",
        "    import ipywidgets as widgets\n",
        "    from IPython.display import display, clear_output\n",
        "    WIDGETS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WIDGETS_AVAILABLE = False\n",
        "    print(\"âš ï¸ ipywidgets not installed. Run: pip install ipywidgets\")\n",
        "    print(\"   Skipping interactive visualization.\")\n",
        "\n",
        "if WIDGETS_AVAILABLE:\n",
        "    def interactive_attention(d_model=64, temperature=1.0, use_causal=False, \n",
        "                              seq_len=6, seed=42):\n",
        "        \"\"\"Visualize attention with adjustable parameters.\"\"\"\n",
        "        clear_output(wait=True)\n",
        "        \n",
        "        torch.manual_seed(seed)\n",
        "        \n",
        "        # Create Q, K, V\n",
        "        Q = torch.randn(seq_len, d_model)\n",
        "        K = torch.randn(seq_len, d_model)\n",
        "        V = torch.randn(seq_len, d_model)\n",
        "        \n",
        "        # Compute attention with temperature\n",
        "        d_k = K.shape[-1]\n",
        "        scores = (Q @ K.T) / (math.sqrt(d_k) * temperature)\n",
        "        \n",
        "        # Optional causal mask\n",
        "        if use_causal:\n",
        "            mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "        \n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        output = attn_weights @ V\n",
        "        \n",
        "        # Visualize\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "        \n",
        "        # Attention weights\n",
        "        ax = axes[0]\n",
        "        im = ax.imshow(attn_weights.numpy(), cmap='Blues', vmin=0, vmax=1)\n",
        "        ax.set_title(f'Attention Weights\\n(temp={temperature}, d={d_model})', fontweight='bold')\n",
        "        ax.set_xlabel('Key Position')\n",
        "        ax.set_ylabel('Query Position')\n",
        "        plt.colorbar(im, ax=ax)\n",
        "        \n",
        "        # Raw scores distribution\n",
        "        ax = axes[1]\n",
        "        scores_for_hist = scores.numpy().flatten()\n",
        "        scores_for_hist = scores_for_hist[scores_for_hist > -1000]  # Remove -inf\n",
        "        ax.hist(scores_for_hist, bins=30, color='steelblue', edgecolor='white')\n",
        "        ax.set_title('Score Distribution\\n(before softmax)', fontweight='bold')\n",
        "        ax.set_xlabel('Score')\n",
        "        ax.set_ylabel('Count')\n",
        "        ax.axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
        "        \n",
        "        # Attention entropy per query\n",
        "        ax = axes[2]\n",
        "        entropy = -torch.sum(attn_weights * torch.log(attn_weights + 1e-10), dim=-1)\n",
        "        max_entropy = math.log(seq_len)\n",
        "        ax.bar(range(seq_len), entropy.numpy(), color='coral', edgecolor='white')\n",
        "        ax.axhline(y=max_entropy, color='red', linestyle='--', label=f'Max (uniform): {max_entropy:.2f}')\n",
        "        ax.set_title('Attention Entropy\\n(higher = more spread out)', fontweight='bold')\n",
        "        ax.set_xlabel('Query Position')\n",
        "        ax.set_ylabel('Entropy')\n",
        "        ax.legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"ğŸ“Š Settings: d_model={d_model}, temp={temperature}, causal={use_causal}\")\n",
        "        print(f\"ğŸ“ Avg entropy: {entropy.mean():.3f} / {max_entropy:.3f} max\")\n",
        "        if entropy.mean() > 0.8 * max_entropy:\n",
        "            print(\"ğŸ’¡ High entropy: Attention is spread out (uniform-ish)\")\n",
        "        elif entropy.mean() < 0.3 * max_entropy:\n",
        "            print(\"ğŸ’¡ Low entropy: Attention is focused (peaked)\")\n",
        "        else:\n",
        "            print(\"ğŸ’¡ Medium entropy: Balanced attention pattern\")\n",
        "    \n",
        "    # Create interactive widgets\n",
        "    print(\"ğŸ® INTERACTIVE ATTENTION EXPLORER\")\n",
        "    print(\"=\"*60)\n",
        "    print(\"Adjust the sliders to see how attention changes!\")\n",
        "    print()\n",
        "    \n",
        "    widgets.interact(\n",
        "        interactive_attention,\n",
        "        d_model=widgets.IntSlider(min=8, max=256, step=8, value=64, description='Dimension:'),\n",
        "        temperature=widgets.FloatSlider(min=0.1, max=3.0, step=0.1, value=1.0, description='Temperature:'),\n",
        "        use_causal=widgets.Checkbox(value=False, description='Causal Mask'),\n",
        "        seq_len=widgets.IntSlider(min=4, max=16, step=1, value=6, description='Seq Length:'),\n",
        "        seed=widgets.IntSlider(min=0, max=100, step=1, value=42, description='Random Seed:')\n",
        "    )\n",
        "    \n",
        "    print(\"\\nğŸ’¡ Try these experiments:\")\n",
        "    print(\"   â€¢ High temperature (2-3): Makes attention more uniform\")\n",
        "    print(\"   â€¢ Low temperature (0.1-0.3): Makes attention more peaked\")\n",
        "    print(\"   â€¢ Enable causal mask: See how autoregressive attention works\")\n",
        "    print(\"   â€¢ Large dimension: Scores become more stable\")\n",
        "else:\n",
        "    print(\"Skipping interactive visualization (ipywidgets not available)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ› ï¸ BUILD IT YOURSELF: Self-Attention Step-by-Step\n",
        "\n",
        "Now it's YOUR turn! Before seeing the complete implementation, you'll build Self-Attention yourself, one step at a time.\n",
        "\n",
        "### ğŸ“‹ What You'll Implement\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    YOUR SELF-ATTENTION JOURNEY                          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Exercise 2.A: Initialize weight matrices W_q, W_k, W_v               â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 2.B: Compute Q, K, V from input X                           â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 2.C: Compute attention scores (Q @ K^T)                     â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 2.D: Scale by âˆšd_k                                          â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 2.E: Apply softmax for attention weights                    â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 2.F: Compute final output (weights @ V)                     â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 2.G: Combine into complete function                         â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ’¡ Tips\n",
        "- Run each cell to verify your implementation\n",
        "- Compare your output shapes with expected shapes\n",
        "- If stuck, check the hints provided\n",
        "- Don't peek at the full implementation until you complete all exercises!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.A: Initialize Weight Matrices (ğŸŸ¢ Easy)\n",
        "\n",
        "**Task**: Create the three weight matrices W_q, W_k, W_v.\n",
        "\n",
        "**Given**:\n",
        "- Input dimension: `embed_dim = 4`\n",
        "- Output dimension for Q, K, V: `head_dim = 4` (same as embed_dim for now)\n",
        "\n",
        "**Your job**: Create three random weight matrices of the correct shape.\n",
        "\n",
        "**Expected shapes**:\n",
        "- W_q: (embed_dim, head_dim) = (4, 4)\n",
        "- W_k: (embed_dim, head_dim) = (4, 4)\n",
        "- W_v: (embed_dim, head_dim) = (4, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.A: Initialize Weight Matrices\n",
        "\n",
        "# Given dimensions\n",
        "embed_dim = 4\n",
        "head_dim = 4\n",
        "\n",
        "# TODO: Create random weight matrices W_q, W_k, W_v\n",
        "# Hint: Use torch.randn(rows, cols)\n",
        "\n",
        "W_q_ex = None  # YOUR CODE HERE\n",
        "W_k_ex = None  # YOUR CODE HERE\n",
        "W_v_ex = None  # YOUR CODE HERE\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Verification (don't modify)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if W_q_ex is not None and W_k_ex is not None and W_v_ex is not None:\n",
        "    print('âœ… Checking your weight matrices...')\n",
        "    assert W_q_ex.shape == (embed_dim, head_dim), f'W_q shape wrong: {W_q_ex.shape}'\n",
        "    assert W_k_ex.shape == (embed_dim, head_dim), f'W_k shape wrong: {W_k_ex.shape}'\n",
        "    assert W_v_ex.shape == (embed_dim, head_dim), f'W_v shape wrong: {W_v_ex.shape}'\n",
        "    print(f'   W_q shape: {W_q_ex.shape} âœ“')\n",
        "    print(f'   W_k shape: {W_k_ex.shape} âœ“')\n",
        "    print(f'   W_v shape: {W_v_ex.shape} âœ“')\n",
        "    print('ğŸ‰ Exercise 2.A complete!')\n",
        "else:\n",
        "    print('â³ Replace None with your code and run again')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.B: Compute Q, K, V (ğŸŸ¢ Easy)\n",
        "\n",
        "**Task**: Given input X and weight matrices, compute the Query, Key, and Value matrices.\n",
        "\n",
        "**Formula**:\n",
        "- Q = X @ W_q\n",
        "- K = X @ W_k\n",
        "- V = X @ W_v\n",
        "\n",
        "**Given**:\n",
        "- X: Input matrix of shape (seq_len, embed_dim) = (3, 4)\n",
        "- Your weight matrices from Exercise 2.A\n",
        "\n",
        "**Expected shapes**: Q, K, V should all be (3, 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.B: Compute Q, K, V\n",
        "\n",
        "# Given input (same as used in the tutorial)\n",
        "X_ex = torch.tensor([\n",
        "    [1.0, 0.0, 1.0, 0.0],   # \"the\"\n",
        "    [0.0, 2.0, 0.0, 2.0],   # \"hot\"\n",
        "    [1.0, 1.0, 1.0, 1.0]    # \"dog\"\n",
        "])\n",
        "\n",
        "# Use weight matrices from 2.A (or create them if you skipped)\n",
        "if W_q_ex is None:\n",
        "    W_q_ex = torch.randn(4, 4)\n",
        "    W_k_ex = torch.randn(4, 4)\n",
        "    W_v_ex = torch.randn(4, 4)\n",
        "\n",
        "# TODO: Compute Q, K, V by multiplying X with weight matrices\n",
        "# Hint: Use the @ operator for matrix multiplication\n",
        "\n",
        "Q_ex = None  # YOUR CODE HERE\n",
        "K_ex = None  # YOUR CODE HERE\n",
        "V_ex = None  # YOUR CODE HERE\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Verification\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if Q_ex is not None and K_ex is not None and V_ex is not None:\n",
        "    print('âœ… Checking your Q, K, V matrices...')\n",
        "    assert Q_ex.shape == (3, 4), f'Q shape wrong: {Q_ex.shape}'\n",
        "    assert K_ex.shape == (3, 4), f'K shape wrong: {K_ex.shape}'\n",
        "    assert V_ex.shape == (3, 4), f'V shape wrong: {V_ex.shape}'\n",
        "    print(f'   Q shape: {Q_ex.shape} âœ“')\n",
        "    print(f'   K shape: {K_ex.shape} âœ“')\n",
        "    print(f'   V shape: {V_ex.shape} âœ“')\n",
        "    print('ğŸ‰ Exercise 2.B complete!')\n",
        "else:\n",
        "    print('â³ Replace None with your code and run again')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.C: Compute Raw Attention Scores (ğŸŸ¢ Easy)\n",
        "\n",
        "**Task**: Compute the raw attention scores by multiplying Q with K transposed.\n",
        "\n",
        "**Formula**:\n",
        "$$\\text{Scores} = Q \\cdot K^T$$\n",
        "\n",
        "**Why transpose K?**: \n",
        "- Q has shape (seq_len, head_dim) = (3, 4)\n",
        "- K has shape (seq_len, head_dim) = (3, 4)\n",
        "- K^T has shape (head_dim, seq_len) = (4, 3)\n",
        "- Q @ K^T gives (3, 3) - a score for each query-key pair!\n",
        "\n",
        "**Expected shape**: (3, 3) - one score for each pair of tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.C: Compute Raw Attention Scores\n",
        "\n",
        "# Use Q and K from previous exercise\n",
        "if Q_ex is None or K_ex is None:\n",
        "    Q_ex = X_ex @ W_q_ex\n",
        "    K_ex = X_ex @ W_k_ex\n",
        "\n",
        "# TODO: Compute raw attention scores\n",
        "# Hint: Multiply Q with K transposed. Use .T or .transpose() for transpose\n",
        "\n",
        "scores_ex = None  # YOUR CODE HERE\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Verification\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if scores_ex is not None:\n",
        "    print('âœ… Checking your attention scores...')\n",
        "    assert scores_ex.shape == (3, 3), f'Scores shape wrong: {scores_ex.shape}'\n",
        "    print(f'   Scores shape: {scores_ex.shape} âœ“')\n",
        "    print(f'\\n   Your scores matrix:')\n",
        "    print(scores_ex)\n",
        "    print('\\n   Each value [i,j] = how much token i attends to token j')\n",
        "    print('ğŸ‰ Exercise 2.C complete!')\n",
        "else:\n",
        "    print('â³ Replace None with your code and run again')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.D: Scale the Scores (ğŸŸ¢ Easy)\n",
        "\n",
        "**Task**: Scale the attention scores by âˆšd_k to prevent softmax saturation.\n",
        "\n",
        "**Formula**:\n",
        "$$\\text{ScaledScores} = \\frac{\\text{Scores}}{\\sqrt{d_k}}$$\n",
        "\n",
        "**Why scale?**:\n",
        "- Large scores â†’ softmax produces values close to 0 or 1\n",
        "- This causes vanishing gradients during training\n",
        "- Dividing by âˆšd_k keeps values in a reasonable range\n",
        "\n",
        "**Given**: d_k = 4 (the dimension of keys)\n",
        "\n",
        "**Hint**: Use `math.sqrt()` or `** 0.5` to compute square root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.D: Scale the Scores\n",
        "\n",
        "import math\n",
        "\n",
        "# Use scores from previous exercise\n",
        "if scores_ex is None:\n",
        "    scores_ex = Q_ex @ K_ex.T\n",
        "\n",
        "# Get the dimension of keys\n",
        "d_k = K_ex.shape[-1]  # This should be 4\n",
        "\n",
        "# TODO: Scale the scores by dividing by sqrt(d_k)\n",
        "# Hint: scaled_scores = scores / math.sqrt(d_k)\n",
        "\n",
        "scaled_scores_ex = None  # YOUR CODE HERE\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Verification\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if scaled_scores_ex is not None:\n",
        "    print('âœ… Checking your scaled scores...')\n",
        "    print(f'   d_k = {d_k}')\n",
        "    print(f'   âˆšd_k = {math.sqrt(d_k):.2f}')\n",
        "    print(f'\\n   Before scaling (max absolute value): {scores_ex.abs().max():.2f}')\n",
        "    print(f'   After scaling (max absolute value): {scaled_scores_ex.abs().max():.2f}')\n",
        "    \n",
        "    # Check that scaling was applied correctly\n",
        "    expected = scores_ex / math.sqrt(d_k)\n",
        "    assert torch.allclose(scaled_scores_ex, expected), 'Scaling factor seems incorrect'\n",
        "    print(f'\\n   Scaled scores matrix:')\n",
        "    print(scaled_scores_ex)\n",
        "    print('\\nğŸ‰ Exercise 2.D complete!')\n",
        "else:\n",
        "    print('â³ Replace None with your code and run again')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.E: Apply Softmax (ğŸŸ¡ Medium)\n",
        "\n",
        "**Task**: Convert scaled scores into attention weights using softmax.\n",
        "\n",
        "**Formula**:\n",
        "$$\\text{AttentionWeights}_{i,j} = \\frac{e^{\\text{ScaledScores}_{i,j}}}{\\sum_k e^{\\text{ScaledScores}_{i,k}}}$$\n",
        "\n",
        "**Key insight**: Apply softmax along the **last dimension** (dim=-1)\n",
        "- Each ROW should sum to 1\n",
        "- Row i = probability distribution of token i's attention over all tokens\n",
        "\n",
        "**Expected**:\n",
        "- Shape: (3, 3)\n",
        "- Each row sums to 1.0\n",
        "- All values between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.E: Apply Softmax\n",
        "\n",
        "# Use scaled scores from previous exercise\n",
        "if scaled_scores_ex is None:\n",
        "    scaled_scores_ex = scores_ex / math.sqrt(d_k)\n",
        "\n",
        "# TODO: Apply softmax to convert scores to attention weights\n",
        "# Hint: Use torch.softmax(tensor, dim=-1) to apply softmax along rows\n",
        "\n",
        "attention_weights_ex = None  # YOUR CODE HERE\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Verification\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if attention_weights_ex is not None:\n",
        "    print('âœ… Checking your attention weights...')\n",
        "    print(f'   Shape: {attention_weights_ex.shape} âœ“')\n",
        "    \n",
        "    # Check each row sums to 1\n",
        "    row_sums = attention_weights_ex.sum(dim=-1)\n",
        "    print(f'\\n   Row sums (should all be 1.0):')\n",
        "    for i, s in enumerate(row_sums):\n",
        "        status = 'âœ“' if abs(s - 1.0) < 0.001 else 'âœ—'\n",
        "        print(f'      Row {i}: {s:.4f} {status}')\n",
        "    \n",
        "    # Check all values are between 0 and 1\n",
        "    all_valid = (attention_weights_ex >= 0).all() and (attention_weights_ex <= 1).all()\n",
        "    print(f'\\n   All values in [0,1]: {all_valid} âœ“')\n",
        "    \n",
        "    print(f'\\n   Your attention weights matrix:')\n",
        "    print(attention_weights_ex)\n",
        "    print('\\nğŸ‰ Exercise 2.E complete!')\n",
        "else:\n",
        "    print('â³ Replace None with your code and run again')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.F: Compute Final Output (ğŸŸ¢ Easy)\n",
        "\n",
        "**Task**: Multiply attention weights with Values to get context-aware representations.\n",
        "\n",
        "**Formula**:\n",
        "$$\\text{Output} = \\text{AttentionWeights} \\times V$$\n",
        "\n",
        "**What this does**:\n",
        "- Each output row is a weighted sum of all Value vectors\n",
        "- Weights come from the attention weights (how much to attend to each position)\n",
        "- Result: context-aware representations!\n",
        "\n",
        "**Expected shape**: (3, 4) - same as input X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.F: Compute Final Output\n",
        "\n",
        "# Use attention weights and V from previous exercises\n",
        "if attention_weights_ex is None:\n",
        "    attention_weights_ex = torch.softmax(scaled_scores_ex, dim=-1)\n",
        "if V_ex is None:\n",
        "    V_ex = X_ex @ W_v_ex\n",
        "\n",
        "# TODO: Compute the final output by multiplying attention weights with V\n",
        "# Hint: output = attention_weights @ V\n",
        "\n",
        "output_ex = None  # YOUR CODE HERE\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Verification\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if output_ex is not None:\n",
        "    print('âœ… Checking your output...')\n",
        "    assert output_ex.shape == (3, 4), f'Output shape wrong: {output_ex.shape}'\n",
        "    print(f'   Output shape: {output_ex.shape} âœ“')\n",
        "    print(f'\\n   Your context-aware output:')\n",
        "    print(output_ex)\n",
        "    \n",
        "    print('\\n   ğŸ’¡ Each row is now a CONTEXT-AWARE representation!')\n",
        "    print('   The \"dog\" representation is now influenced by \"hot\"!')\n",
        "    print('\\nğŸ‰ Exercise 2.F complete!')\n",
        "else:\n",
        "    print('â³ Replace None with your code and run again')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.G: Complete Self-Attention Function (ğŸ”´ Hard)\n",
        "\n",
        "**Task**: Combine all the steps into a complete self-attention function!\n",
        "\n",
        "**Your function should**:\n",
        "1. Take inputs: X, W_q, W_k, W_v\n",
        "2. Compute Q, K, V\n",
        "3. Compute scaled attention scores\n",
        "4. Apply softmax\n",
        "5. Compute and return the output\n",
        "\n",
        "This is the moment of truth - you're implementing the core of transformers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.G: Complete Self-Attention Function\n",
        "\n",
        "def my_self_attention(X, W_q, W_k, W_v):\n",
        "    \"\"\"\n",
        "    YOUR IMPLEMENTATION of self-attention!\n",
        "    \n",
        "    Args:\n",
        "        X: Input matrix (seq_len, embed_dim)\n",
        "        W_q, W_k, W_v: Weight matrices\n",
        "    \n",
        "    Returns:\n",
        "        output: Context-aware representations\n",
        "        attention_weights: The attention probabilities\n",
        "    \"\"\"\n",
        "    # TODO: Implement all steps!\n",
        "    \n",
        "    # Step 1: Compute Q, K, V\n",
        "    Q = None  # YOUR CODE\n",
        "    K = None  # YOUR CODE\n",
        "    V = None  # YOUR CODE\n",
        "    \n",
        "    # Step 2: Compute scaled attention scores\n",
        "    d_k = None  # YOUR CODE - get dimension of keys\n",
        "    scores = None  # YOUR CODE - compute Q @ K^T / sqrt(d_k)\n",
        "    \n",
        "    # Step 3: Apply softmax\n",
        "    attention_weights = None  # YOUR CODE\n",
        "    \n",
        "    # Step 4: Compute output\n",
        "    output = None  # YOUR CODE\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Test your implementation\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "print('ğŸ§ª Testing your self-attention implementation...')\n",
        "print('='*60)\n",
        "\n",
        "# Test inputs\n",
        "X_test = torch.tensor([\n",
        "    [1.0, 0.0, 1.0, 0.0],\n",
        "    [0.0, 2.0, 0.0, 2.0],\n",
        "    [1.0, 1.0, 1.0, 1.0]\n",
        "])\n",
        "\n",
        "torch.manual_seed(42)  # For reproducibility\n",
        "W_q_test = torch.randn(4, 4)\n",
        "W_k_test = torch.randn(4, 4)\n",
        "W_v_test = torch.randn(4, 4)\n",
        "\n",
        "try:\n",
        "    out_test, attn_test = my_self_attention(X_test, W_q_test, W_k_test, W_v_test)\n",
        "    \n",
        "    if out_test is None or attn_test is None:\n",
        "        print('â³ Your function returned None. Fill in the implementation!')\n",
        "    else:\n",
        "        # Verify output shape\n",
        "        assert out_test.shape == (3, 4), f'Output shape wrong: {out_test.shape}'\n",
        "        assert attn_test.shape == (3, 3), f'Attention shape wrong: {attn_test.shape}'\n",
        "        \n",
        "        # Verify attention weights sum to 1\n",
        "        row_sums = attn_test.sum(dim=-1)\n",
        "        assert torch.allclose(row_sums, torch.ones(3)), 'Attention rows should sum to 1'\n",
        "        \n",
        "        print('âœ… Output shape: (3, 4) âœ“')\n",
        "        print('âœ… Attention shape: (3, 3) âœ“')\n",
        "        print('âœ… Attention rows sum to 1 âœ“')\n",
        "        print('\\n' + '='*60)\n",
        "        print('ğŸ‰ğŸ‰ğŸ‰ CONGRATULATIONS! ğŸ‰ğŸ‰ğŸ‰')\n",
        "        print('='*60)\n",
        "        print('\\nYou successfully implemented Self-Attention from scratch!')\n",
        "        print('This is the core mechanism that powers GPT, BERT, and all transformers!')\n",
        "        print('\\nNow see the reference implementation below for comparison.')\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f'âŒ Error: {e}')\n",
        "    print('   Check your implementation and try again.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2.7: Complete Self-Attention Implementation\n",
        "\n",
        "### ğŸ¯ Putting It All Together\n",
        "\n",
        "Now we implement the complete self-attention function that combines all 4 steps.\n",
        "\n",
        "### ğŸ“ The Complete Algorithm\n",
        "\n",
        "```python\n",
        "def self_attention(X, W_q, W_k, W_v):\n",
        "    # Step 1: Create Q, K, V\n",
        "    Q = X @ W_q\n",
        "    K = X @ W_k\n",
        "    V = X @ W_v\n",
        "    \n",
        "    # Step 2: Compute scores\n",
        "    d_k = K.shape[-1]\n",
        "    scores = (Q @ K.T) / sqrt(d_k)\n",
        "    \n",
        "    # Step 3: Softmax\n",
        "    weights = softmax(scores)\n",
        "    \n",
        "    # Step 4: Apply to values\n",
        "    output = weights @ V\n",
        "    \n",
        "    return output, weights\n",
        "```\n",
        "\n",
        "### ğŸ”§ Features\n",
        "- Clean, readable implementation\n",
        "- Returns both output and attention weights\n",
        "- Can add optional masking for causal attention\n",
        "- Foundation for multi-head attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def self_attention(X, W_q, W_k, W_v, mask=None):\n",
        "    \"\"\"\n",
        "    Complete self-attention implementation.\n",
        "    \n",
        "    Args:\n",
        "        X: Input matrix (seq_len, embed_dim)\n",
        "        W_q, W_k, W_v: Weight matrices for Q, K, V\n",
        "        mask: Optional attention mask\n",
        "    \n",
        "    Returns:\n",
        "        output: Context-aware representations (seq_len, embed_dim)\n",
        "        attention_weights: Attention probabilities (seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    # Step 1: Create Q, K, V\n",
        "    Q = X @ W_q\n",
        "    K = X @ W_k\n",
        "    V = X @ W_v\n",
        "    \n",
        "    # Step 2: Compute scaled scores\n",
        "    d_k = K.shape[-1]\n",
        "    scores = (Q @ K.T) / math.sqrt(d_k)\n",
        "    \n",
        "    # Apply mask if provided (for causal/masked attention)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    \n",
        "    # Step 3: Softmax normalization\n",
        "    attention_weights = torch.softmax(scores, dim=-1)\n",
        "    \n",
        "    # Step 4: Apply to values\n",
        "    output = attention_weights @ V\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# Test the function\n",
        "print('='*70)\n",
        "print('TESTING COMPLETE SELF-ATTENTION FUNCTION')\n",
        "print('='*70)\n",
        "\n",
        "out, attn = self_attention(X, W_q, W_k, W_v)\n",
        "\n",
        "print('\\nâœ… Self-attention function works!')\n",
        "print(f'\\nInput shape: {X.shape}')\n",
        "print(f'Output shape: {out.shape}')\n",
        "print(f'Attention weights shape: {attn.shape}')\n",
        "\n",
        "print('\\nOutput (context-aware representations):')\n",
        "print(out)\n",
        "\n",
        "print('\\nAttention weights:')\n",
        "print(attn)\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('ğŸ‰ PART 2 COMPLETE!')\n",
        "print('='*70)\n",
        "print('\\nYou now understand self-attention - the heart of transformers!')\n",
        "print('\\nNext: Multi-Head Self-Attention (learning multiple patterns!)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2.8: Causal (Masked) Attention - For Autoregressive Models\n",
        "\n",
        "### ğŸ”® The Problem with Standard Attention\n",
        "\n",
        "In **autoregressive** models (like GPT), we generate one token at a time:\n",
        "\n",
        "```\n",
        "Generating \"The cat sat on the mat\"\n",
        "\n",
        "Step 1: Generate \"The\"      â†’ can see: nothing\n",
        "Step 2: Generate \"cat\"      â†’ can see: \"The\"\n",
        "Step 3: Generate \"sat\"      â†’ can see: \"The cat\"\n",
        "Step 4: Generate \"on\"       â†’ can see: \"The cat sat\"\n",
        "...\n",
        "\n",
        "PROBLEM: Standard attention lets every token see ALL other tokens!\n",
        "         This would let us \"cheat\" by seeing future tokens.\n",
        "```\n",
        "\n",
        "### âœ… The Solution: Causal Mask\n",
        "\n",
        "Mask out \"future\" positions so tokens can only attend to previous tokens:\n",
        "\n",
        "```\n",
        "Causal Mask (Lower Triangular):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "Token positions:   0    1    2    3    4\n",
        "                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    \"The\"    0   â”‚  1    0    0    0    0  â”‚  â† can only see itself\n",
        "    \"cat\"    1   â”‚  1    1    0    0    0  â”‚  â† can see \"The\", itself\n",
        "    \"sat\"    2   â”‚  1    1    1    0    0  â”‚  â† can see \"The\", \"cat\", itself\n",
        "    \"on\"     3   â”‚  1    1    1    1    0  â”‚  â† can see previous 3 + itself\n",
        "    \"the\"    4   â”‚  1    1    1    1    1  â”‚  â† can see all previous\n",
        "                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                    â†‘\n",
        "              1 = can attend\n",
        "              0 = BLOCKED (masked to -âˆ)\n",
        "              \n",
        "This is a LOWER TRIANGULAR matrix!\n",
        "```\n",
        "\n",
        "### ğŸ”¢ Mathematics\n",
        "\n",
        "Apply mask **before** softmax:\n",
        "\n",
        "$$\n",
        "\\text{CausalAttention} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right) V\n",
        "$$\n",
        "\n",
        "where mask $M$ is:\n",
        "$$\n",
        "M_{ij} = \\begin{cases}\n",
        "0 & \\text{if } i \\geq j \\text{ (can attend)} \\\\\n",
        "-\\infty & \\text{if } i < j \\text{ (blocked)}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "**Why $-\\infty$?** Because $\\text{softmax}(-\\infty) = 0$ â†’ zero attention!\n",
        "\n",
        "### ğŸ¯ Why This Matters\n",
        "\n",
        "| Model Type | Uses Causal Mask? | Examples |\n",
        "|------------|------------------|----------|\n",
        "| **Decoder-only** | âœ… Yes | GPT, LLaMA, Claude |\n",
        "| **Encoder-only** | âŒ No | BERT, RoBERTa |\n",
        "| **Encoder-Decoder** | âœ… Decoder only | T5, BART |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementing Causal (Masked) Attention\n",
        "\n",
        "def create_causal_mask(seq_len: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Create a causal (lower triangular) mask.\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Length of sequence\n",
        "    \n",
        "    Returns:\n",
        "        mask: (seq_len, seq_len) with 1s in lower triangle, 0s above\n",
        "    \"\"\"\n",
        "    mask = torch.tril(torch.ones(seq_len, seq_len))\n",
        "    return mask\n",
        "\n",
        "def causal_self_attention(X, W_q, W_k, W_v):\n",
        "    \"\"\"\n",
        "    Self-attention with causal masking for autoregressive models.\n",
        "    \"\"\"\n",
        "    # Step 1: Create Q, K, V\n",
        "    Q = X @ W_q\n",
        "    K = X @ W_k\n",
        "    V = X @ W_v\n",
        "    \n",
        "    # Step 2: Compute scaled scores\n",
        "    d_k = K.shape[-1]\n",
        "    scores = (Q @ K.T) / math.sqrt(d_k)\n",
        "    \n",
        "    # Step 3: Apply causal mask\n",
        "    seq_len = X.shape[0]\n",
        "    mask = create_causal_mask(seq_len)\n",
        "    \n",
        "    # Where mask is 0, set score to -infinity\n",
        "    scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "    \n",
        "    # Step 4: Softmax (masked positions become 0)\n",
        "    attention_weights = torch.softmax(scores, dim=-1)\n",
        "    \n",
        "    # Step 5: Apply to values\n",
        "    output = attention_weights @ V\n",
        "    \n",
        "    return output, attention_weights, mask\n",
        "\n",
        "# Demonstrate causal masking\n",
        "print('='*70)\n",
        "print('CAUSAL (MASKED) ATTENTION')\n",
        "print('='*70)\n",
        "\n",
        "# Create simple test input\n",
        "X_causal = torch.tensor([\n",
        "    [1.0, 0.0, 0.0, 0.0],  # \"The\"\n",
        "    [0.0, 1.0, 0.0, 0.0],  # \"cat\"\n",
        "    [0.0, 0.0, 1.0, 0.0],  # \"sat\"\n",
        "    [0.0, 0.0, 0.0, 1.0],  # \"on\"\n",
        "])\n",
        "\n",
        "torch.manual_seed(42)\n",
        "W_q_c = torch.randn(4, 4) * 0.1\n",
        "W_k_c = torch.randn(4, 4) * 0.1\n",
        "W_v_c = torch.randn(4, 4) * 0.1\n",
        "\n",
        "output, attn_weights, mask = causal_self_attention(X_causal, W_q_c, W_k_c, W_v_c)\n",
        "\n",
        "# Visualize causal mask\n",
        "tokens = ['The', 'cat', 'sat', 'on']\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Causal Mask\n",
        "ax = axes[0]\n",
        "im = ax.imshow(mask.numpy(), cmap='Blues', vmin=0, vmax=1)\n",
        "ax.set_title('Causal Mask\\n(Lower Triangular)', fontweight='bold', fontsize=14)\n",
        "ax.set_xticks(range(4))\n",
        "ax.set_yticks(range(4))\n",
        "ax.set_xticklabels(tokens)\n",
        "ax.set_yticklabels(tokens)\n",
        "ax.set_xlabel('Key (TO)', fontweight='bold')\n",
        "ax.set_ylabel('Query (FROM)', fontweight='bold')\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        val = int(mask[i, j].item())\n",
        "        color = 'white' if val == 1 else 'black'\n",
        "        ax.text(j, i, val, ha='center', va='center', color=color, fontsize=14, fontweight='bold')\n",
        "plt.colorbar(im, ax=ax, label='1=Can Attend, 0=Masked')\n",
        "\n",
        "# Attention Weights (after masking + softmax)\n",
        "ax = axes[1]\n",
        "im = ax.imshow(attn_weights.detach().numpy(), cmap='Greens', vmin=0, vmax=1)\n",
        "ax.set_title('Attention Weights\\n(After Masking + Softmax)', fontweight='bold', fontsize=14)\n",
        "ax.set_xticks(range(4))\n",
        "ax.set_yticks(range(4))\n",
        "ax.set_xticklabels(tokens)\n",
        "ax.set_yticklabels(tokens)\n",
        "ax.set_xlabel('Key (TO)', fontweight='bold')\n",
        "ax.set_ylabel('Query (FROM)', fontweight='bold')\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        val = attn_weights[i, j].item()\n",
        "        color = 'white' if val > 0.4 else 'black'\n",
        "        ax.text(j, i, f'{val:.2f}', ha='center', va='center', color=color, fontsize=11, fontweight='bold')\n",
        "plt.colorbar(im, ax=ax, label='Attention Weight')\n",
        "\n",
        "# Show the diagonal pattern more clearly\n",
        "ax = axes[2]\n",
        "ax.text(0.5, 0.95, 'What Each Token Can See:', ha='center', fontsize=14, fontweight='bold', transform=ax.transAxes)\n",
        "y_pos = 0.8\n",
        "for i, token in enumerate(tokens):\n",
        "    can_see = tokens[:i+1]\n",
        "    ax.text(0.1, y_pos, f'\"{token}\":', fontsize=12, fontweight='bold', transform=ax.transAxes)\n",
        "    ax.text(0.35, y_pos, f'â†’ {can_see}', fontsize=12, transform=ax.transAxes)\n",
        "    y_pos -= 0.15\n",
        "ax.text(0.5, 0.15, 'âœ¨ Each token only sees\\npast tokens + itself!', \n",
        "        ha='center', fontsize=13, style='italic', transform=ax.transAxes,\n",
        "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nğŸ’¡ Key Points:')\n",
        "print('   â€¢ Upper triangle is ZERO (future tokens are blocked)')\n",
        "print('   â€¢ Each row sums to 1.0 (valid probability distribution)')\n",
        "print('   â€¢ This is how GPT, LLaMA, and Claude work!')\n",
        "print('   â€¢ BERT does NOT use causal masking (sees all tokens)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2.9: Positional Encodings - Teaching Position to Attention\n",
        "\n",
        "### ğŸ¤” The Problem: Attention is Position-Blind!\n",
        "\n",
        "Self-attention treats input as a **set**, not a **sequence**:\n",
        "\n",
        "```\n",
        "Problem:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\"The cat sat on the mat\"  â†’  Same attention weights as:\n",
        "\"mat the on sat cat The\"  â†’  (words are just a bag!)\n",
        "\n",
        "Attention doesn't know word ORDER!\n",
        "\n",
        "But order matters:\n",
        "  \"Dog bites man\" â‰  \"Man bites dog\"\n",
        "```\n",
        "\n",
        "### âœ… The Solution: Add Position Information\n",
        "\n",
        "Inject position information **into the embeddings**:\n",
        "\n",
        "$$\n",
        "\\text{Input} = \\text{WordEmbedding} + \\text{PositionalEncoding}\n",
        "$$\n",
        "\n",
        "```\n",
        "Word Embeddings:           Position Encodings:        Final Input:\n",
        "   \"The\"  = [0.1, 0.2, ...]    pos_0 = [0.0, 1.0, ...]    [0.1, 1.2, ...]\n",
        "   \"cat\"  = [0.5, 0.6, ...]  + pos_1 = [0.8, 0.6, ...]  = [1.3, 1.2, ...]\n",
        "   \"sat\"  = [0.9, 1.0, ...]    pos_2 = [0.9, -0.4, ...]   [1.8, 0.6, ...]\n",
        "```\n",
        "\n",
        "### ğŸ“Š Types of Positional Encodings\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                   POSITIONAL ENCODING METHODS                           â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  1. SINUSOIDAL (Original Transformer - 2017)                           â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚\n",
        "â”‚     PE(pos, 2i)   = sin(pos / 10000^(2i/d))                            â”‚\n",
        "â”‚     PE(pos, 2i+1) = cos(pos / 10000^(2i/d))                            â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚     âœ… No learnable parameters                                         â”‚\n",
        "â”‚     âœ… Can extrapolate to longer sequences                             â”‚\n",
        "â”‚     âŒ Fixed, not task-adaptive                                        â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  2. LEARNED (BERT, GPT-2)                                              â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                              â”‚\n",
        "â”‚     PE = nn.Embedding(max_seq_len, d_model)                            â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚     âœ… Task-adaptive                                                   â”‚\n",
        "â”‚     âŒ Fixed maximum sequence length                                   â”‚\n",
        "â”‚     âŒ Cannot extrapolate                                              â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  3. RoPE - Rotary Position Embeddings (LLaMA, Mistral)                â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                â”‚\n",
        "â”‚     Rotates query/key vectors based on position                        â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚     âœ… Relative position information                                   â”‚\n",
        "â”‚     âœ… Efficient implementation                                        â”‚\n",
        "â”‚     âœ… Better length extrapolation                                     â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  4. ALiBi - Attention with Linear Biases (BLOOM)                       â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”‚\n",
        "â”‚     Adds linear bias based on distance to attention scores             â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚     âœ… No position embeddings needed                                   â”‚\n",
        "â”‚     âœ… Good extrapolation                                              â”‚\n",
        "â”‚     âœ… Memory efficient                                                â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ”¢ Sinusoidal Encoding Formula (Original Transformer)\n",
        "\n",
        "For position $\\text{pos}$ and dimension $i$:\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "PE_{(\\text{pos}, 2i)} &= \\sin\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right) \\\\\n",
        "PE_{(\\text{pos}, 2i+1)} &= \\cos\\left(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}\\right)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "**Why sin/cos?** They create unique patterns for each position that the model can learn to interpret!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementing Sinusoidal Positional Encoding\n",
        "\n",
        "def sinusoidal_position_encoding(seq_len: int, d_model: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Create sinusoidal positional encodings (Original Transformer).\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Maximum sequence length\n",
        "        d_model: Embedding dimension\n",
        "    \n",
        "    Returns:\n",
        "        pe: (seq_len, d_model) positional encoding matrix\n",
        "    \"\"\"\n",
        "    position = torch.arange(seq_len).unsqueeze(1).float()  # (seq_len, 1)\n",
        "    \n",
        "    # Create dimension indices\n",
        "    div_term = torch.exp(\n",
        "        torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
        "    )  # (d_model/2,)\n",
        "    \n",
        "    # Initialize encoding matrix\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "    \n",
        "    # Apply sin to even indices, cos to odd indices\n",
        "    pe[:, 0::2] = torch.sin(position * div_term)  # Even dimensions\n",
        "    pe[:, 1::2] = torch.cos(position * div_term)  # Odd dimensions\n",
        "    \n",
        "    return pe\n",
        "\n",
        "# Visualize positional encodings\n",
        "print('='*70)\n",
        "print('SINUSOIDAL POSITIONAL ENCODINGS')\n",
        "print('='*70)\n",
        "\n",
        "seq_len = 50\n",
        "d_model = 64\n",
        "pe = sinusoidal_position_encoding(seq_len, d_model)\n",
        "\n",
        "print(f'\\nPositional Encoding shape: {pe.shape}')\n",
        "print(f'  â†’ {seq_len} positions, {d_model} dimensions each')\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# Heatmap of all encodings\n",
        "ax = axes[0, 0]\n",
        "im = ax.imshow(pe.numpy(), aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\n",
        "ax.set_xlabel('Dimension', fontweight='bold')\n",
        "ax.set_ylabel('Position', fontweight='bold')\n",
        "ax.set_title('Positional Encoding Heatmap\\n(sin/cos patterns)', fontweight='bold')\n",
        "plt.colorbar(im, ax=ax, label='Encoding Value')\n",
        "\n",
        "# Individual dimension patterns\n",
        "ax = axes[0, 1]\n",
        "dims_to_plot = [0, 1, 10, 20, 40, 60]\n",
        "for d in dims_to_plot:\n",
        "    if d < d_model:\n",
        "        ax.plot(pe[:, d].numpy(), label=f'Dim {d}', alpha=0.8, linewidth=2)\n",
        "ax.set_xlabel('Position', fontweight='bold')\n",
        "ax.set_ylabel('Encoding Value', fontweight='bold')\n",
        "ax.set_title('Position Encoding by Dimension\\n(different frequencies)', fontweight='bold')\n",
        "ax.legend(loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Show that nearby positions are similar\n",
        "ax = axes[1, 0]\n",
        "pos_0 = pe[0]\n",
        "pos_1 = pe[1]\n",
        "pos_25 = pe[25]\n",
        "pos_49 = pe[49]\n",
        "\n",
        "similarities = [\n",
        "    torch.dot(pos_0, pos_0).item(),  # Self\n",
        "    torch.dot(pos_0, pos_1).item(),   # Adjacent\n",
        "    torch.dot(pos_0, pos_25).item(),  # Middle\n",
        "    torch.dot(pos_0, pos_49).item(),  # Far\n",
        "]\n",
        "labels = ['Pos 0â†”0\\n(self)', 'Pos 0â†”1\\n(adjacent)', 'Pos 0â†”25\\n(middle)', 'Pos 0â†”49\\n(far)']\n",
        "colors = ['#2ecc71', '#3498db', '#f39c12', '#e74c3c']\n",
        "bars = ax.bar(labels, similarities, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax.set_ylabel('Dot Product Similarity', fontweight='bold')\n",
        "ax.set_title('Position Similarity\\n(nearby = more similar)', fontweight='bold')\n",
        "ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "for bar, val in zip(bars, similarities):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
        "            f'{val:.1f}', ha='center', fontweight='bold')\n",
        "\n",
        "# How it's added to embeddings\n",
        "ax = axes[1, 1]\n",
        "ax.text(0.5, 0.9, 'How Position Encoding is Used:', \n",
        "        ha='center', fontsize=14, fontweight='bold', transform=ax.transAxes)\n",
        "ax.text(0.5, 0.7, 'input = word_embedding + positional_encoding', \n",
        "        ha='center', fontsize=13, family='monospace', transform=ax.transAxes,\n",
        "        bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
        "\n",
        "ax.text(0.5, 0.45, \n",
        "'''Example:\n",
        "  \"cat\" at position 0: [0.5, 0.6, ...] + PE[0] = [0.5, 1.6, ...]\n",
        "  \"cat\" at position 5: [0.5, 0.6, ...] + PE[5] = [0.2, 0.9, ...]\n",
        "\n",
        "  Same word, DIFFERENT representations based on position!''',\n",
        "        ha='center', fontsize=11, transform=ax.transAxes, family='monospace')\n",
        "ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nğŸ’¡ Key Insights:')\n",
        "print('   â€¢ Each position gets a UNIQUE encoding')\n",
        "print('   â€¢ Low dimensions = slow oscillation (long-range patterns)')\n",
        "print('   â€¢ High dimensions = fast oscillation (local patterns)')\n",
        "print('   â€¢ Similar positions have similar encodings')\n",
        "print('   â€¢ This lets attention learn position-aware patterns!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ› ï¸ BUILD IT YOURSELF: Positional Encodings Step-by-Step\n",
        "\n",
        "Now implement different positional encoding methods yourself!\n",
        "\n",
        "### ğŸ“‹ What You'll Implement\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                YOUR POSITIONAL ENCODING JOURNEY                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Exercise 2.H: Build sinusoidal position encodings from scratch       â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 2.I: Implement learned position embeddings                  â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 2.J: Implement Rotary Position Embeddings (RoPE)            â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 2.K: Apply position encodings to attention                  â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.H: Build Sinusoidal Position Encodings from Scratch\n",
        "\n",
        "**The Formula Explained:**\n",
        "\n",
        "For position $pos$ and dimension $i$:\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\n",
        "\n",
        "**Why this specific formula?**\n",
        "- $10000^{2i/d}$ creates different **frequencies** for each dimension\n",
        "- Low dimensions â†’ slow oscillation (capture long-range position)\n",
        "- High dimensions â†’ fast oscillation (capture fine position differences)\n",
        "- Sin/cos pair allows learning **relative** positions\n",
        "\n",
        "**Your Task:**\n",
        "1. Create position indices\n",
        "2. Calculate the frequency divisor term\n",
        "3. Apply sin to even dimensions, cos to odd dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.H: Build Sinusoidal Position Encodings from Scratch\n",
        "\n",
        "import torch\n",
        "import math\n",
        "\n",
        "def my_sinusoidal_encoding(seq_len: int, d_model: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Build sinusoidal positional encodings from scratch.\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Maximum sequence length (number of positions)\n",
        "        d_model: Embedding dimension (must be even)\n",
        "        \n",
        "    Returns:\n",
        "        pe: (seq_len, d_model) tensor of positional encodings\n",
        "    \"\"\"\n",
        "    # ============ YOUR CODE HERE ============\n",
        "    \n",
        "    # Step 1: Create position indices\n",
        "    # positions = [0, 1, 2, ..., seq_len-1], shape (seq_len, 1)\n",
        "    position = # YOUR CODE: torch.arange(seq_len).unsqueeze(1).float()\n",
        "    \n",
        "    # Step 2: Create dimension indices for the formula\n",
        "    # For d_model=8: dim_indices = [0, 2, 4, 6] (even indices only!)\n",
        "    dim_indices = # YOUR CODE: torch.arange(0, d_model, 2).float()\n",
        "    \n",
        "    # Step 3: Calculate the divisor term: 10000^(2i/d_model)\n",
        "    # Using exp and log for numerical stability:\n",
        "    # 10000^(2i/d) = exp(log(10000) * 2i/d) = exp(2i * log(10000) / d)\n",
        "    # But since we're using dim_indices which are already 2i:\n",
        "    # divisor = exp(dim_indices * log(10000) / d_model)\n",
        "    \n",
        "    div_term = # YOUR CODE: torch.exp(dim_indices * (-math.log(10000.0) / d_model))\n",
        "    \n",
        "    # Step 4: Initialize the encoding matrix\n",
        "    pe = # YOUR CODE: torch.zeros(seq_len, d_model)\n",
        "    \n",
        "    # Step 5: Fill in sin values for even dimensions (0, 2, 4, ...)\n",
        "    # pe[:, 0::2] means all rows, columns 0, 2, 4, ...\n",
        "    # angle = position * div_term (broadcasting: (seq_len, 1) * (d_model/2,))\n",
        "    \n",
        "    pe[:, 0::2] = # YOUR CODE: torch.sin(position * div_term)\n",
        "    \n",
        "    # Step 6: Fill in cos values for odd dimensions (1, 3, 5, ...)\n",
        "    \n",
        "    pe[:, 1::2] = # YOUR CODE: torch.cos(position * div_term)\n",
        "    \n",
        "    # =========================================\n",
        "    \n",
        "    return pe\n",
        "\n",
        "\n",
        "# Test your implementation\n",
        "seq_len = 10\n",
        "d_model = 16\n",
        "\n",
        "pe = my_sinusoidal_encoding(seq_len, d_model)\n",
        "\n",
        "print(f\"Positional encoding shape: {pe.shape}\")\n",
        "print(f\"\\nFirst 3 positions, first 8 dimensions:\")\n",
        "print(pe[:3, :8])\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.imshow(pe.numpy(), aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\n",
        "plt.colorbar(label='Encoding Value')\n",
        "plt.xlabel('Dimension', fontweight='bold')\n",
        "plt.ylabel('Position', fontweight='bold')\n",
        "plt.title('Your Sinusoidal Positional Encoding', fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "# Verify properties\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Values bounded between -1 and 1:\", pe.min().item() >= -1 and pe.max().item() <= 1)\n",
        "print(\"âœ… Each position has unique encoding\")\n",
        "print(\"âœ… Low dimensions oscillate slowly, high dimensions oscillate fast\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.I: Implement Learned Position Embeddings\n",
        "\n",
        "**The Simpler Approach:** Instead of fixed sin/cos, just LEARN the position encodings!\n",
        "\n",
        "```\n",
        "Learned Embeddings (BERT, GPT-2):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "\n",
        "# Each position gets a learnable vector\n",
        "pos_0 â†’ [learnable d_model values]\n",
        "pos_1 â†’ [learnable d_model values]\n",
        "...\n",
        "pos_n â†’ [learnable d_model values]\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        "- âœ… Task-adaptive (model learns best positions)\n",
        "- âœ… Simple to implement\n",
        "\n",
        "**Cons:**\n",
        "- âŒ Fixed maximum length (can't exceed max_seq_len)\n",
        "- âŒ Requires more parameters\n",
        "- âŒ Poor extrapolation to unseen lengths\n",
        "\n",
        "**Your Task:**\n",
        "1. Create a learnable embedding table\n",
        "2. Apply it to position indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.I: Implement Learned Position Embeddings\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class LearnedPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Learned positional embeddings (BERT, GPT-2 style).\n",
        "    \n",
        "    Each position has its own learnable embedding vector.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, max_seq_len: int, d_model: int):\n",
        "        super().__init__()\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # TODO: Create a learnable embedding table\n",
        "        # nn.Embedding(num_embeddings, embedding_dim)\n",
        "        # num_embeddings = number of positions\n",
        "        # embedding_dim = d_model\n",
        "        \n",
        "        self.position_embedding = # YOUR CODE: nn.Embedding(max_seq_len, d_model)\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Add positional embeddings to input.\n",
        "        \n",
        "        Args:\n",
        "            x: (batch, seq_len, d_model) input embeddings\n",
        "            \n",
        "        Returns:\n",
        "            (batch, seq_len, d_model) with position info added\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        # TODO: Create position indices [0, 1, 2, ..., seq_len-1]\n",
        "        positions = # YOUR CODE: torch.arange(seq_len, device=x.device)\n",
        "        \n",
        "        # TODO: Get position embeddings\n",
        "        # Shape: (seq_len, d_model)\n",
        "        pos_emb = # YOUR CODE: self.position_embedding(positions)\n",
        "        \n",
        "        # TODO: Add to input (broadcasts over batch dimension)\n",
        "        output = # YOUR CODE: x + pos_emb\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "# Test your implementation\n",
        "max_seq_len = 512\n",
        "d_model = 64\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "learned_pe = LearnedPositionalEmbedding(max_seq_len, d_model)\n",
        "\n",
        "# Simulate word embeddings\n",
        "word_embeddings = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# Add positional embeddings\n",
        "output = learned_pe(word_embeddings)\n",
        "\n",
        "print(f\"Word embeddings shape: {word_embeddings.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Learnable parameters: {learned_pe.position_embedding.weight.shape}\")\n",
        "print(f\"Total position parameters: {max_seq_len * d_model:,}\")\n",
        "\n",
        "# Visualize the learned embeddings (random initialization)\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.imshow(learned_pe.position_embedding.weight.detach().numpy()[:50], \n",
        "           aspect='auto', cmap='RdBu')\n",
        "plt.colorbar(label='Embedding Value')\n",
        "plt.xlabel('Dimension', fontweight='bold')\n",
        "plt.ylabel('Position', fontweight='bold')\n",
        "plt.title('Learned Position Embeddings (Random Init)\\nThese get updated during training!', \n",
        "          fontweight='bold')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Learned embeddings created!\")\n",
        "print(f\"âœ… Can handle sequences up to {max_seq_len} tokens\")\n",
        "print(\"âœ… Parameters will be learned during training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.J: Implement Rotary Position Embeddings (RoPE)\n",
        "\n",
        "**RoPE is the modern standard!** Used in LLaMA, Mistral, Qwen, and most modern LLMs.\n",
        "\n",
        "**Key Insight:** Instead of ADDING position to embeddings, ROTATE them!\n",
        "\n",
        "```\n",
        "Sinusoidal/Learned:  output = embedding + position_encoding\n",
        "\n",
        "RoPE:                output = ROTATE(embedding, angle_based_on_position)\n",
        "```\n",
        "\n",
        "**Why Rotation Works:**\n",
        "- Dot product of rotated vectors depends on **relative** position\n",
        "- $(q_m \\cdot k_n)$ depends on $(m - n)$, not absolute positions\n",
        "- Better for modeling \"how far apart\" vs \"which position\"\n",
        "\n",
        "**The RoPE Formula:**\n",
        "For a 2D subspace, rotate by angle $\\theta \\cdot pos$:\n",
        "$$\\begin{pmatrix} x'_1 \\\\ x'_2 \\end{pmatrix} = \\begin{pmatrix} \\cos(m\\theta) & -\\sin(m\\theta) \\\\ \\sin(m\\theta) & \\cos(m\\theta) \\end{pmatrix} \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix}$$\n",
        "\n",
        "**Simplified Implementation:**\n",
        "```\n",
        "RoPE splits embedding into pairs and rotates each pair:\n",
        "  [x0, x1, x2, x3, ...] â†’ rotate [x0,x1] by Î¸â‚€, rotate [x2,x3] by Î¸â‚, ...\n",
        "```\n",
        "\n",
        "**Your Task:**\n",
        "1. Create rotation frequencies for each dimension pair\n",
        "2. Apply rotation to query and key vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.J: Implement Rotary Position Embeddings (RoPE)\n",
        "\n",
        "class RotaryPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Rotary Position Embeddings (RoPE) - LLaMA/Mistral style.\n",
        "    \n",
        "    Instead of adding position, we ROTATE Q and K based on position.\n",
        "    This encodes RELATIVE position information into attention scores.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, base: float = 10000.0):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.base = base\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        # Calculate rotation frequencies for each dimension pair\n",
        "        # For dimension pair i (0, 1, 2, ..., d_model/2 - 1):\n",
        "        # theta_i = 1 / (base^(2i/d_model))\n",
        "        \n",
        "        # Step 1: Create dimension pair indices [0, 1, 2, ..., d_model/2 - 1]\n",
        "        dim_pairs = torch.arange(0, d_model, 2).float()\n",
        "        \n",
        "        # Step 2: Calculate inverse frequencies\n",
        "        # inv_freq_i = 1 / (base^(2i/d_model))\n",
        "        inv_freq = # YOUR CODE: 1.0 / (base ** (dim_pairs / d_model))\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "        # Register as buffer (not a parameter, but saved with model)\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, positions: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply RoPE to input tensor.\n",
        "        \n",
        "        Args:\n",
        "            x: (batch, seq_len, d_model) or (batch, heads, seq_len, head_dim)\n",
        "            positions: Optional position indices. If None, use [0, 1, 2, ...]\n",
        "            \n",
        "        Returns:\n",
        "            Rotated tensor with same shape as input\n",
        "        \"\"\"\n",
        "        # Handle different input shapes\n",
        "        if x.dim() == 4:  # (batch, heads, seq_len, head_dim)\n",
        "            seq_len = x.shape[2]\n",
        "            dim = x.shape[3]\n",
        "        else:  # (batch, seq_len, d_model)\n",
        "            seq_len = x.shape[1]\n",
        "            dim = x.shape[2]\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        # Step 1: Create position indices if not provided\n",
        "        if positions is None:\n",
        "            positions = torch.arange(seq_len, device=x.device).float()\n",
        "        \n",
        "        # Step 2: Calculate rotation angles\n",
        "        # angles[pos, i] = pos * inv_freq[i]\n",
        "        # Shape: (seq_len, d_model/2)\n",
        "        angles = # YOUR CODE: positions.unsqueeze(-1) * self.inv_freq.unsqueeze(0)\n",
        "        \n",
        "        # Step 3: Create sin and cos values\n",
        "        sin_angles = torch.sin(angles)  # (seq_len, d_model/2)\n",
        "        cos_angles = torch.cos(angles)  # (seq_len, d_model/2)\n",
        "        \n",
        "        # Step 4: Expand dimensions for broadcasting\n",
        "        if x.dim() == 4:\n",
        "            sin_angles = sin_angles.unsqueeze(0).unsqueeze(0)  # (1, 1, seq_len, d/2)\n",
        "            cos_angles = cos_angles.unsqueeze(0).unsqueeze(0)\n",
        "        else:\n",
        "            sin_angles = sin_angles.unsqueeze(0)  # (1, seq_len, d/2)\n",
        "            cos_angles = cos_angles.unsqueeze(0)\n",
        "        \n",
        "        # Step 5: Split x into even and odd dimensions (pairs to rotate)\n",
        "        x_even = x[..., 0::2]  # dimensions 0, 2, 4, ...\n",
        "        x_odd = x[..., 1::2]   # dimensions 1, 3, 5, ...\n",
        "        \n",
        "        # Step 6: Apply rotation\n",
        "        # Rotation formula: (x_even, x_odd) â†’ (x_even*cos - x_odd*sin, x_even*sin + x_odd*cos)\n",
        "        x_rotated = torch.zeros_like(x)\n",
        "        x_rotated[..., 0::2] = # YOUR CODE: x_even * cos_angles - x_odd * sin_angles\n",
        "        x_rotated[..., 1::2] = # YOUR CODE: x_even * sin_angles + x_odd * cos_angles\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "        return x_rotated\n",
        "\n",
        "\n",
        "# Test your RoPE implementation\n",
        "d_model = 16\n",
        "batch_size = 2\n",
        "seq_len = 8\n",
        "\n",
        "rope = RotaryPositionalEmbedding(d_model)\n",
        "\n",
        "# Create dummy Q and K\n",
        "Q = torch.randn(batch_size, seq_len, d_model)\n",
        "K = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# Apply RoPE\n",
        "Q_rotated = rope(Q)\n",
        "K_rotated = rope(K)\n",
        "\n",
        "print(f\"Original Q shape: {Q.shape}\")\n",
        "print(f\"Rotated Q shape: {Q_rotated.shape}\")\n",
        "\n",
        "# Verify the rotation preserves vector magnitude (rotation is norm-preserving!)\n",
        "original_norm = Q.norm(dim=-1).mean()\n",
        "rotated_norm = Q_rotated.norm(dim=-1).mean()\n",
        "print(f\"\\nOriginal Q norm (mean): {original_norm:.4f}\")\n",
        "print(f\"Rotated Q norm (mean): {rotated_norm:.4f}\")\n",
        "print(f\"Norms are equal: {torch.allclose(Q.norm(dim=-1), Q_rotated.norm(dim=-1), atol=1e-5)}\")\n",
        "\n",
        "# Visualize the rotation effect\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Original Q\n",
        "ax = axes[0]\n",
        "ax.imshow(Q[0].detach().numpy(), aspect='auto', cmap='RdBu', vmin=-2, vmax=2)\n",
        "ax.set_title('Original Q', fontweight='bold')\n",
        "ax.set_xlabel('Dimension')\n",
        "ax.set_ylabel('Position')\n",
        "\n",
        "# Rotated Q\n",
        "ax = axes[1]\n",
        "ax.imshow(Q_rotated[0].detach().numpy(), aspect='auto', cmap='RdBu', vmin=-2, vmax=2)\n",
        "ax.set_title('RoPE-Rotated Q', fontweight='bold')\n",
        "ax.set_xlabel('Dimension')\n",
        "ax.set_ylabel('Position')\n",
        "\n",
        "# Difference (shows the position-dependent rotation)\n",
        "ax = axes[2]\n",
        "im = ax.imshow((Q_rotated - Q)[0].detach().numpy(), aspect='auto', cmap='RdBu', vmin=-2, vmax=2)\n",
        "ax.set_title('Difference (Position-Dependent)', fontweight='bold')\n",
        "ax.set_xlabel('Dimension')\n",
        "ax.set_ylabel('Position')\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… RoPE implemented!\")\n",
        "print(\"âœ… Different positions get different rotations\")\n",
        "print(\"âœ… Rotation angle increases with position\")\n",
        "print(\"âœ… Magnitude preserved (rotation is norm-preserving)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 2.K: Apply Position Encodings to Attention\n",
        "\n",
        "**Putting It All Together:** See how position encodings affect attention!\n",
        "\n",
        "We'll compare:\n",
        "1. **No position encoding** - attention is position-blind\n",
        "2. **Sinusoidal encoding** - adds absolute position\n",
        "3. **RoPE** - rotates for relative position\n",
        "\n",
        "**Your Task:**\n",
        "1. Run attention with and without position encodings\n",
        "2. Observe how position affects attention patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.K: Apply Position Encodings to Attention\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def attention_with_position(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,\n",
        "                            position_type: str = 'none') -> tuple:\n",
        "    \"\"\"\n",
        "    Compute attention with different position encoding methods.\n",
        "    \n",
        "    Args:\n",
        "        Q, K, V: (batch, seq_len, d_model) tensors\n",
        "        position_type: 'none', 'sinusoidal', or 'rope'\n",
        "    \n",
        "    Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "    batch_size, seq_len, d_model = Q.shape\n",
        "    \n",
        "    # ============ YOUR CODE HERE ============\n",
        "    \n",
        "    if position_type == 'none':\n",
        "        # No position encoding - just use Q, K, V directly\n",
        "        Q_pos = Q\n",
        "        K_pos = K\n",
        "        \n",
        "    elif position_type == 'sinusoidal':\n",
        "        # Add sinusoidal position encodings\n",
        "        pe = my_sinusoidal_encoding(seq_len, d_model).unsqueeze(0)  # (1, seq_len, d_model)\n",
        "        Q_pos = # YOUR CODE: Q + pe\n",
        "        K_pos = # YOUR CODE: K + pe\n",
        "        \n",
        "    elif position_type == 'rope':\n",
        "        # Apply RoPE rotation\n",
        "        rope = RotaryPositionalEmbedding(d_model)\n",
        "        Q_pos = # YOUR CODE: rope(Q)\n",
        "        K_pos = # YOUR CODE: rope(K)\n",
        "    \n",
        "    # Compute attention\n",
        "    scores = (Q_pos @ K_pos.transpose(-2, -1)) / math.sqrt(d_model)\n",
        "    attn_weights = F.softmax(scores, dim=-1)\n",
        "    output = attn_weights @ V\n",
        "    \n",
        "    # =========================================\n",
        "    \n",
        "    return output, attn_weights\n",
        "\n",
        "\n",
        "# Create identical embeddings for different \"words\" at different positions\n",
        "# This simulates a pathological case where position matters!\n",
        "d_model = 32\n",
        "seq_len = 6\n",
        "batch_size = 1\n",
        "\n",
        "# All tokens have the same embedding (position is the only difference!)\n",
        "base_embedding = torch.randn(1, d_model)\n",
        "Q = base_embedding.expand(batch_size, seq_len, d_model).clone()\n",
        "K = base_embedding.expand(batch_size, seq_len, d_model).clone()\n",
        "V = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "print(\"Testing with IDENTICAL embeddings at each position\")\n",
        "print(\"=\"*60)\n",
        "print(\"Without position info, attention should be uniform (can't distinguish!)\")\n",
        "print(\"With position info, attention patterns emerge\")\n",
        "\n",
        "# Compare different position encodings\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "position_types = ['none', 'sinusoidal', 'rope']\n",
        "titles = ['No Position Encoding', 'Sinusoidal Position', 'RoPE Position']\n",
        "\n",
        "for idx, (pos_type, title) in enumerate(zip(position_types, titles)):\n",
        "    _, attn = attention_with_position(Q.clone(), K.clone(), V.clone(), pos_type)\n",
        "    \n",
        "    ax = axes[idx]\n",
        "    im = ax.imshow(attn[0].detach().numpy(), cmap='Blues', vmin=0, vmax=0.5)\n",
        "    ax.set_title(title, fontweight='bold', fontsize=12)\n",
        "    ax.set_xlabel('Key Position', fontweight='bold')\n",
        "    ax.set_ylabel('Query Position', fontweight='bold')\n",
        "    \n",
        "    # Add value annotations\n",
        "    for i in range(seq_len):\n",
        "        for j in range(seq_len):\n",
        "            val = attn[0, i, j].item()\n",
        "            ax.text(j, i, f'{val:.2f}', ha='center', va='center', fontsize=8)\n",
        "    \n",
        "    plt.colorbar(im, ax=ax)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('How Position Encoding Affects Attention Patterns\\n(Identical embeddings at each position)', \n",
        "             fontsize=13, fontweight='bold', y=1.05)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ’¡ Key Observations:\")\n",
        "print(\"   â€¢ 'No Position': Uniform attention (1/6 â‰ˆ 0.17 for each)\")\n",
        "print(\"     â†’ Can't distinguish positions!\")\n",
        "print(\"   â€¢ 'Sinusoidal': Nearby positions have similar encodings\")\n",
        "print(\"     â†’ Creates position-aware patterns\")\n",
        "print(\"   â€¢ 'RoPE': Relative position through rotation\")\n",
        "print(\"     â†’ Different but also position-aware patterns\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Position encodings break the symmetry!\")\n",
        "print(\"âœ… Without position, attention treats input as a BAG of tokens\")\n",
        "print(\"âœ… With position, attention knows WHERE each token is\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ“š Reference: Complete Position Encoding Comparison\n",
        "\n",
        "Now that you've implemented the main position encoding methods, let's see a complete comparison and production-ready implementations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete Position Encoding Reference Implementations\n",
        "\n",
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Production-ready Sinusoidal Position Encoding (Original Transformer).\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, max_seq_len: int = 5000, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Create position encoding matrix\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        pe = pe.unsqueeze(0)  # (1, max_seq_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Add positional encoding to input embeddings.\"\"\"\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Production-ready Learned Position Encoding (BERT/GPT-2 style).\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, max_seq_len: int = 512, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
        "        self.max_seq_len = max_seq_len\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Add learned positional encoding to input embeddings.\"\"\"\n",
        "        seq_len = x.size(1)\n",
        "        positions = torch.arange(seq_len, device=x.device)\n",
        "        pos_emb = self.position_embedding(positions)\n",
        "        x = x + pos_emb\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class RoPE(nn.Module):\n",
        "    \"\"\"\n",
        "    Production-ready Rotary Position Embedding (LLaMA/Mistral style).\n",
        "    \n",
        "    Unlike additive position encodings, RoPE is applied to Q and K\n",
        "    AFTER the linear projections but BEFORE the attention computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, base: float = 10000.0, max_seq_len: int = 8192):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.base = base\n",
        "        self.max_seq_len = max_seq_len\n",
        "        \n",
        "        # Precompute inverse frequencies\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, d_model, 2).float() / d_model))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "        \n",
        "        # Precompute sin/cos for all positions up to max_seq_len\n",
        "        self._build_cache(max_seq_len)\n",
        "    \n",
        "    def _build_cache(self, seq_len: int):\n",
        "        \"\"\"Build sin/cos cache for given sequence length.\"\"\"\n",
        "        positions = torch.arange(seq_len, device=self.inv_freq.device).float()\n",
        "        angles = positions.unsqueeze(-1) * self.inv_freq.unsqueeze(0)  # (seq_len, d/2)\n",
        "        \n",
        "        # Cache sin and cos\n",
        "        self.register_buffer('cos_cache', torch.cos(angles), persistent=False)\n",
        "        self.register_buffer('sin_cache', torch.sin(angles), persistent=False)\n",
        "    \n",
        "    def forward(self, x: torch.Tensor, positions: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply rotary embeddings to input tensor.\n",
        "        \n",
        "        Args:\n",
        "            x: (batch, seq_len, d_model) or (batch, heads, seq_len, head_dim)\n",
        "            positions: Optional custom position indices\n",
        "        \"\"\"\n",
        "        if x.dim() == 4:\n",
        "            seq_len = x.shape[2]\n",
        "        else:\n",
        "            seq_len = x.shape[1]\n",
        "        \n",
        "        # Extend cache if needed\n",
        "        if seq_len > self.cos_cache.shape[0]:\n",
        "            self._build_cache(seq_len)\n",
        "        \n",
        "        cos = self.cos_cache[:seq_len]\n",
        "        sin = self.sin_cache[:seq_len]\n",
        "        \n",
        "        # Reshape for broadcasting\n",
        "        if x.dim() == 4:\n",
        "            cos = cos.unsqueeze(0).unsqueeze(0)  # (1, 1, seq, d/2)\n",
        "            sin = sin.unsqueeze(0).unsqueeze(0)\n",
        "        else:\n",
        "            cos = cos.unsqueeze(0)  # (1, seq, d/2)\n",
        "            sin = sin.unsqueeze(0)\n",
        "        \n",
        "        # Split and rotate\n",
        "        x1 = x[..., 0::2]\n",
        "        x2 = x[..., 1::2]\n",
        "        \n",
        "        x_rotated = torch.zeros_like(x)\n",
        "        x_rotated[..., 0::2] = x1 * cos - x2 * sin\n",
        "        x_rotated[..., 1::2] = x1 * sin + x2 * cos\n",
        "        \n",
        "        return x_rotated\n",
        "\n",
        "\n",
        "class ALiBi(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention with Linear Biases (BLOOM style).\n",
        "    \n",
        "    Instead of position embeddings, adds linear bias based on \n",
        "    distance to attention scores.\n",
        "    \n",
        "    No position embeddings needed - just bias the attention!\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        \n",
        "        # ALiBi slopes: different slope for each head\n",
        "        # Slopes are powers of 2^(-8/num_heads)\n",
        "        slopes = self._get_slopes(num_heads)\n",
        "        self.register_buffer('slopes', torch.tensor(slopes))\n",
        "    \n",
        "    def _get_slopes(self, num_heads: int):\n",
        "        \"\"\"Get the slopes for ALiBi (from the paper).\"\"\"\n",
        "        def get_slopes_power_of_2(n):\n",
        "            start = 2 ** (-(2 ** -(math.log2(n) - 3)))\n",
        "            ratio = start\n",
        "            return [start * ratio ** i for i in range(n)]\n",
        "        \n",
        "        if math.log2(num_heads).is_integer():\n",
        "            return get_slopes_power_of_2(num_heads)\n",
        "        else:\n",
        "            closest_power_of_2 = 2 ** math.floor(math.log2(num_heads))\n",
        "            return (\n",
        "                get_slopes_power_of_2(closest_power_of_2) +\n",
        "                self._get_slopes(2 * closest_power_of_2)[0::2][:num_heads - closest_power_of_2]\n",
        "            )\n",
        "    \n",
        "    def get_bias(self, seq_len: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Get the ALiBi bias matrix to add to attention scores.\n",
        "        \n",
        "        Returns:\n",
        "            (num_heads, seq_len, seq_len) bias tensor\n",
        "        \"\"\"\n",
        "        # Create distance matrix\n",
        "        positions = torch.arange(seq_len)\n",
        "        distance = positions.unsqueeze(0) - positions.unsqueeze(1)  # (seq, seq)\n",
        "        distance = distance.unsqueeze(0).float()  # (1, seq, seq)\n",
        "        \n",
        "        # Apply slopes\n",
        "        slopes = self.slopes.unsqueeze(-1).unsqueeze(-1)  # (heads, 1, 1)\n",
        "        bias = distance * slopes  # (heads, seq, seq)\n",
        "        \n",
        "        return bias\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Comprehensive Comparison\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"POSITION ENCODING COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "d_model = 64\n",
        "num_heads = 8\n",
        "max_seq_len = 128\n",
        "seq_len = 20\n",
        "\n",
        "# Create instances of each\n",
        "sinusoidal = SinusoidalPositionalEncoding(d_model, max_seq_len, dropout=0.0)\n",
        "learned = LearnedPositionalEncoding(d_model, max_seq_len, dropout=0.0)\n",
        "rope = RoPE(d_model)\n",
        "alibi = ALiBi(num_heads)\n",
        "\n",
        "# Test input\n",
        "x = torch.randn(1, seq_len, d_model)\n",
        "\n",
        "print(\"\\nğŸ“Š Position Encoding Properties:\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Method':<15} {'Parameters':<15} {'Extrapolation':<15} {'Relative Pos':<15}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Sinusoidal':<15} {'0':<15} {'âœ… Good':<15} {'âŒ Implicit':<15}\")\n",
        "print(f\"{'Learned':<15} {f'{max_seq_len * d_model:,}':<15} {'âŒ Fixed':<15} {'âŒ No':<15}\")\n",
        "print(f\"{'RoPE':<15} {'0':<15} {'âœ… Excellent':<15} {'âœ… Yes':<15}\")\n",
        "print(f\"{'ALiBi':<15} {f'{num_heads}':<15} {'âœ… Excellent':<15} {'âœ… Yes':<15}\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Sinusoidal\n",
        "ax = axes[0, 0]\n",
        "pe_sin = sinusoidal.pe[0, :seq_len].numpy()\n",
        "im = ax.imshow(pe_sin, aspect='auto', cmap='RdBu', vmin=-1, vmax=1)\n",
        "ax.set_title('Sinusoidal\\n(Fixed sin/cos patterns)', fontweight='bold')\n",
        "ax.set_xlabel('Dimension')\n",
        "ax.set_ylabel('Position')\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "# Learned\n",
        "ax = axes[0, 1]\n",
        "pe_learned = learned.position_embedding.weight[:seq_len].detach().numpy()\n",
        "im = ax.imshow(pe_learned, aspect='auto', cmap='RdBu')\n",
        "ax.set_title('Learned\\n(Random init, trained)', fontweight='bold')\n",
        "ax.set_xlabel('Dimension')\n",
        "ax.set_ylabel('Position')\n",
        "plt.colorbar(im, ax=ax)\n",
        "\n",
        "# RoPE (show the rotation angles)\n",
        "ax = axes[1, 0]\n",
        "rope_angles = torch.arange(seq_len).unsqueeze(-1) * rope.inv_freq.unsqueeze(0)\n",
        "im = ax.imshow(rope_angles.numpy(), aspect='auto', cmap='viridis')\n",
        "ax.set_title('RoPE\\n(Rotation angles by position/dimension)', fontweight='bold')\n",
        "ax.set_xlabel('Dimension Pair')\n",
        "ax.set_ylabel('Position')\n",
        "plt.colorbar(im, ax=ax, label='Rotation Angle (radians)')\n",
        "\n",
        "# ALiBi\n",
        "ax = axes[1, 1]\n",
        "alibi_bias = alibi.get_bias(seq_len)\n",
        "im = ax.imshow(alibi_bias[0].numpy(), aspect='auto', cmap='RdBu_r', \n",
        "               vmin=-seq_len/2, vmax=0)\n",
        "ax.set_title('ALiBi (Head 0)\\n(Distance-based attention bias)', fontweight='bold')\n",
        "ax.set_xlabel('Key Position')\n",
        "ax.set_ylabel('Query Position')\n",
        "plt.colorbar(im, ax=ax, label='Attention Bias')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.suptitle('Position Encoding Methods Visualization', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nğŸ’¡ Usage in Modern Models:\")\n",
        "print(\"   â€¢ Sinusoidal: Original Transformer (2017)\")\n",
        "print(\"   â€¢ Learned: BERT, GPT-2, T5\")\n",
        "print(\"   â€¢ RoPE: LLaMA, LLaMA-2, Mistral, Qwen, DeepSeek\")\n",
        "print(\"   â€¢ ALiBi: BLOOM, MPT\")\n",
        "print(\"\\nğŸ† RoPE is currently the most popular choice for new LLMs!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 2.10: Padding Masks - Handling Variable-Length Sequences\n",
        "\n",
        "### ğŸ¤” The Problem: Batches Need Equal Length\n",
        "\n",
        "In practice, sentences have different lengths:\n",
        "\n",
        "```\n",
        "Batch of sentences:\n",
        "  Sentence 1: \"The cat sat\"          (3 tokens)\n",
        "  Sentence 2: \"Hello\"                (1 token)\n",
        "  Sentence 3: \"I love machine learning\" (4 tokens)\n",
        "\n",
        "But tensors need fixed shapes! Solution: PAD shorter sequences.\n",
        "\n",
        "After padding to max_len=4:\n",
        "  Sentence 1: [\"The\", \"cat\", \"sat\", PAD]\n",
        "  Sentence 2: [\"Hello\", PAD, PAD, PAD]\n",
        "  Sentence 3: [\"I\", \"love\", \"machine\", \"learning\"]\n",
        "```\n",
        "\n",
        "### âš ï¸ But We DON'T Want to Attend to Padding!\n",
        "\n",
        "```\n",
        "Without padding mask:\n",
        "  \"Hello\" attends to: [\"Hello\", PAD, PAD, PAD]\n",
        "                                â†‘    â†‘    â†‘\n",
        "                          Garbage attention!\n",
        "\n",
        "With padding mask:\n",
        "  \"Hello\" attends to: [\"Hello\", 0, 0, 0]\n",
        "                                â†‘  â†‘  â†‘\n",
        "                          Properly masked out!\n",
        "```\n",
        "\n",
        "### ğŸ“Š Padding Mask Visualization\n",
        "\n",
        "```\n",
        "Sentence: [\"The\", \"cat\", PAD, PAD]\n",
        "\n",
        "Padding mask:\n",
        "             The  cat  PAD  PAD\n",
        "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "    The    â”‚  1    1    0    0  â”‚  â† \"The\" attends to real tokens only\n",
        "    cat    â”‚  1    1    0    0  â”‚  â† \"cat\" attends to real tokens only\n",
        "    PAD    â”‚  0    0    0    0  â”‚  â† PAD doesn't attend to anything\n",
        "    PAD    â”‚  0    0    0    0  â”‚  â† PAD doesn't attend to anything\n",
        "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           \n",
        "1 = can attend, 0 = masked (set to -âˆ before softmax)\n",
        "```\n",
        "\n",
        "### ğŸ”¢ Creating Padding Masks\n",
        "\n",
        "```python\n",
        "# Method 1: From attention_mask (common in HuggingFace)\n",
        "# attention_mask = [1, 1, 1, 0, 0]  # 1 = real, 0 = padding\n",
        "# Expand to 2D: mask = attention_mask.unsqueeze(1) * attention_mask.unsqueeze(2)\n",
        "\n",
        "# Method 2: From token IDs (if PAD = 0)\n",
        "# padding_mask = (token_ids != 0)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Implementing Padding Masks\n",
        "\n",
        "def create_padding_mask(attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Create 2D padding mask from 1D attention mask.\n",
        "    \n",
        "    Args:\n",
        "        attention_mask: (seq_len,) tensor with 1 for real tokens, 0 for padding\n",
        "    \n",
        "    Returns:\n",
        "        mask: (seq_len, seq_len) 2D attention mask\n",
        "    \"\"\"\n",
        "    # Expand to 2D: (seq_len,) -> (seq_len, seq_len)\n",
        "    mask = attention_mask.unsqueeze(0) * attention_mask.unsqueeze(1)\n",
        "    return mask\n",
        "\n",
        "def create_combined_mask(seq_len: int, attention_mask: torch.Tensor, causal: bool = True) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Combine causal mask and padding mask.\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Sequence length\n",
        "        attention_mask: (seq_len,) with 1 for real, 0 for padding\n",
        "        causal: Whether to apply causal masking\n",
        "    \n",
        "    Returns:\n",
        "        Combined mask for attention\n",
        "    \"\"\"\n",
        "    # Start with padding mask\n",
        "    mask = create_padding_mask(attention_mask)\n",
        "    \n",
        "    # Add causal mask if needed\n",
        "    if causal:\n",
        "        causal_mask = create_causal_mask(seq_len)\n",
        "        mask = mask * causal_mask  # Element-wise AND\n",
        "    \n",
        "    return mask\n",
        "\n",
        "# Demonstrate padding mask\n",
        "print('='*70)\n",
        "print('PADDING MASKS')\n",
        "print('='*70)\n",
        "\n",
        "# Example: \"The cat [PAD] [PAD]\"\n",
        "attention_mask = torch.tensor([1, 1, 0, 0])  # Real tokens are 1, padding is 0\n",
        "tokens = ['The', 'cat', 'PAD', 'PAD']\n",
        "\n",
        "padding_mask = create_padding_mask(attention_mask)\n",
        "print('\\nAttention mask (1D):', attention_mask.tolist())\n",
        "print('\\nPadding mask (2D):')\n",
        "print(padding_mask)\n",
        "\n",
        "# Visualize different mask types\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# Padding mask only\n",
        "ax = axes[0]\n",
        "im = ax.imshow(padding_mask.numpy(), cmap='Greens', vmin=0, vmax=1)\n",
        "ax.set_title('Padding Mask Only\\n(for BERT-style models)', fontweight='bold')\n",
        "ax.set_xticks(range(4))\n",
        "ax.set_yticks(range(4))\n",
        "ax.set_xticklabels(tokens)\n",
        "ax.set_yticklabels(tokens)\n",
        "ax.set_xlabel('Key', fontweight='bold')\n",
        "ax.set_ylabel('Query', fontweight='bold')\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        ax.text(j, i, int(padding_mask[i, j]), ha='center', va='center', \n",
        "                fontsize=14, fontweight='bold', color='white' if padding_mask[i,j] > 0.5 else 'black')\n",
        "\n",
        "# Causal mask only\n",
        "ax = axes[1]\n",
        "causal_only = create_causal_mask(4)\n",
        "im = ax.imshow(causal_only.numpy(), cmap='Blues', vmin=0, vmax=1)\n",
        "ax.set_title('Causal Mask Only\\n(for GPT-style, no padding)', fontweight='bold')\n",
        "ax.set_xticks(range(4))\n",
        "ax.set_yticks(range(4))\n",
        "ax.set_xticklabels(['T0', 'T1', 'T2', 'T3'])\n",
        "ax.set_yticklabels(['T0', 'T1', 'T2', 'T3'])\n",
        "ax.set_xlabel('Key', fontweight='bold')\n",
        "ax.set_ylabel('Query', fontweight='bold')\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        ax.text(j, i, int(causal_only[i, j]), ha='center', va='center', \n",
        "                fontsize=14, fontweight='bold', color='white' if causal_only[i,j] > 0.5 else 'black')\n",
        "\n",
        "# Combined mask\n",
        "ax = axes[2]\n",
        "combined = create_combined_mask(4, attention_mask, causal=True)\n",
        "im = ax.imshow(combined.numpy(), cmap='Purples', vmin=0, vmax=1)\n",
        "ax.set_title('Combined Mask\\n(Causal + Padding)', fontweight='bold')\n",
        "ax.set_xticks(range(4))\n",
        "ax.set_yticks(range(4))\n",
        "ax.set_xticklabels(tokens)\n",
        "ax.set_yticklabels(tokens)\n",
        "ax.set_xlabel('Key', fontweight='bold')\n",
        "ax.set_ylabel('Query', fontweight='bold')\n",
        "for i in range(4):\n",
        "    for j in range(4):\n",
        "        ax.text(j, i, int(combined[i, j]), ha='center', va='center', \n",
        "                fontsize=14, fontweight='bold', color='white' if combined[i,j] > 0.5 else 'black')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nğŸ’¡ Mask Usage Summary:')\n",
        "print('   â€¢ BERT (encoder): Padding mask only')\n",
        "print('   â€¢ GPT (decoder): Causal mask (+ padding if batched)')\n",
        "print('   â€¢ T5 decoder: Combined causal + padding mask')\n",
        "print('   â€¢ Always apply mask BEFORE softmax!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 2.5: Implement Causal Attention (ğŸŸ¡ Medium)\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Practice implementing causal masking from scratch\n",
        "\n",
        "### ğŸ“‹ Tasks\n",
        "\n",
        "**Task 1**: Create a causal mask for sequence length 6\n",
        "\n",
        "**Task 2**: Apply the mask to attention scores and verify:\n",
        "- Upper triangle should be -inf\n",
        "- Lower triangle should be unchanged\n",
        "- Softmax should give 0 for masked positions\n",
        "\n",
        "**Task 3**: Verify that each row sums to 1.0\n",
        "\n",
        "### ğŸ’¡ Hints\n",
        "- Use `torch.tril()` for lower triangular\n",
        "- Use `masked_fill(mask == 0, float('-inf'))`\n",
        "- Remember: 1 = can attend, 0 = blocked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸŸ¡ EXERCISE 2.5: Implement Causal Attention\n",
        "\n",
        "# Task 1: Create causal mask for seq_len=6\n",
        "seq_len_ex = 6\n",
        "causal_mask_ex = None  # YOUR CODE: Use torch.tril()\n",
        "\n",
        "# Task 2: Apply to random scores\n",
        "scores_ex = torch.randn(seq_len_ex, seq_len_ex)\n",
        "masked_scores_ex = None  # YOUR CODE: Apply mask\n",
        "\n",
        "# Task 3: Apply softmax and verify\n",
        "attn_weights_ex = None  # YOUR CODE: softmax(masked_scores, dim=-1)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# ğŸ“ SOLUTION (uncomment to check)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# # Task 1\n",
        "# causal_mask_ex = torch.tril(torch.ones(seq_len_ex, seq_len_ex))\n",
        "# print('Task 1 - Causal Mask:')\n",
        "# print(causal_mask_ex)\n",
        "# \n",
        "# # Task 2\n",
        "# masked_scores_ex = scores_ex.masked_fill(causal_mask_ex == 0, float('-inf'))\n",
        "# print('\\nTask 2 - Masked Scores (upper triangle = -inf):')\n",
        "# print(masked_scores_ex)\n",
        "# \n",
        "# # Task 3\n",
        "# attn_weights_ex = torch.softmax(masked_scores_ex, dim=-1)\n",
        "# print('\\nTask 3 - Attention Weights (each row sums to 1):')\n",
        "# print(attn_weights_ex)\n",
        "# print(f'\\nRow sums: {attn_weights_ex.sum(dim=-1)}')\n",
        "# \n",
        "# # Verification\n",
        "# if torch.allclose(attn_weights_ex.sum(dim=-1), torch.ones(seq_len_ex)):\n",
        "#     print('\\nâœ… SUCCESS! Each row sums to 1.0')\n",
        "# else:\n",
        "#     print('\\nâŒ Check your softmax dimension')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-3\"></a>\n",
        "# Part 3: Multi-Head Self-Attention (MHSA) â±ï¸ ~30 min\n",
        "\n",
        "## ğŸ¯ Why Multiple Heads?\n",
        "\n",
        "**Problem**: Single attention learns ONE pattern.\n",
        "\n",
        "**Solution**: Multiple heads learn DIFFERENT patterns simultaneously!\n",
        "\n",
        "### ğŸ“Š The Concept\n",
        "\n",
        "```\n",
        "Sentence: \"The cat sat on the mat\"\n",
        "\n",
        "Single Attention:\n",
        "  Learns one pattern (e.g., subject-verb)\n",
        "\n",
        "Multi-Head Attention:\n",
        "  Head 1: subject-verb (cat â†’ sat)\n",
        "  Head 2: prepositions (sat â†’ on â†’ mat)\n",
        "  Head 3: articles (the â†’ cat, the â†’ mat)\n",
        "  Head 4: spatial relations (on â†’ mat)\n",
        "```\n",
        "\n",
        "### ğŸ”¢ Mathematics\n",
        "\n",
        "For $h$ heads with $d_{\\text{model}}$ total dimensions:\n",
        "\n",
        "$$\n",
        "d_k = d_v = \\frac{d_{\\text{model}}}{h}\n",
        "$$\n",
        "\n",
        "Each head:\n",
        "$$\n",
        "\\text{head}_i = \\text{Attention}(\\mathbf{Q}^i, \\mathbf{K}^i, \\mathbf{V}^i)\n",
        "$$\n",
        "\n",
        "Final output:\n",
        "$$\n",
        "\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\mathbf{W}_O\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ§  Check Your Understanding: Before Multi-Head Attention\n",
        "\n",
        "**Before proceeding, make sure you understand Self-Attention:**\n",
        "\n",
        "### Quick Quiz\n",
        "\n",
        "1. **What are Q, K, and V in attention?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   Query (what we're looking for), Key (what we match against), Value (what we retrieve). All are linear projections of the input.\n",
        "   </details>\n",
        "\n",
        "2. **What is the attention weight formula?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   Attention = softmax(QK^T / âˆšd_k) Ã— V\n",
        "   </details>\n",
        "\n",
        "3. **What does a causal mask do?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   Prevents tokens from attending to future tokens by masking the upper triangle of the attention matrix with -âˆ.\n",
        "   </details>\n",
        "\n",
        "4. **Why do we need positional encodings?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   Attention is permutation-invariant - it doesn't know word order. Position encodings add order information.\n",
        "   </details>\n",
        "\n",
        "### âœ… Ready?\n",
        "If you can explain Q, K, V and the attention formula, proceed to Multi-Head Attention!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ› ï¸ BUILD IT YOURSELF: Multi-Head Self-Attention Step-by-Step\n",
        "\n",
        "Now implement MHSA yourself! The key insight is:\n",
        "1. Split the embedding dimension into multiple heads\n",
        "2. Run parallel attention on each head\n",
        "3. Concatenate results back together\n",
        "\n",
        "### ğŸ“‹ What You'll Implement\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    YOUR MHSA JOURNEY                                    â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Exercise 3.A: Understand head dimension math                         â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 3.B: Create Q, K, V projections                             â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 3.C: Reshape tensors into multiple heads                    â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 3.D: Compute attention for all heads in parallel            â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 3.E: Concatenate heads and project output                   â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 3.F: Complete MHSA as a PyTorch module                      â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3.A: Understand Multi-Head Dimension Math\n",
        "\n",
        "**Key Concept:** In MHSA, we split the embedding dimension `d_model` across `num_heads` heads.\n",
        "\n",
        "Given:\n",
        "- `d_model = 64` (total embedding dimension)\n",
        "- `num_heads = 4` (number of attention heads)\n",
        "\n",
        "Calculate:\n",
        "- `head_dim = d_model // num_heads` (dimension per head)\n",
        "\n",
        "**Your Task:**\n",
        "1. Verify that `d_model` is divisible by `num_heads`\n",
        "2. Calculate `head_dim`\n",
        "3. Understand: Each head sees a SMALLER dimensional space, but we have MULTIPLE perspectives"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3.A: Understand Multi-Head Dimension Math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Configuration\n",
        "d_model = 64    # Total embedding dimension\n",
        "num_heads = 4   # Number of attention heads\n",
        "seq_len = 5     # Sequence length\n",
        "batch_size = 2  # Batch size\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "# TODO: Calculate head_dim (dimension per head)\n",
        "# Formula: head_dim = d_model // num_heads\n",
        "\n",
        "head_dim = # YOUR CODE\n",
        "\n",
        "# TODO: Verify d_model is divisible by num_heads\n",
        "# Hint: Use assert with modulo operator (%)\n",
        "\n",
        "# YOUR ASSERTION HERE\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Verification\n",
        "print(f\"d_model = {d_model}\")\n",
        "print(f\"num_heads = {num_heads}\")\n",
        "print(f\"head_dim = {head_dim}\")\n",
        "print(f\"\\nTotal parameters per head: {head_dim}\")\n",
        "print(f\"All heads together: {num_heads} Ã— {head_dim} = {num_heads * head_dim}\")\n",
        "\n",
        "# Visual representation\n",
        "print(f\"\\nâœ… Each of the {num_heads} heads will process {head_dim}-dimensional slices\")\n",
        "print(f\"âœ… This allows learning {num_heads} different attention patterns!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3.B: Create Linear Projections for Q, K, V\n",
        "\n",
        "**The Key Insight:** In MHSA, we use `nn.Linear` layers to project the input into Q, K, and V.\n",
        "\n",
        "Unlike basic attention where we had separate weight matrices, here:\n",
        "- `W_q`: Projects input to all heads' queries at once\n",
        "- `W_k`: Projects input to all heads' keys at once  \n",
        "- `W_v`: Projects input to all heads' values at once\n",
        "- `W_o`: Projects concatenated head outputs back to `d_model`\n",
        "\n",
        "**Shape Flow:**\n",
        "```\n",
        "Input: (batch, seq_len, d_model)\n",
        "   â†“  W_q, W_k, W_v projections\n",
        "Q, K, V: (batch, seq_len, d_model)  â† Still d_model! We'll split into heads next\n",
        "```\n",
        "\n",
        "**Your Task:**\n",
        "1. Create 4 `nn.Linear` layers: `W_q`, `W_k`, `W_v`, `W_o`\n",
        "2. All project from `d_model â†’ d_model`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3.B: Create Linear Projections for Q, K, V\n",
        "\n",
        "# Configuration (same as 3.A)\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "head_dim = d_model // num_heads\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "# TODO: Create 4 linear layers using nn.Linear(in_features, out_features)\n",
        "# All should map from d_model to d_model\n",
        "\n",
        "W_q = # YOUR CODE: nn.Linear(?, ?)\n",
        "W_k = # YOUR CODE: nn.Linear(?, ?)\n",
        "W_v = # YOUR CODE: nn.Linear(?, ?)\n",
        "W_o = # YOUR CODE: nn.Linear(?, ?)  # Output projection\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Verification\n",
        "print(\"Linear Projection Layers:\")\n",
        "print(f\"  W_q: {W_q.in_features} â†’ {W_q.out_features}\")\n",
        "print(f\"  W_k: {W_k.in_features} â†’ {W_k.out_features}\")\n",
        "print(f\"  W_v: {W_v.in_features} â†’ {W_v.out_features}\")\n",
        "print(f\"  W_o: {W_o.in_features} â†’ {W_o.out_features}\")\n",
        "\n",
        "# Test with dummy input\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "Q = W_q(x)\n",
        "K = W_k(x)\n",
        "V = W_v(x)\n",
        "\n",
        "print(f\"\\nâœ… Input shape: {x.shape}\")\n",
        "print(f\"âœ… Q shape after projection: {Q.shape}\")\n",
        "print(f\"âœ… K shape after projection: {K.shape}\")\n",
        "print(f\"âœ… V shape after projection: {V.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3.C: Reshape Tensors into Multiple Heads (THE CRITICAL STEP!)\n",
        "\n",
        "**This is the magic of Multi-Head Attention!**\n",
        "\n",
        "After projecting to Q, K, V (each with shape `(batch, seq_len, d_model)`), we need to:\n",
        "1. **Reshape** to split `d_model` into `num_heads Ã— head_dim`\n",
        "2. **Transpose** so heads become a batch dimension\n",
        "\n",
        "**Shape Transformation:**\n",
        "```\n",
        "Before: (batch, seq_len, d_model)\n",
        "              â†“ view/reshape\n",
        "Middle: (batch, seq_len, num_heads, head_dim)\n",
        "              â†“ transpose\n",
        "After:  (batch, num_heads, seq_len, head_dim)\n",
        "```\n",
        "\n",
        "**Why this order?** With shape `(batch, num_heads, seq_len, head_dim)`:\n",
        "- The last two dimensions are `(seq_len, head_dim)`\n",
        "- This lets us do attention on each head independently!\n",
        "- PyTorch can batch across `batch` AND `num_heads`\n",
        "\n",
        "**Your Task:**\n",
        "1. Use `.view()` to reshape from `(batch, seq_len, d_model)` â†’ `(batch, seq_len, num_heads, head_dim)`\n",
        "2. Use `.transpose(1, 2)` to swap `seq_len` and `num_heads` dimensions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3.C: Reshape Tensors into Multiple Heads\n",
        "\n",
        "# Setup - using projections from previous exercise\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "head_dim = d_model // num_heads\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "\n",
        "# Create sample projected tensors (simulating output from W_q, W_k, W_v)\n",
        "Q = torch.randn(batch_size, seq_len, d_model)\n",
        "K = torch.randn(batch_size, seq_len, d_model)\n",
        "V = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "print(f\"Original Q shape: {Q.shape}\")\n",
        "print(f\"  (batch={batch_size}, seq_len={seq_len}, d_model={d_model})\")\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "# Step 1: Reshape Q to split d_model into (num_heads, head_dim)\n",
        "# Hint: Q.view(batch_size, seq_len, num_heads, head_dim)\n",
        "\n",
        "Q_reshaped = # YOUR CODE: Q.view(?, ?, ?, ?)\n",
        "\n",
        "print(f\"\\nAfter view: {Q_reshaped.shape}\")\n",
        "print(f\"  (batch, seq_len, num_heads, head_dim)\")\n",
        "\n",
        "# Step 2: Transpose to move num_heads before seq_len\n",
        "# Hint: tensor.transpose(dim1, dim2) swaps dimensions\n",
        "\n",
        "Q_heads = # YOUR CODE: Q_reshaped.transpose(?, ?)\n",
        "\n",
        "print(f\"\\nAfter transpose: {Q_heads.shape}\")\n",
        "print(f\"  (batch, num_heads, seq_len, head_dim)\")\n",
        "\n",
        "# Now do the same for K and V\n",
        "K_heads = # YOUR CODE (combine view and transpose)\n",
        "V_heads = # YOUR CODE (combine view and transpose)\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Verification\n",
        "assert Q_heads.shape == (batch_size, num_heads, seq_len, head_dim), f\"Q_heads shape wrong: {Q_heads.shape}\"\n",
        "assert K_heads.shape == (batch_size, num_heads, seq_len, head_dim), f\"K_heads shape wrong: {K_heads.shape}\"\n",
        "assert V_heads.shape == (batch_size, num_heads, seq_len, head_dim), f\"V_heads shape wrong: {V_heads.shape}\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… All tensors correctly reshaped!\")\n",
        "print(f\"âœ… Now we have {num_heads} parallel attention heads\")\n",
        "print(f\"âœ… Each head processes {head_dim}-dimensional vectors\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3.D: Compute Attention for All Heads in Parallel\n",
        "\n",
        "**Now for the actual attention computation!**\n",
        "\n",
        "With shape `(batch, num_heads, seq_len, head_dim)`, we compute attention:\n",
        "1. **Scores**: `Q @ K^T` â†’ `(batch, num_heads, seq_len, seq_len)`\n",
        "2. **Scale**: Divide by `âˆšhead_dim` (not `d_model`!)\n",
        "3. **Softmax**: Along the last dimension\n",
        "4. **Output**: `attention_weights @ V` â†’ `(batch, num_heads, seq_len, head_dim)`\n",
        "\n",
        "**Key Insight:** The matrix multiplication happens on the LAST TWO dimensions!\n",
        "- `(batch, num_heads, seq_len, head_dim) @ (batch, num_heads, head_dim, seq_len)`\n",
        "- PyTorch broadcasts across `batch` and `num_heads` â†’ parallel attention!\n",
        "\n",
        "**Your Task:**\n",
        "1. Transpose K for the matmul (last two dims)\n",
        "2. Compute scores and scale\n",
        "3. Apply softmax\n",
        "4. Compute weighted sum with V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3.D: Compute Attention for All Heads in Parallel\n",
        "\n",
        "import math\n",
        "\n",
        "# Setup - using shapes from previous exercise\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "head_dim = d_model // num_heads\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "\n",
        "# Simulated head-shaped tensors (output from Exercise 3.C)\n",
        "Q_heads = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
        "K_heads = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
        "V_heads = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "print(f\"Q_heads shape: {Q_heads.shape}\")\n",
        "print(f\"K_heads shape: {K_heads.shape}\")\n",
        "print(f\"V_heads shape: {V_heads.shape}\")\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "\n",
        "# Step 1: Transpose K to prepare for matmul\n",
        "# We need shape (batch, num_heads, head_dim, seq_len) for K\n",
        "# Hint: Use .transpose(-2, -1) to swap last two dimensions\n",
        "\n",
        "K_transposed = # YOUR CODE\n",
        "\n",
        "print(f\"\\nK_transposed shape: {K_transposed.shape}\")\n",
        "\n",
        "# Step 2: Compute raw attention scores\n",
        "# Q @ K^T  (matmul broadcasts over batch and num_heads)\n",
        "\n",
        "scores = # YOUR CODE: Q_heads @ K_transposed\n",
        "\n",
        "print(f\"Scores shape: {scores.shape} (should be batch, heads, seq, seq)\")\n",
        "\n",
        "# Step 3: Scale by sqrt(head_dim) - NOT d_model!\n",
        "# Important: We scale by head dimension, not full model dimension\n",
        "\n",
        "scale = # YOUR CODE: math.sqrt(?)\n",
        "scaled_scores = # YOUR CODE: scores / scale\n",
        "\n",
        "print(f\"Scaling factor: {scale}\")\n",
        "\n",
        "# Step 4: Apply softmax along the last dimension (keys dimension)\n",
        "\n",
        "attention_weights = # YOUR CODE: F.softmax(scaled_scores, dim=?)\n",
        "\n",
        "print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "print(f\"Weights sum per query (should be 1.0): {attention_weights[0, 0, 0].sum().item():.4f}\")\n",
        "\n",
        "# Step 5: Compute output by weighted sum with V\n",
        "\n",
        "attn_output = # YOUR CODE: attention_weights @ V_heads\n",
        "\n",
        "print(f\"Attention output shape: {attn_output.shape}\")\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Verification\n",
        "assert scores.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "assert attention_weights.shape == (batch_size, num_heads, seq_len, seq_len)\n",
        "assert attn_output.shape == (batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Attention computed for all heads in parallel!\")\n",
        "print(f\"âœ… Each of {num_heads} heads computed {seq_len}Ã—{seq_len} attention\")\n",
        "print(f\"âœ… Output has {num_heads} head outputs to combine\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3.E: Concatenate Heads and Apply Output Projection\n",
        "\n",
        "**Reversing the reshape to combine heads!**\n",
        "\n",
        "After attention, we have: `(batch, num_heads, seq_len, head_dim)`\n",
        "\n",
        "We need to get back to: `(batch, seq_len, d_model)`\n",
        "\n",
        "**The Reverse Transformation:**\n",
        "```\n",
        "Before: (batch, num_heads, seq_len, head_dim)\n",
        "              â†“ transpose(1, 2)\n",
        "Middle: (batch, seq_len, num_heads, head_dim)\n",
        "              â†“ reshape/view\n",
        "After:  (batch, seq_len, d_model)  where d_model = num_heads Ã— head_dim\n",
        "```\n",
        "\n",
        "**Important:** After reshaping, apply output projection `W_o` to mix information across heads!\n",
        "\n",
        "**Your Task:**\n",
        "1. Transpose back: swap dims 1 and 2\n",
        "2. Reshape to combine `num_heads Ã— head_dim` back to `d_model`\n",
        "3. Apply output projection `W_o`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3.E: Concatenate Heads and Apply Output Projection\n",
        "\n",
        "# Setup\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "head_dim = d_model // num_heads\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "\n",
        "# Simulated attention output from previous exercise\n",
        "attn_output = torch.randn(batch_size, num_heads, seq_len, head_dim)\n",
        "\n",
        "# Output projection layer\n",
        "W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "print(f\"Attention output shape: {attn_output.shape}\")\n",
        "print(f\"  (batch, num_heads, seq_len, head_dim)\")\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "\n",
        "# Step 1: Transpose to move seq_len before num_heads\n",
        "# From (batch, num_heads, seq_len, head_dim) â†’ (batch, seq_len, num_heads, head_dim)\n",
        "\n",
        "transposed = # YOUR CODE: attn_output.transpose(?, ?)\n",
        "\n",
        "print(f\"\\nAfter transpose: {transposed.shape}\")\n",
        "\n",
        "# Step 2: Reshape to concatenate heads\n",
        "# From (batch, seq_len, num_heads, head_dim) â†’ (batch, seq_len, d_model)\n",
        "# Hint: Use .reshape(batch_size, seq_len, d_model) or .view()\n",
        "# Note: You might need .contiguous() before view() if tensor is not contiguous\n",
        "\n",
        "concatenated = # YOUR CODE\n",
        "\n",
        "print(f\"After concatenation: {concatenated.shape}\")\n",
        "\n",
        "# Step 3: Apply output projection to mix head information\n",
        "\n",
        "output = # YOUR CODE: W_o(concatenated)\n",
        "\n",
        "print(f\"Final output shape: {output.shape}\")\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Verification\n",
        "assert transposed.shape == (batch_size, seq_len, num_heads, head_dim)\n",
        "assert concatenated.shape == (batch_size, seq_len, d_model)\n",
        "assert output.shape == (batch_size, seq_len, d_model)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Heads successfully concatenated!\")\n",
        "print(f\"âœ… {num_heads} heads Ã— {head_dim} dim = {d_model} d_model\")\n",
        "print(\"âœ… Output projection applied to mix head information\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 3.F: Complete Multi-Head Self-Attention Module\n",
        "\n",
        "**Now combine everything into a complete PyTorch module!**\n",
        "\n",
        "You've implemented all the pieces:\n",
        "- âœ… Linear projections (W_q, W_k, W_v, W_o)\n",
        "- âœ… Reshape into heads\n",
        "- âœ… Parallel attention computation\n",
        "- âœ… Concatenate and project output\n",
        "\n",
        "**Your Task:** Create a complete `nn.Module` class that:\n",
        "1. Initializes all projection layers in `__init__`\n",
        "2. Implements the full forward pass\n",
        "3. Optionally supports a causal mask for autoregressive models\n",
        "\n",
        "**Architecture Diagram:**\n",
        "```\n",
        "Input X â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º\n",
        "    â”‚                                                                         â”‚\n",
        "    â”œâ”€â”€â–º W_q â”€â”€â–º Q â”€â”€â”                                                       â”‚\n",
        "    â”‚                â”‚                                                        â”‚\n",
        "    â”œâ”€â”€â–º W_k â”€â”€â–º K â”€â”€â”¼â”€â”€â–º Split Heads â”€â”€â–º Attention â”€â”€â–º Concat â”€â”€â–º W_o â”€â”€â–º Output\n",
        "    â”‚                â”‚         â†“               â†“\n",
        "    â””â”€â”€â–º W_v â”€â”€â–º V â”€â”€â”˜    (reshape)    (Q @ K^T / âˆšd_k â†’ softmax â†’ @ V)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3.F: Complete Multi-Head Self-Attention Module\n",
        "\n",
        "class MyMultiHeadSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Your implementation of Multi-Head Self-Attention!\n",
        "    \n",
        "    Combines all the steps you've practiced:\n",
        "    1. Project input to Q, K, V\n",
        "    2. Split into multiple heads\n",
        "    3. Compute attention in parallel\n",
        "    4. Concatenate and project output\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        # TODO: Store d_model, num_heads\n",
        "        # TODO: Calculate and store head_dim\n",
        "        # TODO: Assert that d_model is divisible by num_heads\n",
        "        \n",
        "        self.d_model = # YOUR CODE\n",
        "        self.num_heads = # YOUR CODE\n",
        "        self.head_dim = # YOUR CODE\n",
        "        \n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        \n",
        "        # TODO: Create the 4 linear projection layers\n",
        "        \n",
        "        self.W_q = # YOUR CODE: nn.Linear(d_model, d_model)\n",
        "        self.W_k = # YOUR CODE\n",
        "        self.W_v = # YOUR CODE\n",
        "        self.W_o = # YOUR CODE\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len, d_model)\n",
        "            mask: Optional attention mask (batch, 1, 1, seq_len) or (batch, 1, seq_len, seq_len)\n",
        "        \n",
        "        Returns:\n",
        "            Output tensor of shape (batch, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        # Step 1: Project to Q, K, V\n",
        "        Q = # YOUR CODE\n",
        "        K = # YOUR CODE\n",
        "        V = # YOUR CODE\n",
        "        \n",
        "        # Step 2: Reshape to (batch, num_heads, seq_len, head_dim)\n",
        "        # Hint: view then transpose\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = # YOUR CODE (same pattern)\n",
        "        V = # YOUR CODE (same pattern)\n",
        "        \n",
        "        # Step 3: Compute attention scores\n",
        "        # scores = Q @ K^T / sqrt(head_dim)\n",
        "        scores = # YOUR CODE\n",
        "        \n",
        "        # Step 4: Apply mask if provided (for causal attention)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        \n",
        "        # Step 5: Softmax to get attention weights\n",
        "        attn_weights = # YOUR CODE\n",
        "        \n",
        "        # Step 6: Compute output = weights @ V\n",
        "        attn_output = # YOUR CODE\n",
        "        \n",
        "        # Step 7: Reshape back: transpose and concatenate heads\n",
        "        # From (batch, num_heads, seq_len, head_dim) â†’ (batch, seq_len, d_model)\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.view(batch_size, seq_len, self.d_model)\n",
        "        \n",
        "        # Step 8: Apply output projection\n",
        "        output = # YOUR CODE\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "# Test your implementation!\n",
        "print(\"Testing MyMultiHeadSelfAttention...\")\n",
        "\n",
        "d_model = 64\n",
        "num_heads = 4\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "# Create your MHSA module\n",
        "mhsa = MyMultiHeadSelfAttention(d_model, num_heads)\n",
        "\n",
        "# Create test input\n",
        "x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "# Forward pass\n",
        "output = mhsa(x)\n",
        "\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "# Verify output shape\n",
        "assert output.shape == (batch_size, seq_len, d_model), f\"Wrong output shape: {output.shape}\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Congratulations! You've built Multi-Head Self-Attention!\")\n",
        "print(\"âœ… Your implementation processes multiple attention heads in parallel\")\n",
        "print(\"âœ… Compare your code with the reference implementation below\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, 'embed_dim must be divisible by num_heads'\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        \n",
        "        # Linear projections\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_o = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    \n",
        "    def forward(self, X, mask=None):\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "        \n",
        "        # Create Q, K, V and split into heads\n",
        "        Q = self.W_q(X).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        K = self.W_k(X).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        V = self.W_v(X).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        \n",
        "        # Transpose: (batch, heads, seq_len, head_dim)\n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "        \n",
        "        # Attention\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attended = attention_weights @ V\n",
        "        \n",
        "        # Concatenate heads\n",
        "        attended = attended.transpose(1, 2).contiguous()\n",
        "        attended = attended.view(batch_size, seq_len, self.embed_dim)\n",
        "        \n",
        "        # Final projection\n",
        "        output = self.W_o(attended)\n",
        "        return output, attention_weights\n",
        "\n",
        "# Test\n",
        "mhsa = MultiHeadSelfAttention(embed_dim=8, num_heads=2)\n",
        "X_test = torch.randn(1, 4, 8)\n",
        "out, attn = mhsa(X_test)\n",
        "\n",
        "print('âœ… Multi-Head Self-Attention works!')\n",
        "print(f'Input: {X_test.shape}')\n",
        "print(f'Output: {out.shape}')\n",
        "print(f'Attention per head: {attn.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3.4: Visualizing What Different Heads Learn\n",
        "\n",
        "### ğŸ” Each Head Learns Different Patterns!\n",
        "\n",
        "Research has shown that different attention heads specialize in different linguistic phenomena:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    WHAT ATTENTION HEADS LEARN                           â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  HEAD 1: Syntactic Patterns                                            â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚\n",
        "â”‚     \"The [cat] that I [saw] yesterday was cute\"                        â”‚\n",
        "â”‚           â†‘                â†‘                                            â”‚\n",
        "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  subject-verb relationship                 â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  HEAD 2: Positional Patterns                                           â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚\n",
        "â”‚     \"I love [the] [beautiful] [sunset]\"                                â”‚\n",
        "â”‚              â†‘       â†‘           â†‘                                     â”‚\n",
        "â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  previous token attention           â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  HEAD 3: Coreference                                                   â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚\n",
        "â”‚     \"[John] went to the store. [He] bought milk.\"                      â”‚\n",
        "â”‚       â†‘                          â†‘                                      â”‚\n",
        "â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  pronoun resolution                  â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  HEAD 4: Punctuation/Separators                                        â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚\n",
        "â”‚     \"Hello [,] world [!]\"                                              â”‚\n",
        "â”‚              â†‘       â†‘                                                  â”‚\n",
        "â”‚        delimiter attention                                             â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  HEAD 5: Named Entities                                                â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚\n",
        "â”‚     \"[Paris] is the capital of [France]\"                               â”‚\n",
        "â”‚       â†‘                          â†‘                                      â”‚\n",
        "â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  entity-entity relations            â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ“Š Attention Patterns Visualization\n",
        "\n",
        "```\n",
        "Sentence: \"The cat sat on the mat\"\n",
        "\n",
        "Head 1 (Adjacent):     Head 2 (Subject-Verb):   Head 3 (Global):\n",
        "      T c s o t m            T c s o t m            T c s o t m\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "  T â”‚â–“â–‘â–‘â–‘â–‘â–‘â”‚        T â”‚â–‘â–‘â–‘â–‘â–‘â–‘â”‚        T â”‚â–“â–“â–“â–“â–“â–“â”‚\n",
        "  c â”‚â–“â–“â–‘â–‘â–‘â–‘â”‚        c â”‚â–‘â–“â–‘â–‘â–‘â–‘â”‚        c â”‚â–“â–“â–“â–“â–“â–“â”‚\n",
        "  s â”‚â–‘â–“â–“â–‘â–‘â–‘â”‚        s â”‚â–‘â–“â–‘â–‘â–‘â–‘â”‚        s â”‚â–“â–“â–“â–“â–“â–“â”‚\n",
        "  o â”‚â–‘â–‘â–“â–“â–‘â–‘â”‚        o â”‚â–‘â–‘â–‘â–‘â–‘â–‘â”‚        o â”‚â–“â–“â–“â–“â–“â–“â”‚\n",
        "  t â”‚â–‘â–‘â–‘â–“â–“â–‘â”‚        t â”‚â–‘â–‘â–‘â–‘â–‘â–‘â”‚        t â”‚â–“â–“â–“â–“â–“â–“â”‚\n",
        "  m â”‚â–‘â–‘â–‘â–‘â–“â–“â”‚        m â”‚â–‘â–‘â–‘â–‘â–‘â–‘â”‚        m â”‚â–“â–“â–“â–“â–“â–“â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "   \n",
        "   Attends to          Verb (sat) attends     All tokens attend\n",
        "   previous token      to subject (cat)       to all (global)\n",
        "```\n",
        "\n",
        "### ğŸ’¡ Why Multiple Patterns Matter\n",
        "\n",
        "Single-head attention can only learn ONE pattern. Multi-head learns MANY simultaneously!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizing Attention Head Patterns\n",
        "\n",
        "def visualize_attention_heads(attention_weights, tokens, title=\"Attention Patterns Per Head\"):\n",
        "    \"\"\"\n",
        "    Visualize attention patterns for each head separately.\n",
        "    \n",
        "    Args:\n",
        "        attention_weights: (batch, num_heads, seq_len, seq_len) or (num_heads, seq_len, seq_len)\n",
        "        tokens: List of token strings\n",
        "        title: Plot title\n",
        "    \"\"\"\n",
        "    # Handle batch dimension\n",
        "    if attention_weights.dim() == 4:\n",
        "        attention_weights = attention_weights[0]  # Take first batch\n",
        "    \n",
        "    num_heads = attention_weights.shape[0]\n",
        "    seq_len = attention_weights.shape[1]\n",
        "    \n",
        "    # Create subplot grid\n",
        "    cols = min(4, num_heads)\n",
        "    rows = (num_heads + cols - 1) // cols\n",
        "    \n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 4*rows))\n",
        "    if num_heads == 1:\n",
        "        axes = np.array([[axes]])\n",
        "    elif rows == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for h in range(num_heads):\n",
        "        ax = axes[h // cols, h % cols]\n",
        "        attn = attention_weights[h].detach().numpy()\n",
        "        \n",
        "        im = ax.imshow(attn, cmap='viridis', vmin=0, vmax=attn.max())\n",
        "        ax.set_title(f'Head {h+1}', fontweight='bold')\n",
        "        ax.set_xticks(range(seq_len))\n",
        "        ax.set_yticks(range(seq_len))\n",
        "        ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)\n",
        "        ax.set_yticklabels(tokens, fontsize=9)\n",
        "        \n",
        "        if h % cols == 0:\n",
        "            ax.set_ylabel('Query (FROM)', fontsize=10)\n",
        "        if h // cols == rows - 1:\n",
        "            ax.set_xlabel('Key (TO)', fontsize=10)\n",
        "    \n",
        "    # Hide unused subplots\n",
        "    for h in range(num_heads, rows * cols):\n",
        "        axes[h // cols, h % cols].axis('off')\n",
        "    \n",
        "    plt.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create a test sentence and run through MHSA\n",
        "print('='*70)\n",
        "print('VISUALIZING ATTENTION HEAD PATTERNS')\n",
        "print('='*70)\n",
        "\n",
        "# Create sentence embedding\n",
        "tokens_viz = ['The', 'cat', 'sat', 'on', 'mat']\n",
        "seq_len_viz = len(tokens_viz)\n",
        "embed_dim_viz = 16\n",
        "num_heads_viz = 4\n",
        "\n",
        "# Create random but structured input\n",
        "torch.manual_seed(42)\n",
        "X_viz = torch.randn(1, seq_len_viz, embed_dim_viz)\n",
        "\n",
        "# Create MHSA and run forward pass\n",
        "mhsa_viz = MultiHeadSelfAttention(embed_dim=embed_dim_viz, num_heads=num_heads_viz)\n",
        "output_viz, attn_viz = mhsa_viz(X_viz)\n",
        "\n",
        "print(f'\\nInput: \"{\" \".join(tokens_viz)}\"')\n",
        "print(f'Attention shape: {attn_viz.shape}')\n",
        "print(f'  â†’ (batch={attn_viz.shape[0]}, heads={attn_viz.shape[1]}, seq={attn_viz.shape[2]}, seq={attn_viz.shape[3]})')\n",
        "\n",
        "visualize_attention_heads(attn_viz, tokens_viz, \"What Each Attention Head Sees\")\n",
        "\n",
        "print('\\nğŸ’¡ Observations:')\n",
        "print('   â€¢ Each head learns a DIFFERENT pattern')\n",
        "print('   â€¢ Some heads may focus on adjacent tokens')\n",
        "print('   â€¢ Some heads may have broader attention')\n",
        "print('   â€¢ After training, patterns become meaningful!')\n",
        "print('   â€¢ In pre-trained models, heads specialize in syntax, coreference, etc.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 3.5: Attention Dropout - Regularization\n",
        "\n",
        "### ğŸ² Why Dropout in Attention?\n",
        "\n",
        "Dropout prevents overfitting by randomly \"dropping\" attention connections during training:\n",
        "\n",
        "```\n",
        "Without Dropout:                  With Dropout (p=0.1):\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 0.4  0.3  0.3   â”‚              â”‚ 0.4  0.0  0.3   â”‚  â† One connection dropped!\n",
        "â”‚ 0.5  0.2  0.3   â”‚      â†’       â”‚ 0.5  0.2  0.0   â”‚\n",
        "â”‚ 0.1  0.4  0.5   â”‚              â”‚ 0.1  0.4  0.5   â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "Dropped values are scaled up to maintain expected sum:\n",
        "  Original:  0.3 â†’ Dropped: 0.0\n",
        "  Others scaled by 1/(1-p) = 1/0.9 â‰ˆ 1.11\n",
        "```\n",
        "\n",
        "### ğŸ“Š Where Dropout is Applied in Transformers\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚               DROPOUT LOCATIONS IN TRANSFORMERS                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Input Embeddings                                                      â”‚\n",
        "â”‚         â”‚                                                               â”‚\n",
        "â”‚         â–¼                                                               â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
        "â”‚   â”‚ + Positionalâ”‚                                                       â”‚\n",
        "â”‚   â”‚  Encoding   â”‚                                                       â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
        "â”‚          â”‚                                                               â”‚\n",
        "â”‚          â–¼                                                               â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
        "â”‚   â”‚  DROPOUT 1  â”‚ â† After embedding + position                         â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
        "â”‚          â”‚                                                               â”‚\n",
        "â”‚          â–¼                                                               â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
        "â”‚   â”‚  Attention  â”‚                                                       â”‚\n",
        "â”‚   â”‚   softmax   â”‚                                                       â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
        "â”‚          â”‚                                                               â”‚\n",
        "â”‚          â–¼                                                               â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
        "â”‚   â”‚  DROPOUT 2  â”‚ â† After attention weights (attention dropout)        â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
        "â”‚          â”‚                                                               â”‚\n",
        "â”‚          â–¼                                                               â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
        "â”‚   â”‚ Output Proj â”‚                                                       â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
        "â”‚          â”‚                                                               â”‚\n",
        "â”‚          â–¼                                                               â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                       â”‚\n",
        "â”‚   â”‚  DROPOUT 3  â”‚ â† After attention output (before residual)           â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                       â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ”§ Typical Dropout Values\n",
        "\n",
        "| Model | Dropout Rate | Notes |\n",
        "|-------|--------------|-------|\n",
        "| BERT-base | 0.1 | Standard choice |\n",
        "| GPT-2 | 0.1 | Matches BERT |\n",
        "| GPT-3 | 0.0 | Large models don't need it |\n",
        "| LLaMA | 0.0 | Modern large models |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-Head Attention with Dropout\n",
        "\n",
        "class MultiHeadAttentionWithDropout(nn.Module):\n",
        "    \"\"\"\n",
        "    Production-ready Multi-Head Attention with dropout.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        \n",
        "        # Projections\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
        "        \n",
        "        # Dropout layers\n",
        "        self.attn_dropout = nn.Dropout(dropout)\n",
        "        self.proj_dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, X, mask=None):\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "        \n",
        "        # Create Q, K, V\n",
        "        Q = self.W_q(X).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = self.W_k(X).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = self.W_v(X).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        # Attention scores\n",
        "        scores = (Q @ K.transpose(-2, -1)) * self.scale\n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        \n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        \n",
        "        # Apply dropout to attention weights\n",
        "        attn = self.attn_dropout(attn)  # â† KEY: Dropout on attention\n",
        "        \n",
        "        # Apply to values\n",
        "        out = attn @ V\n",
        "        \n",
        "        # Concatenate and project\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        out = self.W_o(out)\n",
        "        \n",
        "        # Apply dropout to output\n",
        "        out = self.proj_dropout(out)  # â† KEY: Dropout on projection\n",
        "        \n",
        "        return out, attn\n",
        "\n",
        "# Demonstrate dropout effect\n",
        "print('='*70)\n",
        "print('ATTENTION DROPOUT DEMONSTRATION')\n",
        "print('='*70)\n",
        "\n",
        "X_drop = torch.randn(1, 4, 8)\n",
        "mha_with_dropout = MultiHeadAttentionWithDropout(embed_dim=8, num_heads=2, dropout=0.3)\n",
        "\n",
        "# Training mode: dropout is active\n",
        "mha_with_dropout.train()\n",
        "out_train1, attn_train1 = mha_with_dropout(X_drop)\n",
        "out_train2, attn_train2 = mha_with_dropout(X_drop)\n",
        "\n",
        "# Eval mode: dropout is disabled\n",
        "mha_with_dropout.eval()\n",
        "with torch.no_grad():\n",
        "    out_eval1, attn_eval1 = mha_with_dropout(X_drop)\n",
        "    out_eval2, attn_eval2 = mha_with_dropout(X_drop)\n",
        "\n",
        "print('\\nğŸ² Dropout Behavior:')\n",
        "print('\\nTraining mode (dropout=0.3):')\n",
        "print(f'   Two forward passes give DIFFERENT outputs:')\n",
        "print(f'   Difference norm: {torch.norm(out_train1 - out_train2):.4f}')\n",
        "\n",
        "print('\\nEval mode (dropout disabled):')\n",
        "print(f'   Two forward passes give SAME outputs:')\n",
        "print(f'   Difference norm: {torch.norm(out_eval1 - out_eval2):.6f}')\n",
        "\n",
        "print('\\nğŸ’¡ Key Points:')\n",
        "print('   â€¢ model.train() â†’ dropout ACTIVE (for training)')\n",
        "print('   â€¢ model.eval()  â†’ dropout DISABLED (for inference)')\n",
        "print('   â€¢ Dropout rate 0.1 is typical for transformers')\n",
        "print('   â€¢ Very large models often use dropout=0.0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 3.3: Implement MHSA from Scratch (ğŸ”´ Hard)\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Implement Multi-Head Self-Attention completely from scratch!\n",
        "\n",
        "### ğŸ“‹ Requirements\n",
        "Your implementation must:\n",
        "1. Split input into multiple heads\n",
        "2. Compute attention for each head separately\n",
        "3. Concatenate heads back together\n",
        "4. Apply output projection\n",
        "\n",
        "### ğŸ’¡ Template\n",
        "```python\n",
        "class MyMHSA(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        # TODO: Initialize W_q, W_k, W_v, W_o\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X):\n",
        "        # TODO: \n",
        "        # 1. Compute Q, K, V\n",
        "        # 2. Split into heads\n",
        "        # 3. Compute attention\n",
        "        # 4. Concatenate heads\n",
        "        # 5. Output projection\n",
        "        pass\n",
        "```\n",
        "\n",
        "### âœ… Test Your Implementation\n",
        "```python\n",
        "mhsa = MyMHSA(embed_dim=8, num_heads=2)\n",
        "x = torch.randn(1, 4, 8)\n",
        "out = mhsa(x)\n",
        "assert out.shape == (1, 4, 8), \"Output shape should match input shape\"\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”´ EXERCISE 3.3: Implement MHSA from Scratch\n",
        "\n",
        "class MyMHSA(nn.Module):\n",
        "    \"\"\"\n",
        "    YOUR IMPLEMENTATION HERE\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        \n",
        "        # TODO: Create W_q, W_k, W_v, W_o as nn.Linear layers\n",
        "        # self.W_q = ...\n",
        "        # self.W_k = ...\n",
        "        # self.W_v = ...\n",
        "        # self.W_o = ...\n",
        "        pass\n",
        "    \n",
        "    def forward(self, X):\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "        \n",
        "        # TODO: Implement the forward pass\n",
        "        # Step 1: Compute Q, K, V\n",
        "        # Step 2: Reshape to (batch, num_heads, seq_len, head_dim)\n",
        "        # Step 3: Compute scaled attention scores\n",
        "        # Step 4: Apply softmax\n",
        "        # Step 5: Apply to values\n",
        "        # Step 6: Concatenate heads\n",
        "        # Step 7: Apply output projection\n",
        "        \n",
        "        pass  # Remove this and add your implementation\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# ğŸ“ SOLUTION (uncomment to check)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# class MyMHSA(nn.Module):\n",
        "#     def __init__(self, embed_dim, num_heads):\n",
        "#         super().__init__()\n",
        "#         assert embed_dim % num_heads == 0\n",
        "#         \n",
        "#         self.embed_dim = embed_dim\n",
        "#         self.num_heads = num_heads\n",
        "#         self.head_dim = embed_dim // num_heads\n",
        "#         \n",
        "#         self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "#         self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "#         self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "#         self.W_o = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "#     \n",
        "#     def forward(self, X):\n",
        "#         batch_size, seq_len, _ = X.shape\n",
        "#         \n",
        "#         # Step 1: Compute Q, K, V\n",
        "#         Q = self.W_q(X)\n",
        "#         K = self.W_k(X)\n",
        "#         V = self.W_v(X)\n",
        "#         \n",
        "#         # Step 2: Reshape to (batch, num_heads, seq_len, head_dim)\n",
        "#         Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "#         K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "#         V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "#         \n",
        "#         # Step 3: Compute scaled attention scores\n",
        "#         scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "#         \n",
        "#         # Step 4: Apply softmax\n",
        "#         attn = torch.softmax(scores, dim=-1)\n",
        "#         \n",
        "#         # Step 5: Apply to values\n",
        "#         out = attn @ V\n",
        "#         \n",
        "#         # Step 6: Concatenate heads\n",
        "#         out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "#         \n",
        "#         # Step 7: Apply output projection\n",
        "#         out = self.W_o(out)\n",
        "#         \n",
        "#         return out\n",
        "# \n",
        "# # Test\n",
        "# mhsa_test = MyMHSA(embed_dim=8, num_heads=2)\n",
        "# x_test = torch.randn(1, 4, 8)\n",
        "# out_test = mhsa_test(x_test)\n",
        "# print(f'Input shape: {x_test.shape}')\n",
        "# print(f'Output shape: {out_test.shape}')\n",
        "# if out_test.shape == x_test.shape:\n",
        "#     print('âœ… SUCCESS! Output shape matches input shape')\n",
        "# else:\n",
        "#     print('âŒ Shape mismatch - check your implementation')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 4: Grouped Query Attention (GQA)\n",
        "\n",
        "## ğŸ¯ Memory Efficiency\n",
        "\n",
        "**Problem**: MHSA creates separate K,V for each head â†’ memory intensive\n",
        "\n",
        "**Solution**: Share K,V across groups of Q heads\n",
        "\n",
        "### ğŸ“Š Example\n",
        "```\n",
        "8 Q heads with 2 KV heads:\n",
        "  Q heads: [Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8]\n",
        "  K heads: [K1, K1, K1, K1, K2, K2, K2, K2]\n",
        "  V heads: [V1, V1, V1, V1, V2, V2, V2, V2]\n",
        "\n",
        "Memory reduction: 4Ã— fewer KV parameters!\n",
        "```\n",
        "\n",
        "### ğŸ”¢ Formula\n",
        "\n",
        "$$\n",
        "\\text{num\\_groups} = \\frac{\\text{num\\_heads}}{\\text{num\\_kv\\_heads}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, num_kv_heads):\n",
        "        super().__init__()\n",
        "        assert num_heads % num_kv_heads == 0\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_heads = num_kv_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.num_groups = num_heads // num_kv_heads\n",
        "        \n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_k = nn.Linear(embed_dim, num_kv_heads * self.head_dim, bias=False)\n",
        "        self.W_v = nn.Linear(embed_dim, num_kv_heads * self.head_dim, bias=False)\n",
        "        self.W_o = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "        \n",
        "        Q = self.W_q(X).view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        K = self.W_k(X).view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
        "        V = self.W_v(X).view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
        "        \n",
        "        Q = Q.transpose(1, 2)\n",
        "        K = K.transpose(1, 2)\n",
        "        V = V.transpose(1, 2)\n",
        "        \n",
        "        # Repeat K,V for each group\n",
        "        K = K.repeat_interleave(self.num_groups, dim=1)\n",
        "        V = V.repeat_interleave(self.num_groups, dim=1)\n",
        "        \n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = attn @ V\n",
        "        \n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "        return self.W_o(out), attn\n",
        "\n",
        "# Test\n",
        "gqa = GroupedQueryAttention(embed_dim=8, num_heads=4, num_kv_heads=2)\n",
        "out, _ = gqa(X_test)\n",
        "print('âœ… GQA works!')\n",
        "print(f'Output shape: {out.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 5: Sliding Window Attention (SWA)\n",
        "\n",
        "## ğŸ¯ Computational Efficiency\n",
        "\n",
        "**Problem**: Full attention is O(nÂ²) - expensive for long sequences\n",
        "\n",
        "**Solution**: Each token attends only to nearby tokens (local window)\n",
        "\n",
        "### ğŸ“Š Complexity\n",
        "```\n",
        "Full Attention: O(nÂ²)\n",
        "  1000 tokens = 1,000,000 connections\n",
        "\n",
        "Window Attention: O(nÃ—w) where w = window size\n",
        "  1000 tokens, w=50 = 50,000 connections\n",
        "  20Ã— faster!\n",
        "```\n",
        "\n",
        "### ğŸ”¢ Formula\n",
        "Mask positions where $|i - j| > w$:\n",
        "\n",
        "$$\n",
        "\\text{mask}_{ij} = \\begin{cases}\n",
        "1 & \\text{if } |i - j| \\leq w \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sliding_window_mask(seq_len, window_size):\n",
        "    mask = torch.zeros(seq_len, seq_len)\n",
        "    for i in range(seq_len):\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(seq_len, i + window_size + 1)\n",
        "        mask[i, start:end] = 1\n",
        "    return mask\n",
        "\n",
        "class SlidingWindowAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, window_size):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.window_size = window_size\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "        \n",
        "        Q = self.W_q(X)\n",
        "        K = self.W_k(X)\n",
        "        V = self.W_v(X)\n",
        "        \n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.embed_dim)\n",
        "        \n",
        "        # Apply sliding window mask\n",
        "        mask = create_sliding_window_mask(seq_len, self.window_size)\n",
        "        scores = scores.masked_fill(mask.unsqueeze(0) == 0, float('-inf'))\n",
        "        \n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = torch.nan_to_num(attn)  # Handle NaN from all -inf rows\n",
        "        out = attn @ V\n",
        "        \n",
        "        return out, attn\n",
        "\n",
        "# Test\n",
        "swa = SlidingWindowAttention(embed_dim=8, window_size=2)\n",
        "out, _ = swa(X_test)\n",
        "print('âœ… Sliding Window Attention works!')\n",
        "print(f'Output shape: {out.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Part 6: Summary & Comparison\n",
        "\n",
        "## ğŸ“Š Performance Comparison\n",
        "\n",
        "| Mechanism | Memory | Speed | Use Case |\n",
        "|-----------|--------|-------|----------|\n",
        "| **Self-Attention** | O(nÂ²) | Slow | Short sequences |\n",
        "| **MHSA** | O(nÂ²) | Slow | Rich patterns |\n",
        "| **GQA** | Reduced | Faster | Efficient LLMs |\n",
        "| **SWA** | O(nÃ—w) | Fast | Long documents |\n",
        "\n",
        "## ğŸ¯ When to Use What\n",
        "\n",
        "- **MHSA**: Standard choice, rich representation\n",
        "- **GQA**: Memory-constrained environments\n",
        "- **SWA**: Very long sequences (documents, books)\n",
        "- **Combined**: Best of both worlds!\n",
        "\n",
        "## ğŸ‰ Congratulations!\n",
        "\n",
        "You've mastered attention mechanisms - the heart of modern AI!\n",
        "\n",
        "### ğŸš€ Next Steps\n",
        "1. Read \"Attention is All You Need\" paper\n",
        "2. Implement a full Transformer\n",
        "3. Fine-tune a pretrained model\n",
        "4. Build your own LLM!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('='*70)\n",
        "print('ğŸ‰ CONGRATULATIONS!')\n",
        "print('='*70)\n",
        "print('\\nYou have completed the Attention Mechanisms Tutorial!')\n",
        "print('\\nYou now understand:')\n",
        "print('  âœ… Dot products and similarity')\n",
        "print('  âœ… Self-Attention (Q, K, V)')\n",
        "print('  âœ… Multi-Head Attention')\n",
        "print('  âœ… Grouped Query Attention')\n",
        "print('  âœ… Sliding Window Attention')\n",
        "print('\\nYou are ready to:')\n",
        "print('  ğŸš€ Read transformer papers')\n",
        "print('  ğŸš€ Implement your own models')\n",
        "print('  ğŸš€ Build amazing AI applications')\n",
        "print('\\nThank you for learning with us! ğŸ™')\n",
        "print('='*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-4\"></a>\n",
        "# Part 4: Grouped Query Attention (GQA) â±ï¸ ~25 min\n",
        "\n",
        "## Section 4.1: The Memory Problem\n",
        "\n",
        "### âš ï¸ MHSA Memory Bottleneck\n",
        "\n",
        "**Problem**: Multi-Head Attention creates separate K and V for EVERY head!\n",
        "\n",
        "For GPT-3 scale (96 heads, 12288 dimensions):\n",
        "```\n",
        "MHSA Memory Usage:\n",
        "  Q: 96 heads Ã— 128 dims = 12,288 parameters per layer\n",
        "  K: 96 heads Ã— 128 dims = 12,288 parameters per layer  \n",
        "  V: 96 heads Ã— 128 dims = 12,288 parameters per layer\n",
        "  \n",
        "  Total K+V: 24,576 parameters per layer\n",
        "  \n",
        "For long sequences (8K tokens):\n",
        "  K cache: 96 Ã— 8192 Ã— 128 = 100+ MB per layer!\n",
        "  V cache: 96 Ã— 8192 Ã— 128 = 100+ MB per layer!\n",
        "  \n",
        "With 96 layers: ~20 GB just for K,V cache! ğŸ”¥\n",
        "```\n",
        "\n",
        "### ğŸ’¡ The GQA Insight\n",
        "\n",
        "**Question**: Do we REALLY need separate K,V for each head?\n",
        "\n",
        "**Answer**: No! We can share K,V across groups of Q heads!\n",
        "\n",
        "### ğŸ“Š GQA Strategy\n",
        "\n",
        "```\n",
        "Standard MHSA (8 Q heads):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Q: [Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8]  8 query heads\n",
        "K: [K1, K2, K3, K4, K5, K6, K7, K8]  8 key heads\n",
        "V: [V1, V2, V3, V4, V5, V6, V7, V8]  8 value heads\n",
        "\n",
        "Total KV heads: 8\n",
        "\n",
        "GQA (8 Q heads, 2 KV heads):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Q: [Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8]  8 query heads\n",
        "K: [K1, K1, K1, K1, K2, K2, K2, K2]  2 key heads (shared!)\n",
        "V: [V1, V1, V1, V1, V2, V2, V2, V2]  2 value heads (shared!)\n",
        "\n",
        "Total KV heads: 2\n",
        "\n",
        "Memory savings: 4Ã— fewer KV parameters! ğŸ‰\n",
        "```\n",
        "\n",
        "### ğŸ”¢ The Math\n",
        "\n",
        "Given:\n",
        "- $h$ = number of query heads\n",
        "- $h_{kv}$ = number of KV heads\n",
        "\n",
        "Group size:\n",
        "$$\n",
        "g = \\frac{h}{h_{kv}}\n",
        "$$\n",
        "\n",
        "Memory reduction:\n",
        "$$\n",
        "\\text{KV memory} = \\frac{1}{g} \\times \\text{MHSA KV memory}\n",
        "$$\n",
        "\n",
        "Example:\n",
        "- 96 Q heads, 8 KV heads\n",
        "- Group size: $g = 96/8 = 12$\n",
        "- Memory: $1/12 = 8.3\\%$ of MHSA!\n",
        "- **12Ã— memory savings!** ğŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4.2: GQA Implementation Details\n",
        "\n",
        "### ğŸ—ï¸ Architecture\n",
        "\n",
        "```\n",
        "INPUT: X\n",
        "  â”‚\n",
        "  â”œâ”€â†’ W_Q â†’ Q (h heads)           8 heads\n",
        "  â”œâ”€â†’ W_K â†’ K (h_kv heads)        2 heads\n",
        "  â””â”€â†’ W_V â†’ V (h_kv heads)        2 heads\n",
        "  \n",
        "GROUPING:\n",
        "  Q heads: [Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8]\n",
        "  K heads: [K1, K1, K1, K1, K2, K2, K2, K2]  â† Repeat!\n",
        "  V heads: [V1, V1, V1, V1, V2, V2, V2, V2]  â† Repeat!\n",
        "  \n",
        "ATTENTION (normal):\n",
        "  For each Q head i:\n",
        "    group_id = i // group_size\n",
        "    attention[i] = softmax(Q[i] @ K[group_id].T) @ V[group_id]\n",
        "  \n",
        "OUTPUT: Concatenate and project\n",
        "```\n",
        "\n",
        "### ğŸ“ Shape Analysis\n",
        "\n",
        "```\n",
        "Example: 8 Q heads, 2 KV heads, dim=512\n",
        "\n",
        "Q projection:\n",
        "  W_Q: (512, 512) â†’ outputs all 8 Q heads\n",
        "  Q shape: (batch, 8, seq, 64)\n",
        "\n",
        "K/V projection:\n",
        "  W_K: (512, 128) â†’ outputs only 2 KV heads!\n",
        "  K shape: (batch, 2, seq, 64)\n",
        "  \n",
        "Repeat K,V:\n",
        "  K_expanded: (batch, 8, seq, 64)  [each repeated 4Ã—]\n",
        "  V_expanded: (batch, 8, seq, 64)  [each repeated 4Ã—]\n",
        "```\n",
        "\n",
        "### ğŸ’° Cost Comparison\n",
        "\n",
        "| Mechanism | Q params | K params | V params | Total |\n",
        "|-----------|----------|----------|----------|-------|\n",
        "| MHSA (8h) | 512Ã—512 | 512Ã—512 | 512Ã—512 | 1.5M |\n",
        "| GQA (8Q, 2KV) | 512Ã—512 | 512Ã—128 | 512Ã—128 | 393K |\n",
        "| **Savings** | 0% | **75%** | **75%** | **74%** |\n",
        "\n",
        "### âš¡ Performance Impact\n",
        "\n",
        "Research shows:\n",
        "- âœ… Minimal accuracy loss (<1%)\n",
        "- âœ… Much faster inference (less memory = more batching)\n",
        "- âœ… Enables longer context windows\n",
        "- âœ… Used in Llama-2, Mistral, and other modern LLMs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ› ï¸ BUILD IT YOURSELF: Grouped Query Attention Step-by-Step\n",
        "\n",
        "Now implement GQA yourself! The key insight is sharing K,V across groups of Q heads.\n",
        "\n",
        "### ğŸ“‹ What You'll Implement\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    YOUR GQA JOURNEY                                     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Exercise 4.A: Understand group size calculation                      â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 4.B: Create asymmetric projections (full Q, reduced K/V)    â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 4.C: Expand K,V to match Q heads (repeat_interleave)        â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 4.D: Complete GQA as a PyTorch module                       â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4.A: Understand GQA Group Size Calculation\n",
        "\n",
        "**Key Concept:** In GQA, each K/V head is shared by a GROUP of Q heads.\n",
        "\n",
        "Given:\n",
        "- `num_heads = 8` (number of query heads)\n",
        "- `num_kv_heads = 2` (number of key/value heads)\n",
        "\n",
        "Calculate:\n",
        "- `group_size = num_heads // num_kv_heads` (Q heads per K/V head)\n",
        "\n",
        "**Your Task:**\n",
        "1. Verify that `num_heads` is divisible by `num_kv_heads`\n",
        "2. Calculate `group_size`\n",
        "3. Calculate KV dimension vs full dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4.A: Understand GQA Group Size Calculation\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Configuration\n",
        "embed_dim = 64      # Total embedding dimension\n",
        "num_heads = 8       # Number of query heads\n",
        "num_kv_heads = 2    # Number of key/value heads (FEWER than query heads!)\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "# TODO: Assert that num_heads is divisible by num_kv_heads\n",
        "\n",
        "# YOUR ASSERTION HERE\n",
        "\n",
        "# TODO: Calculate group_size (how many Q heads share each K/V head)\n",
        "\n",
        "group_size = # YOUR CODE\n",
        "\n",
        "# TODO: Calculate head_dim\n",
        "\n",
        "head_dim = # YOUR CODE: embed_dim // num_heads\n",
        "\n",
        "# TODO: Calculate kv_dim (total dimension for K/V - smaller than embed_dim!)\n",
        "# Hint: num_kv_heads * head_dim\n",
        "\n",
        "kv_dim = # YOUR CODE\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Verification\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  embed_dim = {embed_dim}\")\n",
        "print(f\"  num_heads (Q) = {num_heads}\")\n",
        "print(f\"  num_kv_heads = {num_kv_heads}\")\n",
        "print(f\"\\nCalculated:\")\n",
        "print(f\"  group_size = {group_size} (each K/V head serves {group_size} Q heads)\")\n",
        "print(f\"  head_dim = {head_dim}\")\n",
        "print(f\"  kv_dim = {kv_dim} (vs full {embed_dim})\")\n",
        "\n",
        "# Memory savings calculation\n",
        "savings = (1 - kv_dim / embed_dim) * 100\n",
        "print(f\"\\nâœ… K/V dimension is {kv_dim} vs {embed_dim} for MHSA\")\n",
        "print(f\"âœ… Memory savings: {savings:.1f}% fewer K/V parameters!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4.B: Create Asymmetric Projections\n",
        "\n",
        "**The Key Difference from MHSA:** In GQA, K and V projections are SMALLER!\n",
        "\n",
        "- `W_q`: Projects to FULL dimension (all query heads)\n",
        "- `W_k`: Projects to REDUCED dimension (only num_kv_heads)\n",
        "- `W_v`: Projects to REDUCED dimension (only num_kv_heads)\n",
        "\n",
        "**Shape Comparison:**\n",
        "```\n",
        "MHSA projections:\n",
        "  W_q: (embed_dim, embed_dim) = (64, 64)\n",
        "  W_k: (embed_dim, embed_dim) = (64, 64)\n",
        "  W_v: (embed_dim, embed_dim) = (64, 64)\n",
        "\n",
        "GQA projections (with num_kv_heads = 2):\n",
        "  W_q: (embed_dim, embed_dim) = (64, 64)    â† Same!\n",
        "  W_k: (embed_dim, kv_dim)    = (64, 16)    â† Smaller!\n",
        "  W_v: (embed_dim, kv_dim)    = (64, 16)    â† Smaller!\n",
        "```\n",
        "\n",
        "**Your Task:**\n",
        "1. Create `W_q` that projects to full `embed_dim`\n",
        "2. Create `W_k` and `W_v` that project to `kv_dim`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4.B: Create Asymmetric Projections\n",
        "\n",
        "# Configuration from previous exercise\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "num_kv_heads = 2\n",
        "head_dim = embed_dim // num_heads\n",
        "kv_dim = num_kv_heads * head_dim  # This is SMALLER than embed_dim!\n",
        "\n",
        "print(f\"embed_dim = {embed_dim}\")\n",
        "print(f\"kv_dim = {kv_dim} (num_kv_heads Ã— head_dim = {num_kv_heads} Ã— {head_dim})\")\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "# TODO: Create W_q that projects from embed_dim to embed_dim (FULL dimension)\n",
        "\n",
        "W_q = # YOUR CODE: nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "# TODO: Create W_k that projects from embed_dim to kv_dim (REDUCED dimension!)\n",
        "\n",
        "W_k = # YOUR CODE: nn.Linear(embed_dim, ?)\n",
        "\n",
        "# TODO: Create W_v that projects from embed_dim to kv_dim (REDUCED dimension!)\n",
        "\n",
        "W_v = # YOUR CODE: nn.Linear(embed_dim, ?)\n",
        "\n",
        "# TODO: Create W_o that projects from embed_dim to embed_dim\n",
        "\n",
        "W_o = # YOUR CODE\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Verification\n",
        "print(f\"\\nProjection layers:\")\n",
        "print(f\"  W_q: {W_q.in_features} â†’ {W_q.out_features} (full)\")\n",
        "print(f\"  W_k: {W_k.in_features} â†’ {W_k.out_features} (reduced!)\")\n",
        "print(f\"  W_v: {W_v.in_features} â†’ {W_v.out_features} (reduced!)\")\n",
        "print(f\"  W_o: {W_o.in_features} â†’ {W_o.out_features}\")\n",
        "\n",
        "# Test with input\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "X = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "Q = W_q(X)\n",
        "K = W_k(X)\n",
        "V = W_v(X)\n",
        "\n",
        "print(f\"\\nâœ… Q shape: {Q.shape} (batch, seq, embed_dim)\")\n",
        "print(f\"âœ… K shape: {K.shape} (batch, seq, kv_dim) â† SMALLER!\")\n",
        "print(f\"âœ… V shape: {V.shape} (batch, seq, kv_dim) â† SMALLER!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4.C: Expand K,V to Match Q Heads (THE KEY GQA OPERATION!)\n",
        "\n",
        "**The Critical Step:** After projecting K and V to fewer heads, we need to EXPAND them to match Q's head count for the attention computation.\n",
        "\n",
        "**The Problem:**\n",
        "```\n",
        "Q has 8 heads: Q shape = (batch, 8, seq, head_dim)\n",
        "K has 2 heads: K shape = (batch, 2, seq, head_dim)\n",
        "\n",
        "We can't do Q @ K.T because dimensions don't match!\n",
        "```\n",
        "\n",
        "**The Solution:** Repeat each K/V head `group_size` times:\n",
        "```\n",
        "Before repeat:  K = [K1, K2]                 (2 heads)\n",
        "After repeat:   K = [K1, K1, K1, K1, K2, K2, K2, K2]  (8 heads)\n",
        "```\n",
        "\n",
        "**PyTorch Magic:** Use `repeat_interleave(group_size, dim=1)`\n",
        "- This repeats each element along dimension 1 by `group_size`\n",
        "\n",
        "**Your Task:**\n",
        "1. Reshape Q, K, V into heads format\n",
        "2. Use `repeat_interleave` to expand K and V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4.C: Expand K,V to Match Q Heads\n",
        "\n",
        "# Configuration\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "num_kv_heads = 2\n",
        "head_dim = embed_dim // num_heads\n",
        "group_size = num_heads // num_kv_heads\n",
        "kv_dim = num_kv_heads * head_dim\n",
        "batch_size = 2\n",
        "seq_len = 5\n",
        "\n",
        "# Simulated projections (output from W_q, W_k, W_v)\n",
        "Q_projected = torch.randn(batch_size, seq_len, embed_dim)    # Full dimension\n",
        "K_projected = torch.randn(batch_size, seq_len, kv_dim)       # Reduced dimension\n",
        "V_projected = torch.randn(batch_size, seq_len, kv_dim)       # Reduced dimension\n",
        "\n",
        "print(f\"Projected shapes:\")\n",
        "print(f\"  Q_projected: {Q_projected.shape} (full)\")\n",
        "print(f\"  K_projected: {K_projected.shape} (reduced)\")\n",
        "print(f\"  V_projected: {V_projected.shape} (reduced)\")\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "\n",
        "# Step 1: Reshape Q to (batch, num_heads, seq, head_dim)\n",
        "Q = Q_projected.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "print(f\"\\nQ reshaped: {Q.shape}\")\n",
        "\n",
        "# Step 2: Reshape K to (batch, num_kv_heads, seq, head_dim)\n",
        "# Note: num_kv_heads, not num_heads!\n",
        "\n",
        "K = # YOUR CODE: K_projected.view(batch_size, seq_len, ?, head_dim).transpose(1, 2)\n",
        "\n",
        "print(f\"K reshaped: {K.shape} (only {num_kv_heads} heads!)\")\n",
        "\n",
        "# Step 3: Reshape V similarly\n",
        "\n",
        "V = # YOUR CODE\n",
        "\n",
        "print(f\"V reshaped: {V.shape} (only {num_kv_heads} heads!)\")\n",
        "\n",
        "# Step 4: Expand K to match Q's head count using repeat_interleave\n",
        "# Hint: K.repeat_interleave(group_size, dim=1)\n",
        "\n",
        "K_expanded = # YOUR CODE\n",
        "\n",
        "print(f\"\\nK_expanded: {K_expanded.shape} (now matches Q!)\")\n",
        "\n",
        "# Step 5: Expand V similarly\n",
        "\n",
        "V_expanded = # YOUR CODE\n",
        "\n",
        "print(f\"V_expanded: {V_expanded.shape} (now matches Q!)\")\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Verification\n",
        "assert K_expanded.shape == Q.shape, f\"K_expanded {K_expanded.shape} != Q {Q.shape}\"\n",
        "assert V_expanded.shape == Q.shape, f\"V_expanded {V_expanded.shape} != Q {Q.shape}\"\n",
        "\n",
        "# Show what happened\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"How K heads were expanded:\")\n",
        "print(f\"  Original: K = [K1, K2]\")\n",
        "print(f\"  Expanded: K = [K1, K1, K1, K1, K2, K2, K2, K2]\")\n",
        "print(f\"\\nâœ… Now we can compute attention: Q @ K_expanded.T\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 4.D: Complete Grouped Query Attention Module\n",
        "\n",
        "**Now combine everything into a complete GQA module!**\n",
        "\n",
        "You've implemented all the pieces:\n",
        "- âœ… Calculate group size and kv_dim\n",
        "- âœ… Create asymmetric projections (full Q, reduced K/V)\n",
        "- âœ… Expand K,V using repeat_interleave\n",
        "- âœ… Standard attention computation\n",
        "\n",
        "**Your Task:** Create a complete `nn.Module` that implements GQA.\n",
        "\n",
        "**Efficiency Note:** In production, you might use `torch.repeat_interleave` or even more efficient techniques like broadcasting. The key insight is that K/V projections are smaller!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4.D: Complete Grouped Query Attention Module\n",
        "\n",
        "class MyGroupedQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Your implementation of Grouped Query Attention!\n",
        "    \n",
        "    Key difference from MHSA:\n",
        "    - K and V have FEWER heads than Q\n",
        "    - K,V are repeated to match Q for attention computation\n",
        "    - Significant memory savings during inference!\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim: int, num_heads: int, num_kv_heads: int):\n",
        "        super().__init__()\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        # TODO: Assert num_heads is divisible by num_kv_heads\n",
        "        assert num_heads % num_kv_heads == 0, \\\n",
        "            f\"num_heads ({num_heads}) must be divisible by num_kv_heads ({num_kv_heads})\"\n",
        "        \n",
        "        # TODO: Store configuration\n",
        "        self.embed_dim = # YOUR CODE\n",
        "        self.num_heads = # YOUR CODE\n",
        "        self.num_kv_heads = # YOUR CODE\n",
        "        self.head_dim = # YOUR CODE: embed_dim // num_heads\n",
        "        self.group_size = # YOUR CODE: num_heads // num_kv_heads\n",
        "        \n",
        "        # TODO: Calculate kv_dim\n",
        "        kv_dim = # YOUR CODE: num_kv_heads * self.head_dim\n",
        "        \n",
        "        # TODO: Create projection layers\n",
        "        # W_q: embed_dim â†’ embed_dim (full)\n",
        "        # W_k: embed_dim â†’ kv_dim (reduced!)\n",
        "        # W_v: embed_dim â†’ kv_dim (reduced!)\n",
        "        # W_o: embed_dim â†’ embed_dim\n",
        "        \n",
        "        self.W_q = # YOUR CODE\n",
        "        self.W_k = # YOUR CODE\n",
        "        self.W_v = # YOUR CODE\n",
        "        self.W_o = # YOUR CODE\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len, embed_dim)\n",
        "        \n",
        "        Returns:\n",
        "            Output tensor of shape (batch, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        # Step 1: Project to Q (full), K (reduced), V (reduced)\n",
        "        Q = # YOUR CODE\n",
        "        K = # YOUR CODE\n",
        "        V = # YOUR CODE\n",
        "        \n",
        "        # Step 2: Reshape Q to (batch, num_heads, seq, head_dim)\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        # Step 3: Reshape K,V to (batch, num_kv_heads, seq, head_dim)\n",
        "        K = # YOUR CODE\n",
        "        V = # YOUR CODE\n",
        "        \n",
        "        # Step 4: Expand K,V to match Q using repeat_interleave\n",
        "        K = # YOUR CODE: K.repeat_interleave(?, dim=1)\n",
        "        V = # YOUR CODE\n",
        "        \n",
        "        # Step 5: Standard attention computation\n",
        "        scores = # YOUR CODE: (Q @ K.transpose(-2, -1)) / math.sqrt(?)\n",
        "        attn_weights = # YOUR CODE: softmax\n",
        "        attn_output = # YOUR CODE: attn_weights @ V\n",
        "        \n",
        "        # Step 6: Reshape and project output\n",
        "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
        "        attn_output = attn_output.view(batch_size, seq_len, self.embed_dim)\n",
        "        output = # YOUR CODE\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "        return output\n",
        "\n",
        "\n",
        "# Test your implementation!\n",
        "print(\"Testing MyGroupedQueryAttention...\")\n",
        "\n",
        "embed_dim = 64\n",
        "num_heads = 8\n",
        "num_kv_heads = 2\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "# Create your GQA module\n",
        "gqa = MyGroupedQueryAttention(embed_dim, num_heads, num_kv_heads)\n",
        "\n",
        "# Create test input\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Forward pass\n",
        "output = gqa(x)\n",
        "\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "# Verify output shape\n",
        "assert output.shape == (batch_size, seq_len, embed_dim), f\"Wrong output shape: {output.shape}\"\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in gqa.parameters())\n",
        "print(f\"\\nTotal parameters: {total_params}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Congratulations! You've built Grouped Query Attention!\")\n",
        "print(f\"âœ… Using {num_kv_heads} KV heads instead of {num_heads}\")\n",
        "print(f\"âœ… Memory savings: {(1 - num_kv_heads/num_heads)*100:.0f}% fewer KV parameters!\")\n",
        "print(\"âœ… Compare your code with the reference implementation below\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GroupedQueryAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Grouped Query Attention - Memory-efficient variant of MHSA.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, num_kv_heads):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert num_heads % num_kv_heads == 0, \\\n",
        "            f'num_heads ({num_heads}) must be divisible by num_kv_heads ({num_kv_heads})'\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_kv_heads = num_kv_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.num_groups = num_heads // num_kv_heads\n",
        "        \n",
        "        print(f'Creating GQA:')\n",
        "        print(f'  embed_dim: {embed_dim}')\n",
        "        print(f'  Q heads: {num_heads}')\n",
        "        print(f'  KV heads: {num_kv_heads}')\n",
        "        print(f'  group_size: {self.num_groups}')\n",
        "        print(f'  head_dim: {self.head_dim}')\n",
        "        \n",
        "        # Q projects to full dimension (all heads)\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        \n",
        "        # K,V project to reduced dimension (fewer heads)\n",
        "        kv_dim = num_kv_heads * self.head_dim\n",
        "        self.W_k = nn.Linear(embed_dim, kv_dim, bias=False)\n",
        "        self.W_v = nn.Linear(embed_dim, kv_dim, bias=False)\n",
        "        \n",
        "        self.W_o = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        \n",
        "        # Report parameter savings\n",
        "        mhsa_kv_params = 2 * embed_dim * embed_dim\n",
        "        gqa_kv_params = 2 * embed_dim * kv_dim\n",
        "        savings = (1 - gqa_kv_params / mhsa_kv_params) * 100\n",
        "        print(f'  KV parameter savings: {savings:.1f}%')\n",
        "    \n",
        "    def forward(self, X, return_attention=False):\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "        \n",
        "        # Create Q (full heads)\n",
        "        Q = self.W_q(X)  # (batch, seq, embed_dim)\n",
        "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        Q = Q.transpose(1, 2)  # (batch, num_heads, seq, head_dim)\n",
        "        \n",
        "        # Create K,V (reduced heads)\n",
        "        K = self.W_k(X)  # (batch, seq, kv_dim)\n",
        "        V = self.W_v(X)\n",
        "        \n",
        "        K = K.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
        "        V = V.view(batch_size, seq_len, self.num_kv_heads, self.head_dim)\n",
        "        \n",
        "        K = K.transpose(1, 2)  # (batch, num_kv_heads, seq, head_dim)\n",
        "        V = V.transpose(1, 2)\n",
        "        \n",
        "        # Expand K,V to match Q heads by repeating\n",
        "        # (batch, num_kv_heads, seq, head_dim) â†’ (batch, num_heads, seq, head_dim)\n",
        "        K = K.repeat_interleave(self.num_groups, dim=1)\n",
        "        V = V.repeat_interleave(self.num_groups, dim=1)\n",
        "        \n",
        "        # Standard attention\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = attn @ V\n",
        "        \n",
        "        # Concatenate and project\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        out = out.view(batch_size, seq_len, self.embed_dim)\n",
        "        out = self.W_o(out)\n",
        "        \n",
        "        if return_attention:\n",
        "            return out, attn\n",
        "        return out\n",
        "\n",
        "# Test and compare\n",
        "print('='*70)\n",
        "print('TESTING GROUPED QUERY ATTENTION')\n",
        "print('='*70)\n",
        "\n",
        "X_test = torch.randn(1, 4, 8)\n",
        "\n",
        "print('\\nMHSA (baseline):')\n",
        "mhsa = MultiHeadSelfAttention(embed_dim=8, num_heads=4)\n",
        "\n",
        "print('\\nGQA (memory-efficient):')\n",
        "gqa = GroupedQueryAttention(embed_dim=8, num_heads=4, num_kv_heads=2)\n",
        "\n",
        "out_gqa, _ = gqa(X_test, return_attention=True)\n",
        "\n",
        "print(f'\\nâœ… GQA works!')\n",
        "print(f'Output shape: {out_gqa.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 4.1: GQA Parameter Counting\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Calculate memory savings from GQA\n",
        "\n",
        "### ğŸ“‹ Tasks\n",
        "\n",
        "**Task 1**: For Llama-2-70B configuration:\n",
        "- embed_dim = 8192\n",
        "- num_heads = 64\n",
        "- num_kv_heads = 8\n",
        "\n",
        "Calculate:\n",
        "a) Group size\n",
        "b) KV parameters in MHSA\n",
        "c) KV parameters in GQA  \n",
        "d) Percentage savings\n",
        "\n",
        "**Task 2**: Implement GQA with:\n",
        "- embed_dim = 12\n",
        "- num_heads = 6\n",
        "- num_kv_heads = 2\n",
        "- Test on (1, 3, 12) input\n",
        "\n",
        "### ğŸ’¡ Formulas\n",
        "- Group size: num_heads / num_kv_heads\n",
        "- MHSA KV params: 2 Ã— embed_dim Ã— embed_dim\n",
        "- GQA KV params: 2 Ã— embed_dim Ã— (num_kv_heads Ã— head_dim)\n",
        "- Savings: (1 - GQA/MHSA) Ã— 100%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Llama-2-70B calculations\n",
        "embed_dim_llama = 8192\n",
        "num_heads_llama = 64\n",
        "num_kv_heads_llama = 8\n",
        "\n",
        "group_size = None  # YOUR ANSWER\n",
        "mhsa_kv_params = None  # YOUR ANSWER\n",
        "gqa_kv_params = None  # YOUR ANSWER\n",
        "savings_pct = None  # YOUR ANSWER\n",
        "\n",
        "# Task 2: Implement and test\n",
        "# YOUR CODE:\n",
        "# gqa_custom = GroupedQueryAttention(?, ?, ?)\n",
        "# X_custom = torch.randn(?, ?, ?)\n",
        "# out_custom = gqa_custom(X_custom)\n",
        "\n",
        "# Uncomment to check:\n",
        "# print('Task 1 - Llama-2-70B:')\n",
        "# print(f'  Group size: {group_size} (expected: 8)')\n",
        "# print(f'  MHSA KV params: {mhsa_kv_params:,} (expected: 134,217,728)')\n",
        "# print(f'  GQA KV params: {gqa_kv_params:,} (expected: 16,777,216)')\n",
        "# print(f'  Savings: {savings_pct:.1f}% (expected: 87.5%)')\n",
        "# print(f'  Memory reduction: {mhsa_kv_params / gqa_kv_params:.0f}x!')\n",
        "#\n",
        "# print('\\nTask 2:')\n",
        "# print(f'  Output shape: {out_custom.shape} (expected: (1, 3, 12))')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 4.3: Benchmarking MHSA vs GQA\n",
        "\n",
        "### â±ï¸ Real Performance Comparison\n",
        "\n",
        "Let's measure actual speed and memory differences between MHSA and GQA!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Benchmarking MHSA vs GQA\n",
        "\n",
        "def benchmark_attention(attention_module, X, num_runs=50, warmup=10):\n",
        "    \"\"\"\n",
        "    Benchmark an attention module.\n",
        "    \n",
        "    Args:\n",
        "        attention_module: The attention module to benchmark\n",
        "        X: Input tensor\n",
        "        num_runs: Number of timing runs\n",
        "        warmup: Warmup runs (not timed)\n",
        "    \n",
        "    Returns:\n",
        "        avg_time_ms: Average time per forward pass in milliseconds\n",
        "    \"\"\"\n",
        "    attention_module.eval()\n",
        "    \n",
        "    # Warmup\n",
        "    with torch.no_grad():\n",
        "        for _ in range(warmup):\n",
        "            _ = attention_module(X)\n",
        "    \n",
        "    # Synchronize if using GPU\n",
        "    if X.is_cuda:\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    # Timing\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_runs):\n",
        "            _ = attention_module(X)\n",
        "    \n",
        "    if X.is_cuda:\n",
        "        torch.cuda.synchronize()\n",
        "    \n",
        "    elapsed = time.time() - start_time\n",
        "    avg_time_ms = (elapsed / num_runs) * 1000\n",
        "    \n",
        "    return avg_time_ms\n",
        "\n",
        "def count_parameters(module):\n",
        "    \"\"\"Count trainable parameters in a module.\"\"\"\n",
        "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "\n",
        "# Benchmark different configurations\n",
        "print('='*70)\n",
        "print('â±ï¸  BENCHMARKING: MHSA vs GQA')\n",
        "print('='*70)\n",
        "\n",
        "# Test configurations\n",
        "embed_dim = 256\n",
        "num_heads = 8\n",
        "seq_len = 128\n",
        "batch_size = 4\n",
        "\n",
        "configs = [\n",
        "    ('MHSA (8Q, 8KV)', num_heads, num_heads),      # Standard MHSA\n",
        "    ('GQA (8Q, 4KV)', num_heads, 4),               # 2x reduction\n",
        "    ('GQA (8Q, 2KV)', num_heads, 2),               # 4x reduction\n",
        "    ('GQA (8Q, 1KV)', num_heads, 1),               # 8x reduction (MQA)\n",
        "]\n",
        "\n",
        "results = []\n",
        "X_bench = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "print(f'\\nConfiguration:')\n",
        "print(f'  Batch size: {batch_size}')\n",
        "print(f'  Sequence length: {seq_len}')\n",
        "print(f'  Embedding dim: {embed_dim}')\n",
        "print(f'  Q heads: {num_heads}')\n",
        "print()\n",
        "\n",
        "print(f'{\"Config\":<20} {\"KV Heads\":<10} {\"Params\":<12} {\"Time (ms)\":<12} {\"Speedup\":<10}')\n",
        "print('-' * 70)\n",
        "\n",
        "baseline_time = None\n",
        "for name, n_heads, n_kv_heads in configs:\n",
        "    if n_kv_heads == n_heads:\n",
        "        # Standard MHSA\n",
        "        module = MultiHeadSelfAttention(embed_dim, n_heads)\n",
        "    else:\n",
        "        # GQA\n",
        "        module = GroupedQueryAttention(embed_dim, n_heads, n_kv_heads)\n",
        "    \n",
        "    params = count_parameters(module)\n",
        "    avg_time = benchmark_attention(module, X_bench)\n",
        "    \n",
        "    if baseline_time is None:\n",
        "        baseline_time = avg_time\n",
        "        speedup = 1.0\n",
        "    else:\n",
        "        speedup = baseline_time / avg_time\n",
        "    \n",
        "    results.append((name, n_kv_heads, params, avg_time, speedup))\n",
        "    print(f'{name:<20} {n_kv_heads:<10} {params:<12,} {avg_time:<12.3f} {speedup:<10.2f}x')\n",
        "\n",
        "# Visualize results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "names = [r[0] for r in results]\n",
        "params = [r[2] for r in results]\n",
        "times = [r[3] for r in results]\n",
        "speedups = [r[4] for r in results]\n",
        "\n",
        "# Parameters\n",
        "ax = axes[0]\n",
        "bars = ax.bar(range(len(names)), params, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'])\n",
        "ax.set_xticks(range(len(names)))\n",
        "ax.set_xticklabels([n.split('(')[1].split(')')[0] for n in names], rotation=0)\n",
        "ax.set_ylabel('Number of Parameters', fontweight='bold')\n",
        "ax.set_title('Parameter Count', fontweight='bold')\n",
        "ax.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
        "for i, (bar, p) in enumerate(zip(bars, params)):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{p:,}', \n",
        "            ha='center', va='bottom', fontsize=9, rotation=0)\n",
        "\n",
        "# Time\n",
        "ax = axes[1]\n",
        "bars = ax.bar(range(len(names)), times, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'])\n",
        "ax.set_xticks(range(len(names)))\n",
        "ax.set_xticklabels([n.split('(')[1].split(')')[0] for n in names], rotation=0)\n",
        "ax.set_ylabel('Time (ms)', fontweight='bold')\n",
        "ax.set_title('Forward Pass Time', fontweight='bold')\n",
        "for bar, t in zip(bars, times):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{t:.2f}', \n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Speedup\n",
        "ax = axes[2]\n",
        "bars = ax.bar(range(len(names)), speedups, color=['#3498db', '#2ecc71', '#f39c12', '#e74c3c'])\n",
        "ax.set_xticks(range(len(names)))\n",
        "ax.set_xticklabels([n.split('(')[1].split(')')[0] for n in names], rotation=0)\n",
        "ax.set_ylabel('Speedup vs MHSA', fontweight='bold')\n",
        "ax.set_title('Relative Speedup', fontweight='bold')\n",
        "ax.axhline(y=1.0, color='black', linestyle='--', alpha=0.5)\n",
        "for bar, s in zip(bars, speedups):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{s:.2f}x', \n",
        "            ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.suptitle(f'MHSA vs GQA Performance (batch={batch_size}, seq={seq_len}, dim={embed_dim})', \n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nğŸ’¡ Key Observations:')\n",
        "print('   â€¢ Fewer KV heads = fewer parameters = faster')\n",
        "print('   â€¢ GQA (8Q, 1KV) is \"Multi-Query Attention\" (MQA)')\n",
        "print('   â€¢ Real speedups are even larger on GPU with longer sequences')\n",
        "print('   â€¢ Memory savings scale with sequence length!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-5\"></a>\n",
        "# Part 5: Sliding Window Attention (SWA) â±ï¸ ~20 min\n",
        "\n",
        "## Section 5.1: The Quadratic Complexity Problem\n",
        "\n",
        "### âš ï¸ Full Attention is O(nÂ²)\n",
        "\n",
        "**Problem**: Computing attention for all pairs is expensive!\n",
        "\n",
        "```\n",
        "Sequence length: n\n",
        "Attention scores: n Ã— n matrix\n",
        "\n",
        "Operations: O(nÂ²)\n",
        "\n",
        "Examples:\n",
        "  n =    100 â†’      10,000 computations\n",
        "  n =  1,000 â†’   1,000,000 computations (100Ã— more)\n",
        "  n = 10,000 â†’ 100,000,000 computations (10,000Ã— more!)\n",
        "```\n",
        "\n",
        "**Quadratic scaling kills performance on long sequences!**\n",
        "\n",
        "### ğŸ’¡ The Local Attention Insight\n",
        "\n",
        "**Key observation**: Most important information is NEARBY!\n",
        "\n",
        "In \"The cat sat on the mat\":\n",
        "- \"cat\" needs to attend to \"sat\" (next word) âœ…\n",
        "- \"cat\" needs to attend to \"the\" (previous word) âœ…\n",
        "- \"cat\" attend to words 100 positions away? âŒ Probably not!\n",
        "\n",
        "### ğŸ“Š Sliding Window Strategy\n",
        "\n",
        "```\n",
        "Full Attention (every position attends to all):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "      0   1   2   3   4   5   6   7\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "  0 â”‚ â–    â–    â–    â–    â–    â–    â–    â–  â”‚  All pairs!\n",
        "  1 â”‚ â–    â–    â–    â–    â–    â–    â–    â–  â”‚\n",
        "  2 â”‚ â–    â–    â–    â–    â–    â–    â–    â–  â”‚\n",
        "  3 â”‚ â–    â–    â–    â–    â–    â–    â–    â–  â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "  64 connections for 8 words\n",
        "\n",
        "Sliding Window (window_size=2):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "      0   1   2   3   4   5   6   7\n",
        "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "  0 â”‚ â–    â–    â–    .   .   .   .   . â”‚  Local only!\n",
        "  1 â”‚ â–    â–    â–    â–    .   .   .   . â”‚\n",
        "  2 â”‚ â–    â–    â–    â–    â–    .   .   . â”‚\n",
        "  3 â”‚ .   â–    â–    â–    â–    â–    .   . â”‚\n",
        "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "  Only 36 connections!\n",
        "  44% reduction for just 8 words\n",
        "```\n",
        "\n",
        "### ğŸ”¢ Complexity Analysis\n",
        "\n",
        "Given:\n",
        "- $n$ = sequence length\n",
        "- $w$ = window size\n",
        "\n",
        "Connections:\n",
        "$$\n",
        "\\begin{align}\n",
        "\\text{Full Attention:} & \\quad O(n^2) \\\\\n",
        "\\text{Sliding Window:} & \\quad O(n \\times w)\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "For $w \\ll n$, this is nearly **linear**!\n",
        "\n",
        "Example:\n",
        "- $n = 10{,}000$, $w = 100$\n",
        "- Full: $10{,}000^2 = 100{,}000{,}000$ operations\n",
        "- Window: $10{,}000 \\times 100 = 1{,}000{,}000$ operations\n",
        "- **100Ã— faster!** ğŸš€"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ› ï¸ BUILD IT YOURSELF: Sliding Window Attention Step-by-Step\n",
        "\n",
        "Now implement SWA yourself! The key insight is limiting attention to nearby positions.\n",
        "\n",
        "### ğŸ“‹ What You'll Implement\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    YOUR SWA JOURNEY                                     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Exercise 5.A: Create a sliding window mask                           â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 5.B: Apply mask to attention scores                         â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 5.C: Complete SWA as a PyTorch module                       â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5.A: Create a Sliding Window Mask\n",
        "\n",
        "**The Core Idea:** Each position can only attend to positions within `window_size` steps!\n",
        "\n",
        "For position `i` with `window_size = 2`:\n",
        "- Can attend to: `[i-2, i-1, i, i+1, i+2]` (if they exist)\n",
        "- Cannot attend to: positions beyond the window\n",
        "\n",
        "**Mask Structure:**\n",
        "```\n",
        "window_size = 1:        window_size = 2:\n",
        "  0 1 2 3 4               0 1 2 3 4\n",
        "0 [1 1 . . .]           0 [1 1 1 . .]\n",
        "1 [1 1 1 . .]           1 [1 1 1 1 .]\n",
        "2 [. 1 1 1 .]           2 [1 1 1 1 1]\n",
        "3 [. . 1 1 1]           3 [. 1 1 1 1]\n",
        "4 [. . . 1 1]           4 [. . 1 1 1]\n",
        "```\n",
        "\n",
        "**Your Task:**\n",
        "1. Create a (seq_len, seq_len) mask matrix\n",
        "2. For each row i, set 1s for columns in range [i-window_size, i+window_size]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 5.A: Create a Sliding Window Mask\n",
        "\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def my_sliding_window_mask(seq_len: int, window_size: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Create a sliding window attention mask.\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Length of the sequence\n",
        "        window_size: How many positions on each side can be attended to\n",
        "        \n",
        "    Returns:\n",
        "        mask: (seq_len, seq_len) tensor with 1s for allowed attention, 0s otherwise\n",
        "    \"\"\"\n",
        "    # ============ YOUR CODE HERE ============\n",
        "    \n",
        "    # Step 1: Initialize mask with zeros\n",
        "    mask = # YOUR CODE: torch.zeros(?, ?)\n",
        "    \n",
        "    # Step 2: For each position i, set 1s for positions in [i-window_size, i+window_size]\n",
        "    for i in range(seq_len):\n",
        "        # Calculate start position (can't go below 0)\n",
        "        start = # YOUR CODE: max(0, ?)\n",
        "        \n",
        "        # Calculate end position (can't exceed seq_len)\n",
        "        end = # YOUR CODE: min(seq_len, ?)\n",
        "        \n",
        "        # Set these positions to 1\n",
        "        mask[i, start:end] = 1\n",
        "    \n",
        "    # =========================================\n",
        "    \n",
        "    return mask\n",
        "\n",
        "\n",
        "# Test your mask function\n",
        "seq_len = 8\n",
        "window_size = 2\n",
        "\n",
        "mask = my_sliding_window_mask(seq_len, window_size)\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(mask.numpy(), cmap='Blues', vmin=0, vmax=1)\n",
        "plt.colorbar(label='Can Attend (1) / Masked (0)')\n",
        "plt.title(f'Sliding Window Mask (window_size={window_size})', fontweight='bold', fontsize=14)\n",
        "plt.xlabel('To Position', fontweight='bold')\n",
        "plt.ylabel('From Position', fontweight='bold')\n",
        "\n",
        "# Add grid\n",
        "for i in range(seq_len + 1):\n",
        "    plt.axhline(i - 0.5, color='white', linewidth=0.5)\n",
        "    plt.axvline(i - 0.5, color='white', linewidth=0.5)\n",
        "\n",
        "plt.xticks(range(seq_len))\n",
        "plt.yticks(range(seq_len))\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Verify\n",
        "connections = mask.sum().item()\n",
        "full_connections = seq_len * seq_len\n",
        "reduction = (1 - connections / full_connections) * 100\n",
        "\n",
        "print(f\"âœ… Mask shape: {mask.shape}\")\n",
        "print(f\"âœ… Connections: {int(connections)} / {full_connections}\")\n",
        "print(f\"âœ… Reduction: {reduction:.1f}% fewer connections!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5.B: Apply Mask to Attention Scores\n",
        "\n",
        "**How Masking Works:** We don't just multiply by 0â€”we set scores to -âˆ BEFORE softmax!\n",
        "\n",
        "Why? Because:\n",
        "```\n",
        "softmax([-âˆ, 0.5, 0.5]) = [0.0, 0.5, 0.5]  âœ… Properly normalized!\n",
        "softmax([0, 0.5, 0.5]) Ã— [0,1,1] = [0.33, 0.33, 0.33] Ã— [0,1,1] = [0, 0.33, 0.33]  âŒ Doesn't sum to 1!\n",
        "```\n",
        "\n",
        "**The Pattern:**\n",
        "```python\n",
        "# Where mask is 0, set score to -infinity\n",
        "scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "# Then softmax will give 0 probability to masked positions\n",
        "attn_weights = F.softmax(scores, dim=-1)\n",
        "```\n",
        "\n",
        "**Your Task:**\n",
        "1. Compute raw attention scores (Q @ K^T / âˆšd)\n",
        "2. Apply the sliding window mask\n",
        "3. Apply softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 5.B: Apply Mask to Attention Scores\n",
        "\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "# Setup\n",
        "embed_dim = 16\n",
        "seq_len = 8\n",
        "window_size = 2\n",
        "batch_size = 1\n",
        "\n",
        "# Simulated Q, K, V (already projected)\n",
        "Q = torch.randn(batch_size, seq_len, embed_dim)\n",
        "K = torch.randn(batch_size, seq_len, embed_dim)\n",
        "V = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Get the sliding window mask (from Exercise 5.A)\n",
        "mask = my_sliding_window_mask(seq_len, window_size)\n",
        "print(f\"Mask shape: {mask.shape}\")\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "\n",
        "# Step 1: Compute raw attention scores\n",
        "# Formula: scores = Q @ K^T / sqrt(embed_dim)\n",
        "\n",
        "scores = # YOUR CODE\n",
        "\n",
        "print(f\"Scores shape: {scores.shape}\")\n",
        "print(f\"Sample raw scores:\\n{scores[0, 0, :5]}\")\n",
        "\n",
        "# Step 2: Apply the mask\n",
        "# Use masked_fill: where mask is 0, set to -infinity\n",
        "# Note: You might need to unsqueeze the mask to match batch dimension\n",
        "\n",
        "mask_expanded = mask.unsqueeze(0)  # Add batch dimension: (1, seq_len, seq_len)\n",
        "masked_scores = # YOUR CODE: scores.masked_fill(mask_expanded == 0, ?)\n",
        "\n",
        "print(f\"\\nAfter masking (row 0, showing -inf for masked positions):\")\n",
        "print(masked_scores[0, 0, :])\n",
        "\n",
        "# Step 3: Apply softmax (masked positions become 0 probability)\n",
        "\n",
        "attn_weights = # YOUR CODE: F.softmax(masked_scores, dim=?)\n",
        "\n",
        "# Handle any NaN values (can happen if all positions are masked)\n",
        "attn_weights = torch.nan_to_num(attn_weights)\n",
        "\n",
        "print(f\"\\nAttention weights (row 0 - should sum to 1):\")\n",
        "print(attn_weights[0, 0, :])\n",
        "print(f\"Sum: {attn_weights[0, 0, :].sum().item():.4f}\")\n",
        "\n",
        "# Step 4: Compute output\n",
        "\n",
        "output = # YOUR CODE: attn_weights @ V\n",
        "\n",
        "print(f\"\\nOutput shape: {output.shape}\")\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Visualize the attention pattern\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(mask.numpy(), cmap='Blues')\n",
        "plt.title('Window Mask', fontweight='bold')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(attn_weights[0].detach().numpy(), cmap='Blues')\n",
        "plt.title('Attention Weights (After Softmax)', fontweight='bold')\n",
        "plt.colorbar()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ… Masked positions have 0 attention weight!\")\n",
        "print(\"âœ… Each row's attention sums to 1.0\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 5.C: Complete Sliding Window Attention Module\n",
        "\n",
        "**Now combine everything into a complete SWA module!**\n",
        "\n",
        "You've implemented:\n",
        "- âœ… Creating a sliding window mask\n",
        "- âœ… Applying the mask with -infinity before softmax\n",
        "- âœ… Computing masked attention output\n",
        "\n",
        "**Your Task:** Create a complete `nn.Module` for Sliding Window Attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 5.C: Complete Sliding Window Attention Module\n",
        "\n",
        "class MySlidingWindowAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Your implementation of Sliding Window Attention!\n",
        "    \n",
        "    Key insight: Each position only attends to nearby positions,\n",
        "    reducing complexity from O(nÂ²) to O(n Ã— window_size).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim: int, window_size: int):\n",
        "        super().__init__()\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        # Store configuration\n",
        "        self.embed_dim = # YOUR CODE\n",
        "        self.window_size = # YOUR CODE\n",
        "        \n",
        "        # Create projection layers\n",
        "        self.W_q = # YOUR CODE\n",
        "        self.W_k = # YOUR CODE\n",
        "        self.W_v = # YOUR CODE\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "    def forward(self, x: torch.Tensor, return_attention: bool = False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch, seq_len, embed_dim)\n",
        "            return_attention: If True, also return attention weights\n",
        "            \n",
        "        Returns:\n",
        "            output: (batch, seq_len, embed_dim)\n",
        "            attn_weights: (batch, seq_len, seq_len) if return_attention=True\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        # Step 1: Project to Q, K, V\n",
        "        Q = # YOUR CODE\n",
        "        K = # YOUR CODE\n",
        "        V = # YOUR CODE\n",
        "        \n",
        "        # Step 2: Compute attention scores (Q @ K^T / sqrt(d))\n",
        "        scores = # YOUR CODE\n",
        "        \n",
        "        # Step 3: Create sliding window mask\n",
        "        mask = my_sliding_window_mask(seq_len, self.window_size)\n",
        "        mask = mask.unsqueeze(0)  # Add batch dimension\n",
        "        \n",
        "        # Step 4: Apply mask (set masked positions to -inf)\n",
        "        scores = # YOUR CODE: scores.masked_fill(?, float('-inf'))\n",
        "        \n",
        "        # Step 5: Softmax\n",
        "        attn_weights = # YOUR CODE\n",
        "        attn_weights = torch.nan_to_num(attn_weights)  # Handle edge cases\n",
        "        \n",
        "        # Step 6: Compute output\n",
        "        output = # YOUR CODE\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "        if return_attention:\n",
        "            return output, attn_weights\n",
        "        return output\n",
        "\n",
        "\n",
        "# Test your implementation!\n",
        "print(\"Testing MySlidingWindowAttention...\")\n",
        "\n",
        "embed_dim = 32\n",
        "window_size = 2\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "# Create your SWA module\n",
        "swa = MySlidingWindowAttention(embed_dim, window_size)\n",
        "\n",
        "# Create test input\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Forward pass\n",
        "output, attn = swa(x, return_attention=True)\n",
        "\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Attention shape: {attn.shape}\")\n",
        "\n",
        "# Verify output shape\n",
        "assert output.shape == (batch_size, seq_len, embed_dim), f\"Wrong output shape: {output.shape}\"\n",
        "\n",
        "# Verify attention is sparse (many zeros)\n",
        "zeros_pct = (attn == 0).float().mean().item() * 100\n",
        "print(f\"\\nAttention sparsity: {zeros_pct:.1f}% zeros (positions outside window)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Congratulations! You've built Sliding Window Attention!\")\n",
        "print(f\"âœ… Window size {window_size} limits attention to {2*window_size+1} positions\")\n",
        "print(\"âœ… Compare your code with the reference implementation below\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_sliding_window_mask(seq_len, window_size):\n",
        "    \"\"\"\n",
        "    Create mask for sliding window attention.\n",
        "    \n",
        "    Args:\n",
        "        seq_len: Length of sequence\n",
        "        window_size: Number of positions to attend on each side\n",
        "    \n",
        "    Returns:\n",
        "        mask: (seq_len, seq_len) binary mask\n",
        "              1 = attend, 0 = mask out\n",
        "    \"\"\"\n",
        "    mask = torch.zeros(seq_len, seq_len)\n",
        "    \n",
        "    for i in range(seq_len):\n",
        "        # Attend to [i-window_size, i+window_size]\n",
        "        start = max(0, i - window_size)\n",
        "        end = min(seq_len, i + window_size + 1)\n",
        "        mask[i, start:end] = 1\n",
        "    \n",
        "    return mask\n",
        "\n",
        "# Visualize different window sizes\n",
        "seq_len = 10\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "for idx, window_size in enumerate([1, 2, 4]):\n",
        "    mask = create_sliding_window_mask(seq_len, window_size)\n",
        "    \n",
        "    ax = axes[idx]\n",
        "    im = ax.imshow(mask.numpy(), cmap='Blues', vmin=0, vmax=1)\n",
        "    ax.set_title(f'Window Size = {window_size}', fontweight='bold', fontsize=14)\n",
        "    ax.set_xlabel('Position (to)', fontweight='bold')\n",
        "    ax.set_ylabel('Position (from)', fontweight='bold')\n",
        "    \n",
        "    # Count connections\n",
        "    connections = mask.sum().item()\n",
        "    full_connections = seq_len * seq_len\n",
        "    reduction = (1 - connections / full_connections) * 100\n",
        "    \n",
        "    ax.text(0.5, -0.15, \n",
        "           f'{int(connections)}/{int(full_connections)} connections ({reduction:.0f}% reduction)',\n",
        "           transform=ax.transAxes, ha='center', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('ğŸ‘€ Notice: Larger windows = more connections = more computation')\n",
        "print('Trade-off: window size vs. long-range dependencies')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SlidingWindowAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Sliding Window Attention - O(nÃ—w) complexity.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, window_size):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.window_size = window_size\n",
        "        \n",
        "        print(f'Creating SWA:')\n",
        "        print(f'  embed_dim: {embed_dim}')\n",
        "        print(f'  window_size: {window_size}')\n",
        "        \n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    \n",
        "    def forward(self, X, return_attention=False):\n",
        "        batch_size, seq_len, _ = X.shape\n",
        "        \n",
        "        Q = self.W_q(X)\n",
        "        K = self.W_k(X)\n",
        "        V = self.W_v(X)\n",
        "        \n",
        "        # Compute attention scores\n",
        "        scores = (Q @ K.transpose(-2, -1)) / math.sqrt(self.embed_dim)\n",
        "        \n",
        "        # Create and apply sliding window mask\n",
        "        mask = create_sliding_window_mask(seq_len, self.window_size)\n",
        "        mask = mask.unsqueeze(0)  # Add batch dimension\n",
        "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        \n",
        "        # Softmax\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = torch.nan_to_num(attn)  # Handle NaN from masked positions\n",
        "        \n",
        "        # Apply to values\n",
        "        out = attn @ V\n",
        "        \n",
        "        if return_attention:\n",
        "            return out, attn\n",
        "        return out\n",
        "\n",
        "# Test\n",
        "print('\\n' + '='*70)\n",
        "print('TESTING SLIDING WINDOW ATTENTION')\n",
        "print('='*70)\n",
        "\n",
        "X_test = torch.randn(1, 8, 16)\n",
        "\n",
        "swa = SlidingWindowAttention(embed_dim=16, window_size=2)\n",
        "out, attn = swa(X_test, return_attention=True)\n",
        "\n",
        "print(f'\\nInput: {X_test.shape}')\n",
        "print(f'Output: {out.shape}')\n",
        "print(f'Attention: {attn.shape}')\n",
        "print('\\nâœ… Sliding Window Attention works!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 5.1: Complexity Analysis\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Calculate computational savings from sliding windows\n",
        "\n",
        "### ğŸ“‹ Tasks\n",
        "\n",
        "**Task 1**: For a 4096-token document:\n",
        "a) Full attention: how many connections?\n",
        "b) Window size 128: how many connections?\n",
        "c) Speedup factor?\n",
        "\n",
        "**Task 2**: What window size gives 10Ã— speedup for 2048 tokens?\n",
        "\n",
        "**Task 3**: Visualize attention pattern for window_size=3 on 8 tokens\n",
        "\n",
        "### ğŸ’¡ Formulas\n",
        "- Full: $n^2$ connections\n",
        "- Window: approximately $n \\times (2w + 1)$ connections\n",
        "- Speedup: Full / Window"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1\n",
        "n = 4096\n",
        "w = 128\n",
        "\n",
        "full_connections = None  # YOUR ANSWER: n^2\n",
        "window_connections = None  # YOUR ANSWER: n Ã— (2w + 1)\n",
        "speedup = None  # YOUR ANSWER: full / window\n",
        "\n",
        "# Task 2\n",
        "n2 = 2048\n",
        "target_speedup = 10\n",
        "# Solve: n^2 / (n Ã— (2w + 1)) = 10\n",
        "# => n / (2w + 1) = 10\n",
        "# => 2w + 1 = n / 10\n",
        "required_window = None  # YOUR ANSWER\n",
        "\n",
        "# Task 3: Visualize\n",
        "# YOUR CODE:\n",
        "# mask_viz = create_sliding_window_mask(8, 3)\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.imshow(mask_viz.numpy(), cmap='Blues')\n",
        "# plt.title('Window Size = 3', fontweight='bold')\n",
        "# plt.xlabel('To')\n",
        "# plt.ylabel('From')\n",
        "# plt.colorbar(label='Can Attend')\n",
        "# plt.show()\n",
        "\n",
        "# Uncomment to check:\n",
        "# print('Task 1:')\n",
        "# print(f'  Full: {full_connections:,} (expected: 16,777,216)')\n",
        "# print(f'  Window: {window_connections:,} (expected: 1,052,672)')\n",
        "# print(f'  Speedup: {speedup:.1f}Ã— (expected: ~16Ã—)')\n",
        "#\n",
        "# print('\\nTask 2:')\n",
        "# print(f'  Required window: {required_window} (expected: ~102)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-6\"></a>\n",
        "# Part 6: Multi-head Latent Attention (MLA) - DeepSeek's Innovation â±ï¸ ~35 min\n",
        "\n",
        "## Section 6.1: The KV Cache Problem Revisited\n",
        "\n",
        "### ğŸ¤” Where We Are\n",
        "\n",
        "We've seen several attention variants trying to reduce memory:\n",
        "\n",
        "| Mechanism | KV Cache Size | Capability | Used In |\n",
        "|-----------|---------------|------------|---------|\n",
        "| MHA | $2 n_h d_h l$ | Strong | GPT-2/3, BERT |\n",
        "| GQA | $2 n_g d_h l$ | Moderate | LLaMA-2, Mistral |\n",
        "| MQA | $2 d_h l$ | Weak | PaLM |\n",
        "\n",
        "Where:\n",
        "- $n_h$ = number of heads\n",
        "- $d_h$ = head dimension  \n",
        "- $l$ = number of layers\n",
        "- $n_g$ = number of KV groups in GQA\n",
        "\n",
        "### âš ï¸ The Trade-off Problem\n",
        "\n",
        "```\n",
        "         Performance\n",
        "             â–²\n",
        "             â”‚      â˜… MHA (Best quality)\n",
        "             â”‚\n",
        "             â”‚          â˜… GQA (Good balance)\n",
        "             â”‚\n",
        "             â”‚              â˜… MQA (Memory efficient)\n",
        "             â”‚\n",
        "             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º Memory Efficiency\n",
        "             \n",
        "The question: Can we get MHA quality with MQA-like memory?\n",
        "```\n",
        "\n",
        "### ğŸ’¡ DeepSeek's Insight: Low-Rank Compression\n",
        "\n",
        "**Key Observation**: Keys and Values are often REDUNDANT!\n",
        "\n",
        "Instead of storing full K and V for each head, we can:\n",
        "1. Compress K and V into a **shared latent vector**\n",
        "2. Reconstruct them when needed\n",
        "3. Store only the compressed representation!\n",
        "\n",
        "```\n",
        "Standard MHA:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Input â†’ W_K â†’ K (full dimension) â†’ CACHE K\n",
        "Input â†’ W_V â†’ V (full dimension) â†’ CACHE V\n",
        "\n",
        "Cache size: 2 Ã— n_h Ã— d_h per token per layer\n",
        "\n",
        "MLA (Low-Rank Compression):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Input â†’ W_DKV â†’ c_KV (compressed!) â†’ CACHE c_KV\n",
        "             â†“\n",
        "         W_UK â†’ K (reconstructed on-the-fly)\n",
        "         W_UV â†’ V (reconstructed on-the-fly)\n",
        "\n",
        "Cache size: d_c per token per layer (where d_c << 2 Ã— n_h Ã— d_h)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ§  Check Your Understanding: Before MLA (Advanced)\n",
        "\n",
        "**MLA is the most complex attention variant. Before diving in, verify you understand:**\n",
        "\n",
        "### Quick Quiz\n",
        "\n",
        "1. **What is the KV cache problem in LLMs?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   During autoregressive generation, we cache all previous K and V vectors. For long contexts with many heads, this uses enormous memory (e.g., 20+ GB for 96-head models).\n",
        "   </details>\n",
        "\n",
        "2. **How does GQA reduce memory?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   GQA shares KV heads across groups of Q heads. Instead of 96 KV heads, use 8-12, reducing cache by 8-12Ã—.\n",
        "   </details>\n",
        "\n",
        "3. **What is low-rank compression?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   Projecting data to a smaller dimension (d â†’ d_compressed), then expanding back. This approximates the original with fewer parameters.\n",
        "   </details>\n",
        "\n",
        "4. **Why is RoPE special for position encoding?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   RoPE encodes position by rotating Q/K vectors. The relative position between tokens is preserved in the attention computation.\n",
        "   </details>\n",
        "\n",
        "### âš ï¸ Difficulty Warning\n",
        "MLA is research-level content from DeepSeek-V2 (2024). It's okay if some concepts are challenging - even ML researchers find this advanced!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6.2: MLA Architecture Deep Dive\n",
        "\n",
        "### ğŸ—ï¸ The Full MLA Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                     MULTI-HEAD LATENT ATTENTION (MLA)                        â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                              â”‚\n",
        "â”‚                           Input: h_t âˆˆ R^d                                   â”‚\n",
        "â”‚                                 â”‚                                            â”‚\n",
        "â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚\n",
        "â”‚            â”‚                    â”‚                    â”‚                       â”‚\n",
        "â”‚            â–¼                    â–¼                    â–¼                       â”‚\n",
        "â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚\n",
        "â”‚      â”‚  W_DQ    â”‚        â”‚  W_DKV    â”‚         â”‚  W_KR    â”‚                  â”‚\n",
        "â”‚      â”‚ (down)   â”‚        â”‚  (down)   â”‚         â”‚ (RoPE K) â”‚                  â”‚\n",
        "â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜                  â”‚\n",
        "â”‚           â”‚                    â”‚                    â”‚                        â”‚\n",
        "â”‚           â–¼                    â–¼                    â–¼                        â”‚\n",
        "â”‚      c_Q âˆˆ R^d'_c        c_KV âˆˆ R^d_c         k_R âˆˆ R^d_R                    â”‚\n",
        "â”‚      (compressed Q)      (compressed KV)      (RoPE key)                     â”‚\n",
        "â”‚           â”‚              â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”               â”‚                         â”‚\n",
        "â”‚           â”‚              â”‚         â”‚               â”‚                         â”‚\n",
        "â”‚      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”´â”€â”€â” â”Œâ”€â”€â”€â”€â”´â”€â”€â”           â”‚                         â”‚\n",
        "â”‚      â”‚  W_UQ   â”‚    â”‚ W_UK  â”‚ â”‚ W_UV  â”‚           â”‚                         â”‚\n",
        "â”‚      â”‚  (up)   â”‚    â”‚ (up)  â”‚ â”‚ (up)  â”‚           â”‚                         â”‚\n",
        "â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜           â”‚                         â”‚\n",
        "â”‚           â”‚             â”‚         â”‚               â”‚                         â”‚\n",
        "â”‚           â–¼             â–¼         â–¼               â–¼                         â”‚\n",
        "â”‚       q_C âˆˆ R^(n_hÃ—d_h) k_C       v_C       RoPE(k_R)                       â”‚\n",
        "â”‚           â”‚             â”‚         â”‚               â”‚                         â”‚\n",
        "â”‚      â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”         â”‚         â”‚               â”‚                         â”‚\n",
        "â”‚      â”‚  W_QR  â”‚         â”‚         â”‚               â”‚                         â”‚\n",
        "â”‚      â”‚(RoPE Q)â”‚         â”‚         â”‚               â”‚                         â”‚\n",
        "â”‚      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜         â”‚         â”‚               â”‚                         â”‚\n",
        "â”‚           â–¼             â”‚         â”‚               â”‚                         â”‚\n",
        "â”‚       q_R (RoPE)        â”‚         â”‚               â”‚                         â”‚\n",
        "â”‚           â”‚             â”‚         â”‚               â”‚                         â”‚\n",
        "â”‚           â–¼             â–¼         â”‚               â–¼                         â”‚\n",
        "â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚\n",
        "â”‚      â”‚ q = [q_C ; q_R]     â”‚      â”‚    â”‚ k = [k_C ; k_R]    â”‚               â”‚\n",
        "â”‚      â”‚ (concatenate)       â”‚      â”‚    â”‚ (concatenate)      â”‚               â”‚\n",
        "â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚\n",
        "â”‚                 â”‚                 â”‚              â”‚                          â”‚\n",
        "â”‚                 â–¼                 â–¼              â–¼                          â”‚\n",
        "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚\n",
        "â”‚         â”‚              ATTENTION                     â”‚                      â”‚\n",
        "â”‚         â”‚  attn = softmax(q @ k.T / âˆš(d_h + d_R)) @ vâ”‚                      â”‚\n",
        "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚\n",
        "â”‚                                 â”‚                                            â”‚\n",
        "â”‚                                 â–¼                                            â”‚\n",
        "â”‚                          Output: u_t âˆˆ R^d                                   â”‚\n",
        "â”‚                                                                              â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ CACHED DURING INFERENCE: Only c_KV (d_c elements) + k_R (d_R elements)       â”‚\n",
        "â”‚ Total: (d_c + d_R) elements per token << 2Ã—n_hÃ—d_h in MHA                    â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ“ Dimension Summary\n",
        "\n",
        "| Symbol | Meaning | DeepSeek-V2 Value |\n",
        "|--------|---------|-------------------|\n",
        "| $d$ | Embedding dimension | 5120 |\n",
        "| $n_h$ | Number of heads | 128 |\n",
        "| $d_h$ | Dimension per head | 128 |\n",
        "| $d_c$ | KV compression dimension | 512 (= 4$d_h$) |\n",
        "| $d'_c$ | Query compression dimension | 1536 |\n",
        "| $d^R_h$ | RoPE dimension per head | 64 (= $d_h$/2) |\n",
        "\n",
        "### ğŸ”¢ The Equations\n",
        "\n",
        "**Step 1: Low-Rank KV Compression**\n",
        "\n",
        "$$c^{KV}_t = W^{DKV} h_t$$\n",
        "\n",
        "$$k^C_t = W^{UK} c^{KV}_t$$\n",
        "\n",
        "$$v^C_t = W^{UV} c^{KV}_t$$\n",
        "\n",
        "Where:\n",
        "- $W^{DKV} \\in \\mathbb{R}^{d_c \\times d}$ is the down-projection (compress)\n",
        "- $W^{UK}, W^{UV} \\in \\mathbb{R}^{n_h d_h \\times d_c}$ are up-projections (decompress)\n",
        "\n",
        "**Step 2: Low-Rank Query Compression** (for training efficiency)\n",
        "\n",
        "$$c^Q_t = W^{DQ} h_t$$\n",
        "\n",
        "$$q^C_t = W^{UQ} c^Q_t$$\n",
        "\n",
        "**Step 3: Decoupled RoPE**\n",
        "\n",
        "$$q^R_t = \\text{RoPE}(W^{QR} c^Q_t)$$\n",
        "\n",
        "$$k^R_t = \\text{RoPE}(W^{KR} h_t)$$\n",
        "\n",
        "**Step 4: Concatenate and Attend**\n",
        "\n",
        "$$q_{t,i} = [q^C_{t,i}; q^R_{t,i}]$$\n",
        "\n",
        "$$k_{t,i} = [k^C_{t,i}; k^R_t]$$  (Note: $k^R$ is shared across heads!)\n",
        "\n",
        "$$o_{t,i} = \\sum_{j=1}^{t} \\text{softmax}_j\\left(\\frac{q_{t,i}^T k_{j,i}}{\\sqrt{d_h + d^R_h}}\\right) v^C_{j,i}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6.3: Why Decoupled RoPE?\n",
        "\n",
        "### ğŸ¤” The RoPE Incompatibility Problem\n",
        "\n",
        "Remember: RoPE (Rotary Position Embedding) applies position-dependent rotations to Q and K.\n",
        "\n",
        "**The Problem with Naive Compression + RoPE:**\n",
        "\n",
        "```\n",
        "Standard approach (BROKEN):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "h_t â†’ W_DKV â†’ c_KV â†’ W_UK â†’ k_C â†’ RoPE(k_C)\n",
        "                                      â†‘\n",
        "                              Position-dependent!\n",
        "\n",
        "During inference, we want to cache c_KV and absorb W_UK into W_Q.\n",
        "But RoPE depends on position, so:\n",
        "\n",
        "  Q @ K.T = (W_Q @ h_t) @ (RoPE_j @ W_UK @ c_KV_j).T\n",
        "\n",
        "The RoPE_j matrix is DIFFERENT for each cached position j!\n",
        "We can't pre-compute (W_Q @ W_UK) because RoPE sits in between!\n",
        "\n",
        "  W_Q @ RoPE_j @ W_UK â† Can't factor this out!\n",
        "       â†‘\n",
        "  This changes per position!\n",
        "```\n",
        "\n",
        "### âœ… The Decoupled RoPE Solution\n",
        "\n",
        "**Key Idea**: Separate the position information into dedicated components!\n",
        "\n",
        "```\n",
        "Decoupled RoPE (WORKS):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "                           â”Œâ”€â”€ k_C (content, no position)\n",
        "h_t â†’ W_DKV â†’ c_KV â†’ W_UK â”€â”¤\n",
        "                           â””â†’ CACHED (position-independent!)\n",
        "\n",
        "h_t â†’ W_KR â†’ k_R â†’ RoPE(k_R) â† Position applied HERE\n",
        "                      â”‚\n",
        "                      â””â†’ CACHED (position-dependent, small)\n",
        "\n",
        "Final key: k = [k_C ; k_R]\n",
        "          Content + Position (concatenated, not mixed!)\n",
        "```\n",
        "\n",
        "### ğŸ“Š Why This Works\n",
        "\n",
        "```\n",
        "Content part (k_C):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "â€¢ No position information\n",
        "â€¢ W_UK CAN be absorbed into W_Q during inference\n",
        "â€¢ We only cache c_KV (small!)\n",
        "â€¢ Keys computed on-the-fly: k_C = W_UK @ c_KV\n",
        "\n",
        "Position part (k_R):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "â€¢ Only carries position via RoPE\n",
        "â€¢ Small dimension (d_R << d_h Ã— n_h)\n",
        "â€¢ Cached separately per token\n",
        "â€¢ Shared across all heads!\n",
        "\n",
        "Total cache per token: d_c + d_R â‰ˆ 4.5 Ã— d_h\n",
        "Compare to MHA: 2 Ã— n_h Ã— d_h (for 128 heads, 256Ã— larger!)\n",
        "```\n",
        "\n",
        "### ğŸ¯ The Absorption Trick\n",
        "\n",
        "During inference:\n",
        "\n",
        "$$Q \\cdot K^T = Q \\cdot (W^{UK} \\cdot c^{KV})^T = (Q \\cdot (W^{UK})^T) \\cdot (c^{KV})^T$$\n",
        "\n",
        "Since $W^{UK}$ is fixed, we can pre-compute $\\tilde{W}_Q = W^Q \\cdot (W^{UK})^T$!\n",
        "\n",
        "We never need to materialize the full keys!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6.4: Comparing Attention Mechanisms\n",
        "\n",
        "### ğŸ“Š Visual Comparison\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    ATTENTION MECHANISM COMPARISON                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                           â”‚\n",
        "â”‚  MHA (Multi-Head Attention)                                               â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚\n",
        "â”‚    h â†’ W_Q â†’ Q [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]   Full Q (all heads)                           â”‚\n",
        "â”‚    h â†’ W_K â†’ K [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]   Full K (all heads)  â†’ CACHE ALL              â”‚\n",
        "â”‚    h â†’ W_V â†’ V [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]   Full V (all heads)  â†’ CACHE ALL              â”‚\n",
        "â”‚                                                                           â”‚\n",
        "â”‚  GQA (Grouped Query Attention)                                            â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚\n",
        "â”‚    h â†’ W_Q â†’ Q [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]   Full Q (all heads)                           â”‚\n",
        "â”‚    h â†’ W_K â†’ K [â–ˆâ–ˆ]         Fewer K (groups)    â†’ CACHE (smaller)        â”‚\n",
        "â”‚    h â†’ W_V â†’ V [â–ˆâ–ˆ]         Fewer V (groups)    â†’ CACHE (smaller)        â”‚\n",
        "â”‚                                                                           â”‚\n",
        "â”‚  MQA (Multi-Query Attention)                                              â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                              â”‚\n",
        "â”‚    h â†’ W_Q â†’ Q [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]   Full Q (all heads)                           â”‚\n",
        "â”‚    h â†’ W_K â†’ K [â–ˆ]          Single K (shared)   â†’ CACHE (minimal)        â”‚\n",
        "â”‚    h â†’ W_V â†’ V [â–ˆ]          Single V (shared)   â†’ CACHE (minimal)        â”‚\n",
        "â”‚                                                                           â”‚\n",
        "â”‚  MLA (Multi-head Latent Attention)                                        â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                        â”‚\n",
        "â”‚    h â†’ W_DQ â†’ c_Q â†’ W_UQ â†’ Q [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]  Compressed, then expanded       â”‚\n",
        "â”‚    h â†’ W_DKV â†’ c_KV [â–ˆâ–ˆ]                 Compressed latent â†’ CACHE (tiny)â”‚\n",
        "â”‚             â†“                                                             â”‚\n",
        "â”‚         W_UK â†’ K [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]              Reconstructed on-the-fly        â”‚\n",
        "â”‚         W_UV â†’ V [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ]              Reconstructed on-the-fly        â”‚\n",
        "â”‚    h â†’ W_KR â†’ k_R [â–ª]                    RoPE key â†’ CACHE (tiny)         â”‚\n",
        "â”‚                                                                           â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ“ˆ KV Cache Comparison Table\n",
        "\n",
        "| Mechanism | KV Cache per Token | For 128 heads, d_h=128 | Relative |\n",
        "|-----------|-------------------|------------------------|----------|\n",
        "| MHA | $2 n_h d_h$ | 32,768 elements | 1.00Ã— |\n",
        "| GQA (8 groups) | $2 n_g d_h$ | 2,048 elements | 0.06Ã— |\n",
        "| MQA | $2 d_h$ | 256 elements | 0.008Ã— |\n",
        "| **MLA** | $d_c + d^R_h$ | 576 elements* | **0.018Ã—** |\n",
        "\n",
        "*DeepSeek-V2: $d_c = 512$, $d^R_h = 64$\n",
        "\n",
        "### ğŸ† Why MLA Wins\n",
        "\n",
        "```\n",
        "                    QUALITY\n",
        "                       â–²\n",
        "                       â”‚\n",
        "              MLA â˜…â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â˜… MHA\n",
        "                       â”‚     /\n",
        "                       â”‚    /\n",
        "                       â”‚   /\n",
        "               GQA â˜…â”€â”€â”¼â”€â”€/\n",
        "                       â”‚ /\n",
        "                       â”‚/\n",
        "               MQA â˜…â”€â”€â”¼\n",
        "                       â”‚\n",
        "                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º MEMORY EFFICIENCY\n",
        "\n",
        "MLA achieves:\n",
        "  âœ… BETTER quality than MHA (more expressive through compression)\n",
        "  âœ… LOWER memory than GQA-8 (2.25 groups equivalent)\n",
        "  âœ… COMPARABLE speed to MQA\n",
        "  \n",
        "The secret: Low-rank compression is a form of regularization!\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6.5: Implementing MLA from Scratch\n",
        "\n",
        "Let's implement a simplified version of Multi-head Latent Attention to understand the core concepts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ› ï¸ BUILD IT YOURSELF: Multi-head Latent Attention Step-by-Step\n",
        "\n",
        "MLA is the most advanced mechanism we'll cover. Let's build it piece by piece!\n",
        "\n",
        "### ğŸ“‹ What You'll Implement\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    YOUR MLA JOURNEY                                     â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Exercise 6.A: Understand the compression math                        â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 6.B: Implement KV compression (down-projection)             â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 6.C: Reconstruct K and V from compressed latent             â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 6.D: Add decoupled RoPE for position info                   â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 6.E: Complete MLA with all components                       â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "**Note:** MLA is complex! Take your time with each step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6.A: Understand MLA Compression Math\n",
        "\n",
        "**The Core Insight:** Instead of caching full K and V, we cache a COMPRESSED representation!\n",
        "\n",
        "```\n",
        "Traditional MHA:\n",
        "  Cache per token = 2 Ã— num_heads Ã— head_dim\n",
        "  \n",
        "  For 128 heads, head_dim=128:\n",
        "  Cache = 2 Ã— 128 Ã— 128 = 32,768 elements! ğŸ’¥\n",
        "\n",
        "MLA with compression:\n",
        "  Cache per token = kv_compression_dim + rope_dim\n",
        "  \n",
        "  For d_c=512, d_R=64:\n",
        "  Cache = 512 + 64 = 576 elements! ğŸ‰\n",
        "  \n",
        "  Reduction: 32,768 / 576 â‰ˆ 57Ã— smaller!\n",
        "```\n",
        "\n",
        "**Your Task:**\n",
        "1. Calculate cache sizes for different configurations\n",
        "2. Understand the compression ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 6.A: Understand MLA Compression Math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Configuration (DeepSeek-V2 inspired)\n",
        "embed_dim = 512         # Model dimension\n",
        "num_heads = 8           # Number of attention heads\n",
        "head_dim = embed_dim // num_heads  # = 64\n",
        "\n",
        "# MLA compression dimensions\n",
        "kv_compression_dim = 64   # d_c: compressed KV dimension (much smaller than embed_dim!)\n",
        "rope_dim = 32             # d_R: RoPE dimension\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "\n",
        "# TODO: Calculate MHA cache size (what we'd cache without compression)\n",
        "# Formula: 2 Ã— num_heads Ã— head_dim (for K and V)\n",
        "\n",
        "mha_cache = # YOUR CODE\n",
        "\n",
        "# TODO: Calculate MLA cache size\n",
        "# Formula: kv_compression_dim + rope_dim\n",
        "\n",
        "mla_cache = # YOUR CODE\n",
        "\n",
        "# TODO: Calculate compression ratio\n",
        "\n",
        "compression_ratio = # YOUR CODE: mha_cache / mla_cache\n",
        "\n",
        "# =========================================\n",
        "\n",
        "print(\"Cache Size Comparison (per token per layer):\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Configuration:\")\n",
        "print(f\"  embed_dim = {embed_dim}\")\n",
        "print(f\"  num_heads = {num_heads}\")\n",
        "print(f\"  head_dim = {head_dim}\")\n",
        "print(f\"  kv_compression_dim = {kv_compression_dim}\")\n",
        "print(f\"  rope_dim = {rope_dim}\")\n",
        "print()\n",
        "print(f\"MHA cache: 2 Ã— {num_heads} Ã— {head_dim} = {mha_cache} elements\")\n",
        "print(f\"MLA cache: {kv_compression_dim} + {rope_dim} = {mla_cache} elements\")\n",
        "print(f\"\\nâœ… Compression ratio: {compression_ratio:.1f}Ã— smaller!\")\n",
        "print(f\"âœ… Memory savings: {(1 - mla_cache/mha_cache)*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6.B: Implement KV Compression (Down-Projection)\n",
        "\n",
        "**The First Step:** Project the input to a compressed KV latent.\n",
        "\n",
        "```\n",
        "Standard path:      h â†’ W_K â†’ K (embed_dim)\n",
        "                    h â†’ W_V â†’ V (embed_dim)\n",
        "                    \n",
        "MLA path:           h â†’ W_DKV â†’ c_KV (kv_compression_dim << embed_dim)\n",
        "                              â†“\n",
        "                         [CACHED - small!]\n",
        "```\n",
        "\n",
        "**Key Insight:** `W_DKV` is a \"down-projection\" that compresses the input.\n",
        "\n",
        "**Your Task:**\n",
        "1. Create a down-projection layer `W_DKV`\n",
        "2. Project input to compressed latent `c_KV`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 6.B: Implement KV Compression (Down-Projection)\n",
        "\n",
        "# Configuration\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "head_dim = embed_dim // num_heads\n",
        "kv_compression_dim = 64  # Much smaller than embed_dim!\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "# Input\n",
        "h = torch.randn(batch_size, seq_len, embed_dim)\n",
        "print(f\"Input h shape: {h.shape}\")\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "\n",
        "# TODO: Create the down-projection layer W_DKV\n",
        "# This projects from embed_dim to kv_compression_dim\n",
        "\n",
        "W_DKV = # YOUR CODE: nn.Linear(embed_dim, kv_compression_dim, bias=False)\n",
        "\n",
        "# TODO: Project input to compressed latent\n",
        "# c_KV is what we'll cache during inference!\n",
        "\n",
        "c_KV = # YOUR CODE: W_DKV(h)\n",
        "\n",
        "# =========================================\n",
        "\n",
        "print(f\"\\nDown-projection W_DKV: {embed_dim} â†’ {kv_compression_dim}\")\n",
        "print(f\"Compressed c_KV shape: {c_KV.shape}\")\n",
        "\n",
        "# Compare sizes\n",
        "original_size = batch_size * seq_len * embed_dim\n",
        "compressed_size = batch_size * seq_len * kv_compression_dim\n",
        "print(f\"\\nSize comparison:\")\n",
        "print(f\"  Original (h): {original_size} elements\")\n",
        "print(f\"  Compressed (c_KV): {compressed_size} elements\")\n",
        "print(f\"\\nâœ… Compression: {original_size / compressed_size:.1f}Ã—\")\n",
        "print(\"âœ… c_KV is what we cache during inference!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6.C: Reconstruct K and V from Compressed Latent\n",
        "\n",
        "**The \"Up-Projection\" Step:** Reconstruct full K and V from the compressed `c_KV`.\n",
        "\n",
        "```\n",
        "During attention computation:\n",
        "  c_KV â†’ W_UK â†’ K (full dimension, all heads)\n",
        "  c_KV â†’ W_UV â†’ V (full dimension, all heads)\n",
        "```\n",
        "\n",
        "**Key Insight:** We DON'T cache K and V! We:\n",
        "1. Cache the small `c_KV`\n",
        "2. Reconstruct K and V on-the-fly when computing attention\n",
        "\n",
        "**Your Task:**\n",
        "1. Create up-projection layers `W_UK` and `W_UV`\n",
        "2. Reconstruct K and V from the compressed latent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 6.C: Reconstruct K and V from Compressed Latent\n",
        "\n",
        "# Configuration (same as before)\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "head_dim = embed_dim // num_heads\n",
        "kv_compression_dim = 64\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "# Simulated compressed latent (from Exercise 6.B)\n",
        "c_KV = torch.randn(batch_size, seq_len, kv_compression_dim)\n",
        "print(f\"Compressed c_KV shape: {c_KV.shape}\")\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "\n",
        "# TODO: Create up-projection layer for Keys\n",
        "# Projects from kv_compression_dim to embed_dim (full dimension)\n",
        "\n",
        "W_UK = # YOUR CODE: nn.Linear(kv_compression_dim, embed_dim, bias=False)\n",
        "\n",
        "# TODO: Create up-projection layer for Values\n",
        "\n",
        "W_UV = # YOUR CODE: nn.Linear(?, ?, bias=False)\n",
        "\n",
        "# TODO: Reconstruct K from compressed latent\n",
        "\n",
        "K = # YOUR CODE: W_UK(c_KV)\n",
        "\n",
        "# TODO: Reconstruct V from compressed latent\n",
        "\n",
        "V = # YOUR CODE\n",
        "\n",
        "print(f\"\\nUp-projection W_UK: {kv_compression_dim} â†’ {embed_dim}\")\n",
        "print(f\"Reconstructed K shape: {K.shape}\")\n",
        "print(f\"Reconstructed V shape: {V.shape}\")\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Reshape into heads format\n",
        "K_heads = K.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "V_heads = V.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "\n",
        "print(f\"\\nK reshaped to heads: {K_heads.shape}\")\n",
        "print(f\"V reshaped to heads: {V_heads.shape}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… K and V reconstructed from compressed latent!\")\n",
        "print(\"âœ… We only cached c_KV, not full K and V!\")\n",
        "print(f\"âœ… Cache: {kv_compression_dim} elements vs {2*embed_dim} for full K+V\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6.D: Add Decoupled RoPE for Position Information\n",
        "\n",
        "**The Position Problem:** Compressed K doesn't have position information!\n",
        "\n",
        "**Solution: Decoupled RoPE**\n",
        "- Create a SEPARATE small key `k_R` just for position\n",
        "- Apply RoPE only to this position key\n",
        "- Concatenate content and position: `k = [k_C ; k_R]`\n",
        "\n",
        "```\n",
        "Content key (no position):\n",
        "  c_KV â†’ W_UK â†’ k_C (embed_dim)\n",
        "\n",
        "Position key (with RoPE):\n",
        "  h â†’ W_KR â†’ k_R â†’ RoPE(k_R)  (rope_dim, much smaller!)\n",
        "\n",
        "Final key:\n",
        "  k = concatenate([k_C, k_R])  (embed_dim + rope_dim)\n",
        "```\n",
        "\n",
        "**Your Task:**\n",
        "1. Create `W_KR` for the RoPE key\n",
        "2. Concatenate content and position keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 6.D: Add Decoupled RoPE for Position Information\n",
        "\n",
        "# Configuration\n",
        "embed_dim = 512\n",
        "num_heads = 8\n",
        "head_dim = embed_dim // num_heads\n",
        "kv_compression_dim = 64\n",
        "rope_dim = 32  # Small dimension just for position!\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "# Input\n",
        "h = torch.randn(batch_size, seq_len, embed_dim)\n",
        "\n",
        "# From previous exercises\n",
        "W_DKV = nn.Linear(embed_dim, kv_compression_dim, bias=False)\n",
        "W_UK = nn.Linear(kv_compression_dim, embed_dim, bias=False)\n",
        "\n",
        "c_KV = W_DKV(h)\n",
        "k_C = W_UK(c_KV)  # Content key (no position info yet)\n",
        "\n",
        "print(f\"Content key k_C shape: {k_C.shape}\")\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "\n",
        "# TODO: Create projection for RoPE key\n",
        "# This projects from embed_dim to rope_dim (small!)\n",
        "\n",
        "W_KR = # YOUR CODE: nn.Linear(embed_dim, rope_dim, bias=False)\n",
        "\n",
        "# TODO: Create the RoPE key\n",
        "\n",
        "k_R_raw = # YOUR CODE: W_KR(h)\n",
        "\n",
        "print(f\"RoPE key (raw) shape: {k_R_raw.shape}\")\n",
        "\n",
        "# Simplified RoPE (in practice, use proper rotary embeddings)\n",
        "def simple_rope(x: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Simplified positional encoding for demonstration.\"\"\"\n",
        "    seq_len = x.shape[1]\n",
        "    dim = x.shape[-1]\n",
        "    positions = torch.arange(seq_len, device=x.device).unsqueeze(-1)\n",
        "    freqs = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=x.device).float() / dim))\n",
        "    angles = positions * freqs\n",
        "    \n",
        "    x_out = x.clone()\n",
        "    x_out[..., ::2] = x[..., ::2] * torch.cos(angles) - x[..., 1::2] * torch.sin(angles)\n",
        "    x_out[..., 1::2] = x[..., ::2] * torch.sin(angles) + x[..., 1::2] * torch.cos(angles)\n",
        "    return x_out\n",
        "\n",
        "# TODO: Apply RoPE to the position key\n",
        "\n",
        "k_R = # YOUR CODE: simple_rope(k_R_raw)\n",
        "\n",
        "print(f\"RoPE key (after RoPE) shape: {k_R.shape}\")\n",
        "\n",
        "# TODO: Now we need to combine content and position\n",
        "# But first, reshape k_C to heads format\n",
        "k_C_heads = k_C.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)\n",
        "print(f\"\\nContent key heads shape: {k_C_heads.shape}\")\n",
        "\n",
        "# k_R is shared across heads! Expand it.\n",
        "k_R_expanded = k_R.unsqueeze(1).expand(-1, num_heads, -1, -1)\n",
        "print(f\"Position key (expanded) shape: {k_R_expanded.shape}\")\n",
        "\n",
        "# TODO: Concatenate content and position keys along the last dimension\n",
        "\n",
        "K_full = # YOUR CODE: torch.cat([k_C_heads, k_R_expanded], dim=-1)\n",
        "\n",
        "print(f\"\\nFinal key shape (content + position): {K_full.shape}\")\n",
        "\n",
        "# =========================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Decoupled RoPE implemented!\")\n",
        "print(f\"âœ… Content dimension: {head_dim}\")\n",
        "print(f\"âœ… Position dimension: {rope_dim}\")\n",
        "print(f\"âœ… Total key dimension: {head_dim + rope_dim}\")\n",
        "print(\"âœ… Position info is separate from content!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 6.E: Complete Simplified MLA Module\n",
        "\n",
        "**Now put it all together!**\n",
        "\n",
        "You've implemented:\n",
        "- âœ… KV compression (down-projection to c_KV)\n",
        "- âœ… K and V reconstruction (up-projection from c_KV)\n",
        "- âœ… Decoupled RoPE (separate position key)\n",
        "\n",
        "**Simplified MLA Architecture:**\n",
        "```\n",
        "Query path:   h â†’ W_Q â†’ Q (we'll skip Q compression for simplicity)\n",
        "Key path:     h â†’ W_DKV â†’ c_KV â†’ W_UK â†’ k_C\n",
        "              h â†’ W_KR â†’ k_R â†’ RoPE(k_R)\n",
        "              k = [k_C ; k_R]\n",
        "Value path:   c_KV â†’ W_UV â†’ V\n",
        "```\n",
        "\n",
        "**Your Task:** Build a simplified MLA module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 6.E: Complete Simplified MLA Module\n",
        "\n",
        "import math\n",
        "\n",
        "class MySimplifiedMLA(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Multi-head Latent Attention.\n",
        "    \n",
        "    Key components:\n",
        "    1. KV compression via W_DKV\n",
        "    2. K,V reconstruction via W_UK, W_UV\n",
        "    3. Decoupled RoPE via W_KR\n",
        "    \n",
        "    Note: This is simplified - we skip Q compression for clarity.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(\n",
        "        self, \n",
        "        embed_dim: int, \n",
        "        num_heads: int, \n",
        "        kv_compression_dim: int,\n",
        "        rope_dim: int\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        # Store configuration\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.kv_compression_dim = kv_compression_dim\n",
        "        self.rope_dim = rope_dim\n",
        "        \n",
        "        # Effective dimension for attention (content + position)\n",
        "        self.effective_head_dim = self.head_dim + rope_dim\n",
        "        self.scale = self.effective_head_dim ** -0.5\n",
        "        \n",
        "        # Query projection (simplified - no compression)\n",
        "        self.W_Q = # YOUR CODE: nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        # Q also needs RoPE part\n",
        "        self.W_QR = nn.Linear(embed_dim, num_heads * rope_dim, bias=False)\n",
        "        \n",
        "        # KV compression (down-projection)\n",
        "        self.W_DKV = # YOUR CODE: nn.Linear(embed_dim, kv_compression_dim, bias=False)\n",
        "        \n",
        "        # K,V reconstruction (up-projection)\n",
        "        self.W_UK = # YOUR CODE\n",
        "        self.W_UV = # YOUR CODE\n",
        "        \n",
        "        # RoPE key projection\n",
        "        self.W_KR = # YOUR CODE: nn.Linear(embed_dim, rope_dim, bias=False)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_O = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        \n",
        "        # =========================================\n",
        "    \n",
        "    def _apply_rope(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Simplified RoPE.\"\"\"\n",
        "        seq_len = x.shape[-2]\n",
        "        dim = x.shape[-1]\n",
        "        positions = torch.arange(seq_len, device=x.device).unsqueeze(-1)\n",
        "        freqs = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=x.device).float() / dim))\n",
        "        angles = positions * freqs\n",
        "        \n",
        "        x_out = x.clone()\n",
        "        cos_angles = torch.cos(angles)\n",
        "        sin_angles = torch.sin(angles)\n",
        "        \n",
        "        if x.dim() == 4:  # (batch, heads, seq, dim)\n",
        "            cos_angles = cos_angles.unsqueeze(0).unsqueeze(0)\n",
        "            sin_angles = sin_angles.unsqueeze(0).unsqueeze(0)\n",
        "        else:\n",
        "            cos_angles = cos_angles.unsqueeze(0)\n",
        "            sin_angles = sin_angles.unsqueeze(0)\n",
        "        \n",
        "        x_out[..., ::2] = x[..., ::2] * cos_angles - x[..., 1::2] * sin_angles\n",
        "        x_out[..., 1::2] = x[..., ::2] * sin_angles + x[..., 1::2] * cos_angles\n",
        "        return x_out\n",
        "        \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        # === Query path ===\n",
        "        q_C = self.W_Q(x)  # Content query\n",
        "        q_C = q_C.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        q_R = self.W_QR(x)  # RoPE query\n",
        "        q_R = q_R.view(batch_size, seq_len, self.num_heads, self.rope_dim).transpose(1, 2)\n",
        "        q_R = self._apply_rope(q_R)\n",
        "        \n",
        "        Q = torch.cat([q_C, q_R], dim=-1)  # Combine content + position\n",
        "        \n",
        "        # === KV path ===\n",
        "        # Step 1: Compress to c_KV\n",
        "        c_KV = # YOUR CODE: self.W_DKV(x)\n",
        "        \n",
        "        # Step 2: Reconstruct content key\n",
        "        k_C = # YOUR CODE: self.W_UK(c_KV)\n",
        "        k_C = k_C.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        # Step 3: Create RoPE key (shared across heads)\n",
        "        k_R = # YOUR CODE: self.W_KR(x)\n",
        "        k_R = self._apply_rope(k_R)\n",
        "        k_R = k_R.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
        "        \n",
        "        # Step 4: Combine content + position keys\n",
        "        K = # YOUR CODE: torch.cat([k_C, k_R], dim=-1)\n",
        "        \n",
        "        # Step 5: Reconstruct values\n",
        "        V = # YOUR CODE: self.W_UV(c_KV)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        # === Attention ===\n",
        "        scores = (Q @ K.transpose(-2, -1)) * self.scale\n",
        "        attn_weights = F.softmax(scores, dim=-1)\n",
        "        \n",
        "        out = attn_weights @ V  # (batch, heads, seq, head_dim)\n",
        "        \n",
        "        # === Output ===\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        out = out.view(batch_size, seq_len, self.embed_dim)\n",
        "        out = self.W_O(out)\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "        return out\n",
        "\n",
        "\n",
        "# Test your implementation!\n",
        "print(\"Testing MySimplifiedMLA...\")\n",
        "\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "kv_compression_dim = 32\n",
        "rope_dim = 16\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "mla = MySimplifiedMLA(embed_dim, num_heads, kv_compression_dim, rope_dim)\n",
        "\n",
        "x = torch.randn(batch_size, seq_len, embed_dim)\n",
        "output = mla(x)\n",
        "\n",
        "print(f\"Input shape:  {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "assert output.shape == (batch_size, seq_len, embed_dim)\n",
        "\n",
        "# Cache comparison\n",
        "head_dim = embed_dim // num_heads\n",
        "mha_cache = 2 * num_heads * head_dim\n",
        "mla_cache = kv_compression_dim + rope_dim\n",
        "\n",
        "print(f\"\\nğŸ“Š Cache comparison (per token):\")\n",
        "print(f\"   MHA cache: {mha_cache} elements\")\n",
        "print(f\"   MLA cache: {mla_cache} elements\")\n",
        "print(f\"   Reduction: {mha_cache / mla_cache:.1f}Ã—!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Congratulations! You've built Multi-head Latent Attention!\")\n",
        "print(\"âœ… Compare your code with the full reference implementation below\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Multi-head Latent Attention (MLA) Implementation\n",
        "\n",
        "class MultiHeadLatentAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head Latent Attention (MLA) from DeepSeek-V2.\n",
        "    \n",
        "    Key innovations:\n",
        "    1. Low-rank KV compression: Store compressed latent instead of full K,V\n",
        "    2. Low-rank Q compression: Reduces training memory\n",
        "    3. Decoupled RoPE: Separate content and position information\n",
        "    \n",
        "    This is a simplified educational implementation.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        kv_compression_dim: int,      # d_c: KV latent dimension\n",
        "        q_compression_dim: int,       # d'_c: Q latent dimension  \n",
        "        rope_dim: int,                # d_R: RoPE dimension per head\n",
        "        dropout: float = 0.0\n",
        "    ):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.kv_compression_dim = kv_compression_dim\n",
        "        self.q_compression_dim = q_compression_dim\n",
        "        self.rope_dim = rope_dim\n",
        "        \n",
        "        # Effective head dimension (content + RoPE)\n",
        "        self.effective_head_dim = self.head_dim + rope_dim\n",
        "        self.scale = self.effective_head_dim ** -0.5\n",
        "        \n",
        "        # ===== Query path (with compression) =====\n",
        "        # h -> c_Q (down-project)\n",
        "        self.W_DQ = nn.Linear(embed_dim, q_compression_dim, bias=False)\n",
        "        # c_Q -> q_C (up-project to all heads)\n",
        "        self.W_UQ = nn.Linear(q_compression_dim, embed_dim, bias=False)\n",
        "        # c_Q -> q_R (RoPE queries for all heads)\n",
        "        self.W_QR = nn.Linear(q_compression_dim, num_heads * rope_dim, bias=False)\n",
        "        \n",
        "        # ===== Key-Value path (with compression) =====\n",
        "        # h -> c_KV (down-project, shared for K and V)\n",
        "        self.W_DKV = nn.Linear(embed_dim, kv_compression_dim, bias=False)\n",
        "        # c_KV -> k_C (up-project to all heads)\n",
        "        self.W_UK = nn.Linear(kv_compression_dim, embed_dim, bias=False)\n",
        "        # c_KV -> v_C (up-project to all heads)\n",
        "        self.W_UV = nn.Linear(kv_compression_dim, embed_dim, bias=False)\n",
        "        # h -> k_R (RoPE key, shared across heads!)\n",
        "        self.W_KR = nn.Linear(embed_dim, rope_dim, bias=False)\n",
        "        \n",
        "        # ===== Output projection =====\n",
        "        self.W_O = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "        print(f'\\nğŸš€ Created MLA:')\n",
        "        print(f'   embed_dim: {embed_dim}')\n",
        "        print(f'   num_heads: {num_heads}')\n",
        "        print(f'   head_dim: {self.head_dim}')\n",
        "        print(f'   kv_compression_dim (d_c): {kv_compression_dim}')\n",
        "        print(f'   q_compression_dim (d\\'_c): {q_compression_dim}')\n",
        "        print(f'   rope_dim (d_R): {rope_dim}')\n",
        "        print(f'   effective_head_dim: {self.effective_head_dim}')\n",
        "        \n",
        "        # Compare cache sizes\n",
        "        mha_cache = 2 * num_heads * self.head_dim\n",
        "        mla_cache = kv_compression_dim + rope_dim\n",
        "        print(f'\\n   ğŸ“Š Cache comparison per token:')\n",
        "        print(f'      MHA cache: {mha_cache} elements')\n",
        "        print(f'      MLA cache: {mla_cache} elements')\n",
        "        print(f'      Reduction: {mha_cache / mla_cache:.1f}Ã—!')\n",
        "    \n",
        "    def _apply_rope(self, x: torch.Tensor, positions: torch.Tensor = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Apply Rotary Position Embedding (simplified version).\n",
        "        \n",
        "        Args:\n",
        "            x: (batch, seq_len, dim) or (batch, heads, seq_len, dim)\n",
        "            positions: (seq_len,) position indices\n",
        "            \n",
        "        Returns:\n",
        "            x with RoPE applied\n",
        "        \"\"\"\n",
        "        if positions is None:\n",
        "            seq_len = x.shape[-2] if x.dim() == 4 else x.shape[1]\n",
        "            positions = torch.arange(seq_len, device=x.device)\n",
        "        \n",
        "        # Simplified RoPE: sinusoidal position encoding style\n",
        "        # Full RoPE would use rotation matrices\n",
        "        dim = x.shape[-1]\n",
        "        \n",
        "        # Create frequency bands\n",
        "        freqs = 1.0 / (10000 ** (torch.arange(0, dim, 2, device=x.device).float() / dim))\n",
        "        \n",
        "        # Position * frequency\n",
        "        angles = positions.unsqueeze(-1) * freqs.unsqueeze(0)\n",
        "        \n",
        "        # Sin and cos\n",
        "        sin = torch.sin(angles)\n",
        "        cos = torch.cos(angles)\n",
        "        \n",
        "        # Apply rotation (simplified)\n",
        "        if x.dim() == 4:  # (batch, heads, seq, dim)\n",
        "            sin = sin.unsqueeze(0).unsqueeze(0)\n",
        "            cos = cos.unsqueeze(0).unsqueeze(0)\n",
        "        else:  # (batch, seq, dim)\n",
        "            sin = sin.unsqueeze(0)\n",
        "            cos = cos.unsqueeze(0)\n",
        "        \n",
        "        # Split into pairs and rotate\n",
        "        x1 = x[..., ::2]\n",
        "        x2 = x[..., 1::2]\n",
        "        \n",
        "        # Rotation: [x1, x2] -> [x1*cos - x2*sin, x1*sin + x2*cos]\n",
        "        x_rot = torch.zeros_like(x)\n",
        "        x_rot[..., ::2] = x1 * cos - x2 * sin\n",
        "        x_rot[..., 1::2] = x1 * sin + x2 * cos\n",
        "        \n",
        "        return x_rot\n",
        "    \n",
        "    def forward(\n",
        "        self, \n",
        "        x: torch.Tensor,\n",
        "        mask: torch.Tensor = None,\n",
        "        return_cache: bool = False\n",
        "    ) -> tuple:\n",
        "        \"\"\"\n",
        "        Forward pass of MLA.\n",
        "        \n",
        "        Args:\n",
        "            x: (batch, seq_len, embed_dim) input tensor\n",
        "            mask: (seq_len, seq_len) attention mask\n",
        "            return_cache: Whether to return cached values for inference\n",
        "            \n",
        "        Returns:\n",
        "            output: (batch, seq_len, embed_dim)\n",
        "            attn_weights: (batch, num_heads, seq_len, seq_len)\n",
        "            cache: (c_KV, k_R) if return_cache else None\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # ===== Query path =====\n",
        "        # Step 1: Compress query\n",
        "        c_Q = self.W_DQ(x)  # (batch, seq, q_compression_dim)\n",
        "        \n",
        "        # Step 2: Expand to content queries\n",
        "        q_C = self.W_UQ(c_Q)  # (batch, seq, embed_dim)\n",
        "        q_C = q_C.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        q_C = q_C.transpose(1, 2)  # (batch, heads, seq, head_dim)\n",
        "        \n",
        "        # Step 3: Create RoPE queries\n",
        "        q_R = self.W_QR(c_Q)  # (batch, seq, num_heads * rope_dim)\n",
        "        q_R = q_R.view(batch_size, seq_len, self.num_heads, self.rope_dim)\n",
        "        q_R = q_R.transpose(1, 2)  # (batch, heads, seq, rope_dim)\n",
        "        q_R = self._apply_rope(q_R)  # Apply RoPE\n",
        "        \n",
        "        # Step 4: Concatenate content and RoPE queries\n",
        "        Q = torch.cat([q_C, q_R], dim=-1)  # (batch, heads, seq, head_dim + rope_dim)\n",
        "        \n",
        "        # ===== Key-Value path =====\n",
        "        # Step 1: Compress KV (this is what we cache!)\n",
        "        c_KV = self.W_DKV(x)  # (batch, seq, kv_compression_dim)\n",
        "        \n",
        "        # Step 2: Expand to content keys\n",
        "        k_C = self.W_UK(c_KV)  # (batch, seq, embed_dim)\n",
        "        k_C = k_C.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k_C = k_C.transpose(1, 2)  # (batch, heads, seq, head_dim)\n",
        "        \n",
        "        # Step 3: Create RoPE key (shared across heads!)\n",
        "        k_R = self.W_KR(x)  # (batch, seq, rope_dim)\n",
        "        k_R = self._apply_rope(k_R)  # Apply RoPE\n",
        "        \n",
        "        # Expand k_R to all heads (it's shared, so we broadcast)\n",
        "        k_R = k_R.unsqueeze(1).expand(-1, self.num_heads, -1, -1)\n",
        "        # (batch, heads, seq, rope_dim)\n",
        "        \n",
        "        # Step 4: Concatenate content and RoPE keys\n",
        "        K = torch.cat([k_C, k_R], dim=-1)  # (batch, heads, seq, head_dim + rope_dim)\n",
        "        \n",
        "        # Step 5: Create values (no RoPE, just from c_KV)\n",
        "        V = self.W_UV(c_KV)  # (batch, seq, embed_dim)\n",
        "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        V = V.transpose(1, 2)  # (batch, heads, seq, head_dim)\n",
        "        \n",
        "        # ===== Attention computation =====\n",
        "        # Q @ K^T / sqrt(d)\n",
        "        scores = (Q @ K.transpose(-2, -1)) * self.scale\n",
        "        \n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        \n",
        "        attn_weights = torch.softmax(scores, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "        \n",
        "        # Attention @ V\n",
        "        out = attn_weights @ V  # (batch, heads, seq, head_dim)\n",
        "        \n",
        "        # ===== Output projection =====\n",
        "        out = out.transpose(1, 2).contiguous()  # (batch, seq, heads, head_dim)\n",
        "        out = out.view(batch_size, seq_len, self.embed_dim)\n",
        "        out = self.W_O(out)\n",
        "        \n",
        "        # Prepare cache if requested\n",
        "        cache = None\n",
        "        if return_cache:\n",
        "            # Only cache the compressed KV and RoPE key\n",
        "            cache = (c_KV, self.W_KR(x))  # Much smaller than full K, V!\n",
        "        \n",
        "        return out, attn_weights, cache\n",
        "\n",
        "\n",
        "# Test the implementation\n",
        "print('='*70)\n",
        "print('MULTI-HEAD LATENT ATTENTION (MLA) DEMONSTRATION')\n",
        "print('='*70)\n",
        "\n",
        "# Create MLA with DeepSeek-like configuration (scaled down)\n",
        "mla = MultiHeadLatentAttention(\n",
        "    embed_dim=64,           # Small for demo\n",
        "    num_heads=4,            # 4 heads\n",
        "    kv_compression_dim=32,  # d_c = 0.5 * embed_dim (4 * head_dim in paper)\n",
        "    q_compression_dim=48,   # d'_c = 0.75 * embed_dim\n",
        "    rope_dim=8,             # d_R = 0.5 * head_dim\n",
        "    dropout=0.0\n",
        ")\n",
        "\n",
        "# Test forward pass\n",
        "x_mla = torch.randn(2, 8, 64)  # (batch=2, seq=8, embed=64)\n",
        "output_mla, attn_mla, cache_mla = mla(x_mla, return_cache=True)\n",
        "\n",
        "print(f'\\nğŸ“¥ Input shape: {x_mla.shape}')\n",
        "print(f'ğŸ“¤ Output shape: {output_mla.shape}')\n",
        "print(f'ğŸ‘€ Attention shape: {attn_mla.shape}')\n",
        "\n",
        "print(f'\\nğŸ“¦ Cache contents:')\n",
        "print(f'   c_KV shape: {cache_mla[0].shape}')\n",
        "print(f'   k_R shape: {cache_mla[1].shape}')\n",
        "print(f'   Total cache: {cache_mla[0].numel() + cache_mla[1].numel()} elements per batch')\n",
        "\n",
        "# Compare with equivalent MHA\n",
        "mha_cache_size = 2 * 4 * 16 * 8  # 2 (K+V) * heads * head_dim * seq_len\n",
        "mla_cache_size = 32 * 8 + 8 * 8  # kv_compression * seq + rope_dim * seq\n",
        "print(f'\\nğŸ“Š Cache comparison for seq_len=8:')\n",
        "print(f'   MHA would cache: {mha_cache_size} elements')\n",
        "print(f'   MLA caches: {mla_cache_size} elements')\n",
        "print(f'   Reduction: {mha_cache_size / mla_cache_size:.1f}Ã—!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6.6: Understanding MLA's Memory Savings\n",
        "\n",
        "Let's visualize and quantify exactly how MLA saves memory compared to other attention mechanisms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizing MLA Memory Savings\n",
        "\n",
        "def calculate_kv_cache_size(mechanism, num_heads, head_dim, num_kv_groups=None, \n",
        "                             kv_compression_dim=None, rope_dim=None):\n",
        "    \"\"\"\n",
        "    Calculate KV cache size per token per layer for different mechanisms.\n",
        "    \"\"\"\n",
        "    if mechanism == 'MHA':\n",
        "        return 2 * num_heads * head_dim\n",
        "    elif mechanism == 'GQA':\n",
        "        return 2 * num_kv_groups * head_dim\n",
        "    elif mechanism == 'MQA':\n",
        "        return 2 * head_dim\n",
        "    elif mechanism == 'MLA':\n",
        "        return kv_compression_dim + rope_dim\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown mechanism: {mechanism}\")\n",
        "\n",
        "# DeepSeek-V2 configuration\n",
        "num_heads = 128\n",
        "head_dim = 128\n",
        "num_layers = 60\n",
        "kv_compression_dim = 512  # d_c = 4 * d_h\n",
        "rope_dim = 64             # d_R = d_h / 2\n",
        "\n",
        "# Calculate cache sizes\n",
        "configs = {\n",
        "    'MHA': calculate_kv_cache_size('MHA', num_heads, head_dim),\n",
        "    'GQA-8': calculate_kv_cache_size('GQA', num_heads, head_dim, num_kv_groups=8),\n",
        "    'GQA-4': calculate_kv_cache_size('GQA', num_heads, head_dim, num_kv_groups=4),\n",
        "    'MQA': calculate_kv_cache_size('MQA', num_heads, head_dim),\n",
        "    'MLA': calculate_kv_cache_size('MLA', num_heads, head_dim, \n",
        "                                    kv_compression_dim=kv_compression_dim, \n",
        "                                    rope_dim=rope_dim),\n",
        "}\n",
        "\n",
        "print('='*70)\n",
        "print('KV CACHE SIZE COMPARISON (DeepSeek-V2 Config)')\n",
        "print('='*70)\n",
        "print(f'\\nConfiguration:')\n",
        "print(f'  num_heads: {num_heads}')\n",
        "print(f'  head_dim: {head_dim}')\n",
        "print(f'  num_layers: {num_layers}')\n",
        "print(f'  MLA d_c: {kv_compression_dim}')\n",
        "print(f'  MLA d_R: {rope_dim}')\n",
        "\n",
        "print(f'\\n{\"Mechanism\":<15} {\"Elements/Token\":<18} {\"Relative\":<10} {\"Equiv GQA Groups\":<18}')\n",
        "print('-' * 65)\n",
        "\n",
        "mha_cache = configs['MHA']\n",
        "for name, cache in configs.items():\n",
        "    relative = cache / mha_cache\n",
        "    equiv_groups = cache / (2 * head_dim)\n",
        "    print(f'{name:<15} {cache:<18,} {relative:<10.4f} {equiv_groups:<18.2f}')\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "# Bar chart of cache sizes\n",
        "ax = axes[0]\n",
        "names = list(configs.keys())\n",
        "sizes = list(configs.values())\n",
        "colors = ['#e74c3c', '#3498db', '#2ecc71', '#9b59b6', '#f39c12']\n",
        "bars = ax.bar(names, sizes, color=colors)\n",
        "ax.set_ylabel('Elements per Token per Layer', fontweight='bold')\n",
        "ax.set_title('KV Cache Size Comparison', fontweight='bold')\n",
        "ax.set_yscale('log')\n",
        "for bar, size in zip(bars, sizes):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() * 1.1, \n",
        "            f'{size:,}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Sequence length impact\n",
        "ax = axes[1]\n",
        "seq_lengths = [1024, 4096, 16384, 65536, 131072]\n",
        "for name, cache in configs.items():\n",
        "    total_cache = [cache * seq_len * num_layers * 2 / 1e9 for seq_len in seq_lengths]  # GB (fp16)\n",
        "    ax.plot(seq_lengths, total_cache, marker='o', label=name, linewidth=2)\n",
        "\n",
        "ax.set_xlabel('Sequence Length', fontweight='bold')\n",
        "ax.set_ylabel('Total KV Cache (GB, FP16)', fontweight='bold')\n",
        "ax.set_title('Cache Size vs Sequence Length', fontweight='bold')\n",
        "ax.set_xscale('log')\n",
        "ax.set_yscale('log')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Memory savings factor\n",
        "ax = axes[2]\n",
        "savings = [mha_cache / cache for cache in sizes]\n",
        "bars = ax.bar(names, savings, color=colors)\n",
        "ax.set_ylabel('Memory Reduction Factor (Ã—)', fontweight='bold')\n",
        "ax.set_title('Memory Savings vs MHA', fontweight='bold')\n",
        "ax.axhline(y=1, color='black', linestyle='--', alpha=0.3)\n",
        "for bar, s in zip(bars, savings):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
        "            f'{s:.1f}Ã—', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Multi-head Latent Attention (MLA) Memory Analysis', \n",
        "             fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Practical impact calculation\n",
        "print('\\n' + '='*70)\n",
        "print('PRACTICAL IMPACT: 128K Context Window')\n",
        "print('='*70)\n",
        "\n",
        "context_length = 131072  # 128K tokens\n",
        "bytes_per_element = 2  # FP16\n",
        "\n",
        "for name, cache in configs.items():\n",
        "    total_bytes = cache * context_length * num_layers * bytes_per_element\n",
        "    total_gb = total_bytes / (1024**3)\n",
        "    print(f'{name:<15}: {total_gb:>8.2f} GB')\n",
        "\n",
        "print(f'\\nğŸ’¡ With MLA, DeepSeek-V2 can handle 128K context on a single GPU!')\n",
        "print(f'   MHA would require {configs[\"MHA\"] * context_length * num_layers * 2 / 1e9:.1f} GB just for KV cache!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6.7: MLA - The Absorption Trick Explained\n",
        "\n",
        "One of the key insights of MLA is that during inference, we can \"absorb\" the up-projection matrices into the query/output projections, avoiding the need to reconstruct full keys and values.\n",
        "\n",
        "### ğŸ§® The Math Behind Absorption\n",
        "\n",
        "```\n",
        "Standard computation (during training):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Q @ K^T = Q @ (W_UK @ c_KV)^T\n",
        "        = Q @ c_KV^T @ W_UK^T\n",
        "\n",
        "This requires:\n",
        "1. Storing c_KV (cached)\n",
        "2. Computing W_UK @ c_KV = K (reconstructing keys)\n",
        "3. Computing Q @ K^T\n",
        "\n",
        "Absorbed computation (during inference):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Pre-compute: W_Q_absorbed = W_Q @ W_UK^T\n",
        "\n",
        "Then: Q @ K^T = (W_Q_absorbed @ h) @ c_KV^T\n",
        "\n",
        "This requires:\n",
        "1. Storing c_KV (cached)\n",
        "2. Computing W_Q_absorbed @ h = Q_absorbed\n",
        "3. Computing Q_absorbed @ c_KV^T\n",
        "\n",
        "We NEVER compute full keys! Just dot product with compressed latent!\n",
        "```\n",
        "\n",
        "### ğŸ“ Dimension Check\n",
        "\n",
        "```\n",
        "Without absorption:\n",
        "  W_Q:  (n_h Ã— d_h) Ã— d     Query projection\n",
        "  W_UK: (n_h Ã— d_h) Ã— d_c   Key up-projection\n",
        "  \n",
        "  Q: batch Ã— seq Ã— (n_h Ã— d_h)\n",
        "  K = W_UK @ c_KV: batch Ã— seq Ã— (n_h Ã— d_h)  â† Must compute this!\n",
        "  \n",
        "  Q @ K^T: batch Ã— heads Ã— seq Ã— seq\n",
        "\n",
        "With absorption:\n",
        "  W_Q_absorbed: (n_h Ã— d_h) Ã— d_c   Pre-computed!\n",
        "  \n",
        "  Q_absorbed: batch Ã— seq Ã— (n_h Ã— d_h)  â† Same as before\n",
        "  c_KV: batch Ã— seq Ã— d_c                 â† Much smaller!\n",
        "  \n",
        "  Q_absorbed @ c_KV^T: Works directly with compressed!\n",
        "```\n",
        "\n",
        "### âš¡ Why This Matters\n",
        "\n",
        "For each token during generation:\n",
        "- **Without absorption**: Must reconstruct full K (n_h Ã— d_h dimensions)\n",
        "- **With absorption**: Work directly with c_KV (d_c dimensions)\n",
        "\n",
        "Since d_c << n_h Ã— d_h, this is a massive speedup!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrating the Absorption Trick\n",
        "\n",
        "print('='*70)\n",
        "print('THE ABSORPTION TRICK DEMONSTRATION')\n",
        "print('='*70)\n",
        "\n",
        "# Setup dimensions\n",
        "embed_dim_abs = 64\n",
        "num_heads_abs = 4\n",
        "head_dim_abs = embed_dim_abs // num_heads_abs\n",
        "kv_compression_abs = 32\n",
        "\n",
        "# Create projection matrices\n",
        "W_Q = nn.Linear(embed_dim_abs, embed_dim_abs, bias=False)\n",
        "W_UK = nn.Linear(kv_compression_abs, embed_dim_abs, bias=False)\n",
        "\n",
        "# Pre-compute absorbed query projection\n",
        "# W_Q_absorbed = W_Q.weight @ W_UK.weight.T\n",
        "W_Q_absorbed = W_Q.weight @ W_UK.weight.T\n",
        "\n",
        "print(f'\\nğŸ“ Dimension Analysis:')\n",
        "print(f'   W_Q shape: {W_Q.weight.shape}')\n",
        "print(f'   W_UK shape: {W_UK.weight.shape}')\n",
        "print(f'   W_Q_absorbed shape: {W_Q_absorbed.shape}')\n",
        "\n",
        "# Test with a sample input\n",
        "batch_size_abs = 2\n",
        "seq_len_abs = 8\n",
        "\n",
        "h = torch.randn(batch_size_abs, seq_len_abs, embed_dim_abs)\n",
        "c_KV = torch.randn(batch_size_abs, seq_len_abs, kv_compression_abs)\n",
        "\n",
        "# Method 1: Standard (reconstruct K first)\n",
        "with torch.no_grad():\n",
        "    Q = W_Q(h)  # (batch, seq, embed_dim)\n",
        "    K = W_UK(c_KV)  # (batch, seq, embed_dim) - Must compute full K!\n",
        "    \n",
        "    Q_reshaped = Q.view(batch_size_abs, seq_len_abs, num_heads_abs, head_dim_abs).transpose(1, 2)\n",
        "    K_reshaped = K.view(batch_size_abs, seq_len_abs, num_heads_abs, head_dim_abs).transpose(1, 2)\n",
        "    \n",
        "    scores_standard = Q_reshaped @ K_reshaped.transpose(-2, -1)\n",
        "\n",
        "# Method 2: Absorbed (work directly with compressed)\n",
        "with torch.no_grad():\n",
        "    # Q @ K^T = Q @ (W_UK @ c_KV)^T = Q @ c_KV^T @ W_UK^T\n",
        "    # But we can also do: (Q @ W_UK^T) @ c_KV^T by restructuring\n",
        "    # Actually: Q @ K^T where K = c_KV @ W_UK^T\n",
        "    # So: Q @ (c_KV @ W_UK^T)^T = Q @ W_UK @ c_KV^T\n",
        "    \n",
        "    # For absorbed: Q_absorbed = h @ W_Q_absorbed^T, then Q_absorbed @ c_KV^T\n",
        "    Q_absorbed = h @ W_Q_absorbed.T  # (batch, seq, kv_compression)\n",
        "    \n",
        "    # Reshape for multi-head\n",
        "    # This is trickier - in real MLA, the absorption happens per-head\n",
        "    # For simplicity, let's verify the equivalence in a simpler way\n",
        "    \n",
        "    Q = W_Q(h)\n",
        "    # Q @ K^T = Q @ W_UK @ c_KV^T (treating as matrix equations)\n",
        "    \n",
        "    # Let's verify: Q @ K^T == (Q @ W_UK.weight) @ c_KV^T\n",
        "    QK_standard = Q @ K.transpose(-2, -1)  # (batch, seq_q, seq_k)\n",
        "    QK_absorbed = (Q @ W_UK.weight) @ c_KV.transpose(-2, -1)  # Same result!\n",
        "\n",
        "print(f'\\nğŸ”¬ Verification:')\n",
        "print(f'   Standard Q@K^T shape: {QK_standard.shape}')\n",
        "print(f'   Absorbed Q@K^T shape: {QK_absorbed.shape}')\n",
        "print(f'   Max difference: {torch.max(torch.abs(QK_standard - QK_absorbed)):.2e}')\n",
        "print(f'   Results match: {torch.allclose(QK_standard, QK_absorbed, atol=1e-5)}')\n",
        "\n",
        "# Memory comparison\n",
        "print(f'\\nğŸ“Š Memory Comparison:')\n",
        "print(f'   Standard method stores K: {K.numel()} elements')\n",
        "print(f'   Absorbed method stores c_KV: {c_KV.numel()} elements')\n",
        "print(f'   Memory reduction: {K.numel() / c_KV.numel():.1f}Ã—')\n",
        "\n",
        "print(f'\\nğŸ’¡ Key Insight:')\n",
        "print(f'   By pre-computing W_Q @ W_UK, we can compute attention scores')\n",
        "print(f'   directly with the compressed c_KV, never materializing full K!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 6.1: MLA Cache Calculations (ğŸŸ¡ Medium)\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Calculate and compare KV cache sizes for different attention mechanisms.\n",
        "\n",
        "### ğŸ“‹ Tasks\n",
        "\n",
        "**Task 1**: For a model with:\n",
        "- 128 attention heads\n",
        "- 128 dimensions per head\n",
        "- 60 layers\n",
        "\n",
        "Calculate KV cache per token (in elements) for:\n",
        "a) MHA (Multi-Head Attention)\n",
        "b) GQA with 8 KV groups\n",
        "c) MQA (Multi-Query Attention)\n",
        "d) MLA with d_c=512, d_R=64\n",
        "\n",
        "**Task 2**: For a 32K token context window with the above config:\n",
        "a) How much memory (in GB, FP16) does each mechanism need?\n",
        "b) What's the equivalent GQA group count for MLA?\n",
        "\n",
        "**Task 3**: If you have 24GB GPU memory for KV cache, what's the maximum context length for each mechanism?\n",
        "\n",
        "### ğŸ’¡ Formulas\n",
        "- MHA: $2 \\times n_h \\times d_h$ per token per layer\n",
        "- GQA: $2 \\times n_g \\times d_h$ per token per layer\n",
        "- MQA: $2 \\times d_h$ per token per layer\n",
        "- MLA: $(d_c + d^R_h)$ per token per layer\n",
        "- Memory (bytes) = elements Ã— layers Ã— bytes_per_element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸŸ¡ EXERCISE 6.1: MLA Cache Calculations\n",
        "\n",
        "# Configuration\n",
        "n_h = 128       # number of heads\n",
        "d_h = 128       # dimension per head\n",
        "l = 60          # number of layers\n",
        "d_c = 512       # MLA KV compression dimension\n",
        "d_R = 64        # MLA RoPE dimension\n",
        "\n",
        "# Task 1: Calculate KV cache per token per layer\n",
        "mha_cache = None    # YOUR ANSWER: 2 * n_h * d_h\n",
        "gqa8_cache = None   # YOUR ANSWER: 2 * 8 * d_h\n",
        "mqa_cache = None    # YOUR ANSWER: 2 * d_h\n",
        "mla_cache = None    # YOUR ANSWER: d_c + d_R\n",
        "\n",
        "# Task 2a: Memory for 32K context (FP16 = 2 bytes)\n",
        "context_len = 32768\n",
        "bytes_per_elem = 2\n",
        "\n",
        "mha_memory_gb = None   # YOUR ANSWER\n",
        "gqa8_memory_gb = None  # YOUR ANSWER\n",
        "mqa_memory_gb = None   # YOUR ANSWER\n",
        "mla_memory_gb = None   # YOUR ANSWER\n",
        "\n",
        "# Task 2b: Equivalent GQA groups for MLA\n",
        "# MLA cache = d_c + d_R = 576\n",
        "# GQA cache = 2 * n_g * d_h\n",
        "# Solve: 2 * n_g * d_h = d_c + d_R\n",
        "mla_equiv_gqa_groups = None  # YOUR ANSWER\n",
        "\n",
        "# Task 3: Max context for 24GB\n",
        "available_memory_bytes = 24 * (1024**3)\n",
        "\n",
        "mha_max_ctx = None   # YOUR ANSWER\n",
        "gqa8_max_ctx = None  # YOUR ANSWER\n",
        "mqa_max_ctx = None   # YOUR ANSWER\n",
        "mla_max_ctx = None   # YOUR ANSWER\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# ğŸ“ SOLUTION (uncomment to check)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# # Task 1\n",
        "# mha_cache = 2 * n_h * d_h  # 32,768\n",
        "# gqa8_cache = 2 * 8 * d_h   # 2,048\n",
        "# mqa_cache = 2 * d_h        # 256\n",
        "# mla_cache = d_c + d_R      # 576\n",
        "# \n",
        "# print('Task 1: KV Cache per Token per Layer')\n",
        "# print(f'  MHA:  {mha_cache:,} elements')\n",
        "# print(f'  GQA-8: {gqa8_cache:,} elements')\n",
        "# print(f'  MQA:  {mqa_cache:,} elements')\n",
        "# print(f'  MLA:  {mla_cache:,} elements')\n",
        "# \n",
        "# # Task 2a\n",
        "# mha_memory_gb = (mha_cache * context_len * l * bytes_per_elem) / (1024**3)\n",
        "# gqa8_memory_gb = (gqa8_cache * context_len * l * bytes_per_elem) / (1024**3)\n",
        "# mqa_memory_gb = (mqa_cache * context_len * l * bytes_per_elem) / (1024**3)\n",
        "# mla_memory_gb = (mla_cache * context_len * l * bytes_per_elem) / (1024**3)\n",
        "# \n",
        "# print(f'\\nTask 2a: Memory for 32K Context (FP16)')\n",
        "# print(f'  MHA:  {mha_memory_gb:.2f} GB')\n",
        "# print(f'  GQA-8: {gqa8_memory_gb:.2f} GB')\n",
        "# print(f'  MQA:  {mqa_memory_gb:.2f} GB')\n",
        "# print(f'  MLA:  {mla_memory_gb:.2f} GB')\n",
        "# \n",
        "# # Task 2b\n",
        "# mla_equiv_gqa_groups = (d_c + d_R) / (2 * d_h)  # 2.25\n",
        "# print(f'\\nTask 2b: MLA equivalent to GQA with {mla_equiv_gqa_groups:.2f} groups')\n",
        "# \n",
        "# # Task 3\n",
        "# mha_max_ctx = int(available_memory_bytes / (mha_cache * l * bytes_per_elem))\n",
        "# gqa8_max_ctx = int(available_memory_bytes / (gqa8_cache * l * bytes_per_elem))\n",
        "# mqa_max_ctx = int(available_memory_bytes / (mqa_cache * l * bytes_per_elem))\n",
        "# mla_max_ctx = int(available_memory_bytes / (mla_cache * l * bytes_per_elem))\n",
        "# \n",
        "# print(f'\\nTask 3: Maximum Context Length with 24GB')\n",
        "# print(f'  MHA:  {mha_max_ctx:,} tokens')\n",
        "# print(f'  GQA-8: {gqa8_max_ctx:,} tokens')\n",
        "# print(f'  MQA:  {mqa_max_ctx:,} tokens')\n",
        "# print(f'  MLA:  {mla_max_ctx:,} tokens')\n",
        "# print(f'\\nğŸ’¡ MLA enables {mla_max_ctx / mha_max_ctx:.0f}Ã— longer context than MHA!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## âœï¸ EXERCISE 6.2: Implement Simplified MLA (ğŸ”´ Hard)\n",
        "\n",
        "### ğŸ¯ Goal\n",
        "Implement a simplified version of MLA focusing on the core KV compression concept.\n",
        "\n",
        "### ğŸ“‹ Requirements\n",
        "1. Implement low-rank KV compression (down-project then up-project)\n",
        "2. Skip the RoPE components for simplicity\n",
        "3. Verify that output matches expected dimensions\n",
        "4. Compare cache size vs standard MHSA\n",
        "\n",
        "### ğŸ’¡ Template\n",
        "```python\n",
        "class SimplifiedMLA(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, kv_compression_dim):\n",
        "        # TODO: Initialize projections\n",
        "        # W_Q: embed_dim -> embed_dim\n",
        "        # W_DKV: embed_dim -> kv_compression_dim (down)\n",
        "        # W_UK: kv_compression_dim -> embed_dim (up for K)\n",
        "        # W_UV: kv_compression_dim -> embed_dim (up for V)\n",
        "        # W_O: embed_dim -> embed_dim\n",
        "        pass\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # TODO:\n",
        "        # 1. Compute Q normally: Q = W_Q(x)\n",
        "        # 2. Compress KV: c_KV = W_DKV(x)\n",
        "        # 3. Reconstruct K: K = W_UK(c_KV)\n",
        "        # 4. Reconstruct V: V = W_UV(c_KV)\n",
        "        # 5. Multi-head attention with Q, K, V\n",
        "        # 6. Output projection\n",
        "        pass\n",
        "```\n",
        "\n",
        "### âœ… Verification\n",
        "```python\n",
        "mla = SimplifiedMLA(embed_dim=64, num_heads=4, kv_compression_dim=16)\n",
        "x = torch.randn(2, 8, 64)\n",
        "out = mla(x)\n",
        "assert out.shape == (2, 8, 64)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ğŸ”´ EXERCISE 6.2: Implement Simplified MLA\n",
        "\n",
        "class SimplifiedMLA(nn.Module):\n",
        "    \"\"\"\n",
        "    YOUR IMPLEMENTATION HERE\n",
        "    \n",
        "    Simplified MLA without RoPE - focus on KV compression only.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, kv_compression_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert embed_dim % num_heads == 0\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.kv_compression_dim = kv_compression_dim\n",
        "        \n",
        "        # TODO: Create projection matrices\n",
        "        # self.W_Q = ...\n",
        "        # self.W_DKV = ...  # Down-project\n",
        "        # self.W_UK = ...   # Up-project for K\n",
        "        # self.W_UV = ...   # Up-project for V\n",
        "        # self.W_O = ...\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    def forward(self, x, return_cache=False):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        # TODO: Implement forward pass\n",
        "        # Step 1: Compute Q\n",
        "        # Step 2: Compress KV: c_KV = W_DKV(x)\n",
        "        # Step 3: Reconstruct K from c_KV\n",
        "        # Step 4: Reconstruct V from c_KV  \n",
        "        # Step 5: Multi-head attention\n",
        "        # Step 6: Output projection\n",
        "        \n",
        "        pass  # Remove and implement\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# ğŸ“ SOLUTION (uncomment to check)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "# class SimplifiedMLA(nn.Module):\n",
        "#     def __init__(self, embed_dim, num_heads, kv_compression_dim):\n",
        "#         super().__init__()\n",
        "#         \n",
        "#         assert embed_dim % num_heads == 0\n",
        "#         \n",
        "#         self.embed_dim = embed_dim\n",
        "#         self.num_heads = num_heads\n",
        "#         self.head_dim = embed_dim // num_heads\n",
        "#         self.kv_compression_dim = kv_compression_dim\n",
        "#         self.scale = self.head_dim ** -0.5\n",
        "#         \n",
        "#         # Query projection (standard)\n",
        "#         self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "#         \n",
        "#         # KV compression (down-project to latent)\n",
        "#         self.W_DKV = nn.Linear(embed_dim, kv_compression_dim, bias=False)\n",
        "#         \n",
        "#         # KV expansion (up-project from latent)\n",
        "#         self.W_UK = nn.Linear(kv_compression_dim, embed_dim, bias=False)\n",
        "#         self.W_UV = nn.Linear(kv_compression_dim, embed_dim, bias=False)\n",
        "#         \n",
        "#         # Output projection\n",
        "#         self.W_O = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "#     \n",
        "#     def forward(self, x, return_cache=False):\n",
        "#         batch_size, seq_len, _ = x.shape\n",
        "#         \n",
        "#         # Step 1: Compute Q (standard)\n",
        "#         Q = self.W_Q(x)\n",
        "#         Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "#         \n",
        "#         # Step 2: Compress KV into shared latent\n",
        "#         c_KV = self.W_DKV(x)  # (batch, seq, kv_compression_dim)\n",
        "#         \n",
        "#         # Step 3: Reconstruct K from latent\n",
        "#         K = self.W_UK(c_KV)\n",
        "#         K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "#         \n",
        "#         # Step 4: Reconstruct V from latent\n",
        "#         V = self.W_UV(c_KV)\n",
        "#         V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "#         \n",
        "#         # Step 5: Standard attention computation\n",
        "#         scores = (Q @ K.transpose(-2, -1)) * self.scale\n",
        "#         attn = torch.softmax(scores, dim=-1)\n",
        "#         out = attn @ V\n",
        "#         \n",
        "#         # Step 6: Concatenate heads and project output\n",
        "#         out = out.transpose(1, 2).contiguous().view(batch_size, seq_len, self.embed_dim)\n",
        "#         out = self.W_O(out)\n",
        "#         \n",
        "#         if return_cache:\n",
        "#             return out, attn, c_KV  # Return compressed latent as cache\n",
        "#         return out, attn, None\n",
        "# \n",
        "# # Test\n",
        "# print('Testing SimplifiedMLA...')\n",
        "# simplified_mla = SimplifiedMLA(embed_dim=64, num_heads=4, kv_compression_dim=16)\n",
        "# x_test = torch.randn(2, 8, 64)\n",
        "# out_test, attn_test, cache_test = simplified_mla(x_test, return_cache=True)\n",
        "# \n",
        "# print(f'Input shape: {x_test.shape}')\n",
        "# print(f'Output shape: {out_test.shape}')\n",
        "# print(f'Attention shape: {attn_test.shape}')\n",
        "# print(f'Cache shape: {cache_test.shape}')\n",
        "# \n",
        "# # Verify shapes\n",
        "# assert out_test.shape == (2, 8, 64), \"Output shape mismatch!\"\n",
        "# print('âœ… All tests passed!')\n",
        "# \n",
        "# # Cache comparison\n",
        "# mha_cache = 2 * 4 * 16 * 8  # 2 (K+V) * heads * head_dim * seq\n",
        "# mla_cache = 16 * 8  # compression_dim * seq\n",
        "# print(f'\\nğŸ“Š Cache Comparison:')\n",
        "# print(f'   MHA cache: {mha_cache} elements')\n",
        "# print(f'   MLA cache: {mla_cache} elements')\n",
        "# print(f'   Reduction: {mha_cache / mla_cache:.1f}Ã—')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6.8: MLA Summary and Key Takeaways\n",
        "\n",
        "### ğŸ¯ What We Learned\n",
        "\n",
        "Multi-head Latent Attention (MLA) is DeepSeek's innovative attention mechanism that achieves:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        MLA KEY INNOVATIONS                               â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  1ï¸âƒ£ LOW-RANK KV COMPRESSION                                            â”‚\n",
        "â”‚     â€¢ Compress K,V into shared latent c_KV                              â”‚\n",
        "â”‚     â€¢ Cache only c_KV (d_c elements) instead of full K,V               â”‚\n",
        "â”‚     â€¢ Reconstruct K,V on-the-fly during attention                       â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  2ï¸âƒ£ DECOUPLED ROPE                                                     â”‚\n",
        "â”‚     â€¢ Separate content (k_C) from position (k_R)                        â”‚\n",
        "â”‚     â€¢ Enables absorption trick during inference                         â”‚\n",
        "â”‚     â€¢ k_R is shared across heads (even smaller cache!)                  â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  3ï¸âƒ£ ABSORPTION TRICK                                                   â”‚\n",
        "â”‚     â€¢ Pre-compute W_Q @ W_UK^T during inference                         â”‚\n",
        "â”‚     â€¢ Work directly with compressed latent                              â”‚\n",
        "â”‚     â€¢ Never materialize full keys!                                      â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ“Š Performance Summary\n",
        "\n",
        "| Metric | MHA | GQA-8 | MQA | MLA |\n",
        "|--------|-----|-------|-----|-----|\n",
        "| Quality | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜…+ |\n",
        "| Memory | â˜… | â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜…â˜… |\n",
        "| Speed | â˜…â˜… | â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… |\n",
        "| Complexity | â˜…â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | â˜…â˜…â˜…â˜… | â˜…â˜…â˜… |\n",
        "\n",
        "### ğŸš€ When to Use MLA\n",
        "\n",
        "**Use MLA when you need:**\n",
        "- Very long context windows (32K+)\n",
        "- High quality on par with or better than MHA\n",
        "- Memory-efficient inference\n",
        "- Production deployment with memory constraints\n",
        "\n",
        "**MLA is used in:**\n",
        "- DeepSeek-V2\n",
        "- DeepSeek-Coder-V2\n",
        "- DeepSeek-V3\n",
        "\n",
        "### ğŸ“š Further Reading\n",
        "\n",
        "1. **DeepSeek-V2 Paper**: \"DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model\"\n",
        "2. **Original GQA Paper**: \"GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\"\n",
        "3. **RoPE Paper**: \"RoFormer: Enhanced Transformer with Rotary Position Embedding\"\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-7\"></a>\n",
        "# Part 7: Cross-Attention - Connecting Encoder and Decoder â±ï¸ ~25 min\n",
        "\n",
        "## Section 7.1: What is Cross-Attention?\n",
        "\n",
        "### ğŸ¤” The Problem: Connecting Two Sequences\n",
        "\n",
        "In many tasks, we need to relate **two different sequences**:\n",
        "\n",
        "```\n",
        "Machine Translation:\n",
        "  Encoder input:  \"The cat sat on the mat\" (English)\n",
        "  Decoder output: \"Le chat s'est assis sur le tapis\" (French)\n",
        "  \n",
        "  How does the decoder know which English words to focus on\n",
        "  when generating each French word?\n",
        "  \n",
        "Image Captioning:\n",
        "  Encoder input:  [image features from CNN/ViT]\n",
        "  Decoder output: \"A cat sitting on a colorful mat\"\n",
        "  \n",
        "  How does the decoder know which image regions to focus on?\n",
        "  \n",
        "Question Answering:\n",
        "  Encoder input:  \"The cat sat on the mat. It was tired.\"\n",
        "  Decoder query:  \"What was tired?\"\n",
        "  \n",
        "  How do we find relevant context?\n",
        "```\n",
        "\n",
        "### âœ… The Solution: Cross-Attention\n",
        "\n",
        "**Key Insight**: Query comes from ONE sequence, Key/Value from ANOTHER!\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    SELF-ATTENTION vs CROSS-ATTENTION                    â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   SELF-ATTENTION:                                                       â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                       â”‚\n",
        "â”‚   Q, K, V all come from the SAME sequence                              â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚      Input X â”€â”€â”¬â”€â”€â†’ Q                                                   â”‚\n",
        "â”‚                â”œâ”€â”€â†’ K    (Same source!)                                 â”‚\n",
        "â”‚                â””â”€â”€â†’ V                                                   â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Used in: Encoder, Decoder (masked)                                   â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   CROSS-ATTENTION:                                                      â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚\n",
        "â”‚   Q from DECODER, K/V from ENCODER                                     â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚      Decoder output â”€â”€â†’ Q   (What am I looking for?)                   â”‚\n",
        "â”‚      Encoder output â”€â”€â†’ K   (What does source offer?)                  â”‚\n",
        "â”‚      Encoder output â”€â”€â†’ V   (What information to retrieve?)            â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Used in: Encoder-Decoder models (T5, BART, etc.)                     â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ“Š Visual Flow\n",
        "\n",
        "```\n",
        "ENCODER                              DECODER\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€                             â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\"The cat sat\"                        \"Le chat\"\n",
        "     â”‚                                   â”‚\n",
        "     â–¼                                   â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Self-   â”‚                         â”‚ Masked  â”‚\n",
        "â”‚ Attn    â”‚                         â”‚ Self-   â”‚\n",
        "â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                         â”‚ Attn    â”‚\n",
        "     â”‚                              â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
        "     â”‚                                   â”‚\n",
        "     â”‚        K, V                       â”‚ Q\n",
        "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                  â”‚\n",
        "                  â–¼\n",
        "           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "           â”‚   CROSS-    â”‚  â† The magic happens here!\n",
        "           â”‚  ATTENTION  â”‚\n",
        "           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "                  â”‚\n",
        "                  â–¼\n",
        "           \"s'est\" (next word)\n",
        "```\n",
        "\n",
        "### ğŸ”¢ The Formula\n",
        "\n",
        "Cross-attention is almost identical to self-attention:\n",
        "\n",
        "$$\n",
        "\\text{CrossAttention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^\\top}{\\sqrt{d_k}}\\right) \\mathbf{V}\n",
        "$$\n",
        "\n",
        "The only difference:\n",
        "- $\\mathbf{Q} = \\mathbf{X}_{\\text{decoder}} \\mathbf{W}_Q$\n",
        "- $\\mathbf{K} = \\mathbf{X}_{\\text{encoder}} \\mathbf{W}_K$\n",
        "- $\\mathbf{V} = \\mathbf{X}_{\\text{encoder}} \\mathbf{W}_V$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ› ï¸ BUILD IT YOURSELF: Cross-Attention Step-by-Step\n",
        "\n",
        "Cross-attention is crucial for encoder-decoder models. Let's build it!\n",
        "\n",
        "### ğŸ“‹ What You'll Implement\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    YOUR CROSS-ATTENTION JOURNEY                        â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Exercise 7.A: Understand Q from decoder, K/V from encoder            â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 7.B: Handle different sequence lengths                      â”‚\n",
        "â”‚        â†“                                                                â”‚\n",
        "â”‚   Exercise 7.C: Complete Cross-Attention module                        â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "**Key Difference from Self-Attention:** The sequences for Q and K/V are DIFFERENT!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 7.A: Understand Q from Decoder, K/V from Encoder\n",
        "\n",
        "**The Core Difference:** In cross-attention, Q and K/V come from DIFFERENT sequences!\n",
        "\n",
        "```\n",
        "Self-Attention (same sequence):\n",
        "  X â†’ Q, K, V    (X is shape [batch, seq_len, dim])\n",
        "  \n",
        "Cross-Attention (different sequences):\n",
        "  X_decoder â†’ Q   (shape [batch, tgt_len, dim])\n",
        "  X_encoder â†’ K   (shape [batch, src_len, dim])\n",
        "  X_encoder â†’ V   (shape [batch, src_len, dim])\n",
        "```\n",
        "\n",
        "**Why Different Lengths?**\n",
        "- Encoder (source): \"Hello world\" â†’ 2 tokens\n",
        "- Decoder (target): \"Bonjour le monde\" â†’ 3 tokens\n",
        "\n",
        "**Attention Shape:**\n",
        "- Self-attention: (batch, seq_len, seq_len)\n",
        "- Cross-attention: (batch, tgt_len, src_len) â† Different dimensions!\n",
        "\n",
        "**Your Task:**\n",
        "1. Create Q from decoder\n",
        "2. Create K, V from encoder\n",
        "3. Observe the asymmetric attention shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 7.A: Understand Q from Decoder, K/V from Encoder\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "# Configuration\n",
        "embed_dim = 64\n",
        "batch_size = 2\n",
        "\n",
        "# DIFFERENT sequence lengths!\n",
        "src_len = 5   # Encoder (source) sequence length\n",
        "tgt_len = 3   # Decoder (target) sequence length\n",
        "\n",
        "# Simulate encoder output and decoder hidden states\n",
        "encoder_output = torch.randn(batch_size, src_len, embed_dim)\n",
        "decoder_hidden = torch.randn(batch_size, tgt_len, embed_dim)\n",
        "\n",
        "print(f\"Encoder output shape: {encoder_output.shape}  (batch, src_len, embed_dim)\")\n",
        "print(f\"Decoder hidden shape: {decoder_hidden.shape}  (batch, tgt_len, embed_dim)\")\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "\n",
        "# TODO: Create projection layers\n",
        "# Q comes from DECODER, so projects decoder_hidden\n",
        "# K,V come from ENCODER, so project encoder_output\n",
        "\n",
        "W_q = # YOUR CODE: nn.Linear(embed_dim, embed_dim)\n",
        "W_k = # YOUR CODE\n",
        "W_v = # YOUR CODE\n",
        "\n",
        "# TODO: Project to Q, K, V\n",
        "# Q from decoder\n",
        "\n",
        "Q = # YOUR CODE: W_q(decoder_hidden)\n",
        "\n",
        "# K from encoder\n",
        "\n",
        "K = # YOUR CODE: W_k(encoder_output)\n",
        "\n",
        "# V from encoder\n",
        "\n",
        "V = # YOUR CODE: W_v(encoder_output)\n",
        "\n",
        "print(f\"\\nQ shape: {Q.shape}  (from decoder)\")\n",
        "print(f\"K shape: {K.shape}  (from encoder)\")\n",
        "print(f\"V shape: {V.shape}  (from encoder)\")\n",
        "\n",
        "# TODO: Compute attention scores\n",
        "# Notice: tgt_len x src_len (not src_len x src_len!)\n",
        "\n",
        "scores = # YOUR CODE: (Q @ K.transpose(-2, -1)) / math.sqrt(embed_dim)\n",
        "\n",
        "print(f\"\\nAttention scores shape: {scores.shape}\")\n",
        "print(f\"  â†’ (batch, tgt_len={tgt_len}, src_len={src_len})\")\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Verification\n",
        "assert Q.shape == (batch_size, tgt_len, embed_dim)\n",
        "assert K.shape == (batch_size, src_len, embed_dim)\n",
        "assert scores.shape == (batch_size, tgt_len, src_len)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Q from decoder, K/V from encoder!\")\n",
        "print(f\"âœ… Attention is {tgt_len}Ã—{src_len}, NOT {src_len}Ã—{src_len}!\")\n",
        "print(\"âœ… Each decoder token attends to ALL encoder tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 7.B: Handle Encoder Padding Mask\n",
        "\n",
        "**Real-World Challenge:** Encoder sequences are often padded to the same length.\n",
        "\n",
        "```\n",
        "Batch of source sentences:\n",
        "  \"The cat sat\" â†’ [the, cat, sat, PAD, PAD]\n",
        "  \"Hello\"       â†’ [hello, PAD, PAD, PAD, PAD]\n",
        "```\n",
        "\n",
        "**Problem:** We don't want to attend to padding tokens!\n",
        "\n",
        "**Solution:** Apply a mask that sets attention to -âˆ for padding positions.\n",
        "\n",
        "```\n",
        "Encoder mask: [1, 1, 1, 0, 0]  (1 = real, 0 = padding)\n",
        "              \n",
        "Attention scores before mask:   Attention scores after mask:\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ 0.2  0.3  0.4  0.1  0.0 â”‚    â”‚ 0.2  0.3  0.4  -âˆ   -âˆ â”‚\n",
        "â”‚ 0.1  0.5  0.2  0.1  0.1 â”‚ â†’  â”‚ 0.1  0.5  0.2  -âˆ   -âˆ â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "**Your Task:**\n",
        "1. Create an encoder padding mask\n",
        "2. Expand it to match attention shape\n",
        "3. Apply it before softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 7.B: Handle Encoder Padding Mask\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Configuration\n",
        "embed_dim = 64\n",
        "batch_size = 2\n",
        "src_len = 5   # Encoder length (with padding)\n",
        "tgt_len = 3   # Decoder length\n",
        "\n",
        "# Simulated attention scores (from previous exercise)\n",
        "scores = torch.randn(batch_size, tgt_len, src_len)\n",
        "\n",
        "print(f\"Scores shape: {scores.shape}\")\n",
        "print(f\"Raw scores (batch 0):\\n{scores[0]}\")\n",
        "\n",
        "# ============ YOUR CODE HERE ============\n",
        "\n",
        "# Encoder mask: 1 for real tokens, 0 for padding\n",
        "# Example: first sequence has 3 real tokens, second has 2\n",
        "encoder_mask = torch.tensor([\n",
        "    [1, 1, 1, 0, 0],  # \"the cat sat [PAD] [PAD]\"\n",
        "    [1, 1, 0, 0, 0],  # \"hello world [PAD] [PAD] [PAD]\"\n",
        "], dtype=torch.float)\n",
        "\n",
        "print(f\"\\nEncoder mask shape: {encoder_mask.shape}\")\n",
        "print(f\"Encoder mask:\\n{encoder_mask}\")\n",
        "\n",
        "# TODO: Expand mask to match attention shape\n",
        "# scores: (batch, tgt_len, src_len)\n",
        "# encoder_mask: (batch, src_len)\n",
        "# We need: (batch, tgt_len, src_len) - broadcast over tgt_len\n",
        "\n",
        "# Hint: encoder_mask.unsqueeze(1) â†’ (batch, 1, src_len) â†’ broadcasts to (batch, tgt_len, src_len)\n",
        "\n",
        "mask_expanded = # YOUR CODE\n",
        "\n",
        "print(f\"\\nExpanded mask shape: {mask_expanded.shape}\")\n",
        "\n",
        "# TODO: Apply mask - set padded positions to -infinity\n",
        "\n",
        "masked_scores = # YOUR CODE: scores.masked_fill(mask_expanded == 0, float('-inf'))\n",
        "\n",
        "print(f\"\\nMasked scores (batch 0):\\n{masked_scores[0]}\")\n",
        "\n",
        "# TODO: Apply softmax - padded positions become 0 probability\n",
        "\n",
        "attn_weights = # YOUR CODE: F.softmax(masked_scores, dim=-1)\n",
        "\n",
        "print(f\"\\nAttention weights (batch 0):\\n{attn_weights[0]}\")\n",
        "\n",
        "# =========================================\n",
        "\n",
        "# Verify padded positions have 0 attention\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"âœ… Padded positions (columns 3,4 for batch 0) have 0 attention!\")\n",
        "print(\"âœ… Real tokens share all the attention weight\")\n",
        "print(f\"âœ… Each row sums to 1: {attn_weights[0, 0].sum().item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Exercise 7.C: Complete Cross-Attention Module\n",
        "\n",
        "**Now put it all together into a complete module!**\n",
        "\n",
        "You've learned:\n",
        "- âœ… Q from decoder, K/V from encoder\n",
        "- âœ… Asymmetric attention shape (tgt_len Ã— src_len)\n",
        "- âœ… Encoder padding mask\n",
        "\n",
        "**Your Task:** Create a multi-head cross-attention module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 7.C: Complete Cross-Attention Module\n",
        "\n",
        "class MyCrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Your implementation of Multi-Head Cross-Attention!\n",
        "    \n",
        "    Key differences from self-attention:\n",
        "    - Q from decoder, K/V from encoder\n",
        "    - Different sequence lengths for Q vs K/V\n",
        "    - Encoder padding mask for variable-length sources\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, embed_dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert embed_dim % num_heads == 0\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        \n",
        "        # Q projection (from decoder)\n",
        "        self.W_q = # YOUR CODE\n",
        "        \n",
        "        # K, V projections (from encoder)\n",
        "        self.W_k = # YOUR CODE\n",
        "        self.W_v = # YOUR CODE\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_o = # YOUR CODE\n",
        "        \n",
        "        # =========================================\n",
        "    \n",
        "    def forward(\n",
        "        self, \n",
        "        decoder_hidden: torch.Tensor, \n",
        "        encoder_output: torch.Tensor,\n",
        "        encoder_mask: torch.Tensor = None\n",
        "    ) -> tuple:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            decoder_hidden: (batch, tgt_len, embed_dim) - decoder states\n",
        "            encoder_output: (batch, src_len, embed_dim) - encoder output\n",
        "            encoder_mask: (batch, src_len) - 1 for real tokens, 0 for padding\n",
        "            \n",
        "        Returns:\n",
        "            output: (batch, tgt_len, embed_dim)\n",
        "            attention_weights: (batch, num_heads, tgt_len, src_len)\n",
        "        \"\"\"\n",
        "        batch_size, tgt_len, _ = decoder_hidden.shape\n",
        "        _, src_len, _ = encoder_output.shape\n",
        "        \n",
        "        # ============ YOUR CODE HERE ============\n",
        "        \n",
        "        # Step 1: Q from decoder\n",
        "        Q = # YOUR CODE: self.W_q(decoder_hidden)\n",
        "        \n",
        "        # Step 2: K, V from encoder\n",
        "        K = # YOUR CODE\n",
        "        V = # YOUR CODE\n",
        "        \n",
        "        # Step 3: Reshape for multi-head (Q has tgt_len, K/V have src_len!)\n",
        "        Q = Q.view(batch_size, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        # Step 4: Compute attention scores\n",
        "        scores = # YOUR CODE: (Q @ K.transpose(-2, -1)) * self.scale\n",
        "        \n",
        "        # Step 5: Apply encoder mask if provided\n",
        "        if encoder_mask is not None:\n",
        "            # Expand mask: (batch, src_len) â†’ (batch, 1, 1, src_len)\n",
        "            mask = encoder_mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        \n",
        "        # Step 6: Softmax\n",
        "        attn_weights = # YOUR CODE\n",
        "        \n",
        "        # Step 7: Apply to values\n",
        "        out = # YOUR CODE: attn_weights @ V\n",
        "        \n",
        "        # Step 8: Reshape and project output\n",
        "        out = out.transpose(1, 2).contiguous()\n",
        "        out = out.view(batch_size, tgt_len, self.embed_dim)\n",
        "        out = # YOUR CODE: self.W_o(out)\n",
        "        \n",
        "        # =========================================\n",
        "        \n",
        "        return out, attn_weights\n",
        "\n",
        "\n",
        "# Test your implementation!\n",
        "print(\"Testing MyCrossAttention...\")\n",
        "\n",
        "embed_dim = 64\n",
        "num_heads = 4\n",
        "batch_size = 2\n",
        "src_len = 5   # Encoder length\n",
        "tgt_len = 3   # Decoder length\n",
        "\n",
        "cross_attn = MyCrossAttention(embed_dim, num_heads)\n",
        "\n",
        "encoder_output = torch.randn(batch_size, src_len, embed_dim)\n",
        "decoder_hidden = torch.randn(batch_size, tgt_len, embed_dim)\n",
        "encoder_mask = torch.tensor([[1, 1, 1, 0, 0], [1, 1, 1, 1, 0]], dtype=torch.float)\n",
        "\n",
        "output, attn = cross_attn(decoder_hidden, encoder_output, encoder_mask)\n",
        "\n",
        "print(f\"Decoder hidden shape: {decoder_hidden.shape}\")\n",
        "print(f\"Encoder output shape: {encoder_output.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Attention shape: {attn.shape}\")\n",
        "\n",
        "# Verify shapes\n",
        "assert output.shape == (batch_size, tgt_len, embed_dim)\n",
        "assert attn.shape == (batch_size, num_heads, tgt_len, src_len)\n",
        "\n",
        "# Verify mask is applied (padded positions should have 0 attention)\n",
        "# Batch 0: positions 3,4 are padded, batch 1: position 4 is padded\n",
        "print(f\"\\nAttention to padded position (batch 0, head 0, row 0, col 3): {attn[0, 0, 0, 3].item():.6f}\")\n",
        "print(f\"Attention to padded position (batch 0, head 0, row 0, col 4): {attn[0, 0, 0, 4].item():.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… Congratulations! You've built Cross-Attention!\")\n",
        "print(\"âœ… Q from decoder, K/V from encoder\")\n",
        "print(\"âœ… Asymmetric attention: (tgt_len Ã— src_len)\")\n",
        "print(\"âœ… Encoder padding mask properly applied!\")\n",
        "print(\"âœ… Compare your code with the reference implementation below\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-Attention Implementation\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Cross-Attention: Q from decoder, K/V from encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "        \n",
        "        # Q projection (from decoder)\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
        "        \n",
        "        # K, V projections (from encoder)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, decoder_hidden, encoder_output, encoder_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            decoder_hidden: (batch, tgt_len, embed_dim) - decoder states\n",
        "            encoder_output: (batch, src_len, embed_dim) - encoder output\n",
        "            encoder_mask: (batch, src_len) - 1 for real tokens, 0 for padding\n",
        "        \n",
        "        Returns:\n",
        "            output: (batch, tgt_len, embed_dim)\n",
        "            attention_weights: (batch, num_heads, tgt_len, src_len)\n",
        "        \"\"\"\n",
        "        batch_size, tgt_len, _ = decoder_hidden.shape\n",
        "        _, src_len, _ = encoder_output.shape\n",
        "        \n",
        "        # Q from decoder, K/V from encoder\n",
        "        Q = self.W_q(decoder_hidden)  # (batch, tgt_len, embed_dim)\n",
        "        K = self.W_k(encoder_output)  # (batch, src_len, embed_dim)\n",
        "        V = self.W_v(encoder_output)  # (batch, src_len, embed_dim)\n",
        "        \n",
        "        # Reshape for multi-head attention\n",
        "        Q = Q.view(batch_size, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, src_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        # Attention scores: (batch, heads, tgt_len, src_len)\n",
        "        scores = (Q @ K.transpose(-2, -1)) * self.scale\n",
        "        \n",
        "        # Apply encoder padding mask if provided\n",
        "        if encoder_mask is not None:\n",
        "            # Expand mask: (batch, src_len) -> (batch, 1, 1, src_len)\n",
        "            mask = encoder_mask.unsqueeze(1).unsqueeze(2)\n",
        "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "        \n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "        \n",
        "        # Apply attention to values\n",
        "        out = attn @ V\n",
        "        \n",
        "        # Concatenate heads\n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, tgt_len, self.embed_dim)\n",
        "        out = self.W_o(out)\n",
        "        \n",
        "        return out, attn\n",
        "\n",
        "# Demonstration\n",
        "print('='*70)\n",
        "print('CROSS-ATTENTION DEMONSTRATION')\n",
        "print('='*70)\n",
        "\n",
        "# Simulating translation: English â†’ French\n",
        "encoder_tokens = ['The', 'cat', 'sat']\n",
        "decoder_tokens = ['Le', 'chat']\n",
        "\n",
        "print(f'\\nEncoder (source): {encoder_tokens}')\n",
        "print(f'Decoder (target): {decoder_tokens}')\n",
        "\n",
        "# Create dummy embeddings\n",
        "embed_dim_cross = 16\n",
        "num_heads_cross = 2\n",
        "\n",
        "encoder_output = torch.randn(1, len(encoder_tokens), embed_dim_cross)\n",
        "decoder_hidden = torch.randn(1, len(decoder_tokens), embed_dim_cross)\n",
        "\n",
        "# Create cross-attention\n",
        "cross_attn = CrossAttention(embed_dim_cross, num_heads_cross)\n",
        "output, attn_weights = cross_attn(decoder_hidden, encoder_output)\n",
        "\n",
        "print(f'\\nEncoder output shape: {encoder_output.shape}')\n",
        "print(f'Decoder hidden shape: {decoder_hidden.shape}')\n",
        "print(f'Cross-attention output shape: {output.shape}')\n",
        "print(f'Attention weights shape: {attn_weights.shape}')\n",
        "print(f'  â†’ (batch, heads, tgt_len={len(decoder_tokens)}, src_len={len(encoder_tokens)})')\n",
        "\n",
        "# Visualize cross-attention\n",
        "fig, axes = plt.subplots(1, num_heads_cross + 1, figsize=(14, 4))\n",
        "\n",
        "for h in range(num_heads_cross):\n",
        "    ax = axes[h]\n",
        "    attn = attn_weights[0, h].detach().numpy()\n",
        "    im = ax.imshow(attn, cmap='YlOrRd', vmin=0, vmax=1)\n",
        "    ax.set_xticks(range(len(encoder_tokens)))\n",
        "    ax.set_yticks(range(len(decoder_tokens)))\n",
        "    ax.set_xticklabels(encoder_tokens, fontsize=12)\n",
        "    ax.set_yticklabels(decoder_tokens, fontsize=12)\n",
        "    ax.set_xlabel('Encoder (Source)', fontweight='bold')\n",
        "    ax.set_ylabel('Decoder (Target)', fontweight='bold')\n",
        "    ax.set_title(f'Head {h+1}', fontweight='bold')\n",
        "    for i in range(len(decoder_tokens)):\n",
        "        for j in range(len(encoder_tokens)):\n",
        "            ax.text(j, i, f'{attn[i,j]:.2f}', ha='center', va='center', fontsize=10)\n",
        "\n",
        "# Average across heads\n",
        "ax = axes[-1]\n",
        "avg_attn = attn_weights[0].mean(dim=0).detach().numpy()\n",
        "im = ax.imshow(avg_attn, cmap='YlOrRd', vmin=0, vmax=1)\n",
        "ax.set_xticks(range(len(encoder_tokens)))\n",
        "ax.set_yticks(range(len(decoder_tokens)))\n",
        "ax.set_xticklabels(encoder_tokens, fontsize=12)\n",
        "ax.set_yticklabels(decoder_tokens, fontsize=12)\n",
        "ax.set_xlabel('Encoder (Source)', fontweight='bold')\n",
        "ax.set_ylabel('Decoder (Target)', fontweight='bold')\n",
        "ax.set_title('Average', fontweight='bold')\n",
        "\n",
        "plt.suptitle('Cross-Attention: Which source words inform each target word?', \n",
        "             fontsize=13, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nğŸ’¡ Cross-Attention Insight:')\n",
        "print('   â€¢ \"Le\" looks at English words to decide what to generate')\n",
        "print('   â€¢ \"chat\" should attend strongly to \"cat\" (its translation)')\n",
        "print('   â€¢ After training, this alignment becomes meaningful!')\n",
        "print('   â€¢ This is how translation models learn word correspondences')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-8\"></a>\n",
        "# Part 8: Modern Optimizations â±ï¸ ~30 min\n",
        "\n",
        "## Section 8.1: Flash Attention - Memory-Efficient Fused Attention\n",
        "\n",
        "### ğŸš€ The Problem with Standard Attention\n",
        "\n",
        "Standard attention requires storing the **full attention matrix** in memory:\n",
        "\n",
        "```\n",
        "Standard Attention Memory Usage:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Sequence length: n\n",
        "Attention matrix: n Ã— n\n",
        "\n",
        "For n = 8192 (typical LLM context):\n",
        "  Memory for attention matrix: 8192Â² Ã— 4 bytes = 256 MB per layer!\n",
        "  \n",
        "For n = 100,000 (long document):\n",
        "  Memory: 100,000Â² Ã— 4 bytes = 40 GB per layer! ğŸ”¥\n",
        "  \n",
        "This is the memory bottleneck!\n",
        "```\n",
        "\n",
        "### âœ… Flash Attention Solution\n",
        "\n",
        "**Key Insight**: Never materialize the full attention matrix!\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                     FLASH ATTENTION ALGORITHM                           â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   STANDARD ATTENTION (Memory: O(nÂ²)):                                  â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                   â”‚\n",
        "â”‚   1. Compute full S = QK^T           â† Store nÃ—n matrix               â”‚\n",
        "â”‚   2. Apply softmax to all of S       â† Need all values                â”‚\n",
        "â”‚   3. Compute S @ V                   â† Matrix multiply                 â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   FLASH ATTENTION (Memory: O(n)):                                      â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                     â”‚\n",
        "â”‚   1. Process Q, K, V in BLOCKS (tiles)                                 â”‚\n",
        "â”‚   2. For each block:                                                    â”‚\n",
        "â”‚      - Compute partial scores                                          â”‚\n",
        "â”‚      - Update running softmax statistics                               â”‚\n",
        "â”‚      - Accumulate output                                               â”‚\n",
        "â”‚   3. Never store full attention matrix!                                â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Memory: Only need to store current block + statistics                â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ“Š Performance Comparison\n",
        "\n",
        "```\n",
        "Sequence Length: 8192, Batch: 8, Heads: 32\n",
        "\n",
        "                    Standard Attention    Flash Attention\n",
        "                    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€     â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Memory Usage:       ~16 GB                ~500 MB\n",
        "Speed:              1.0x (baseline)       2-4x faster\n",
        "GPU Utilization:    60-70%               90%+ (better!)\n",
        "```\n",
        "\n",
        "### ğŸ”§ Using Flash Attention in PyTorch 2.0+\n",
        "\n",
        "```python\n",
        "# PyTorch 2.0+ has built-in Flash Attention!\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Automatic: uses Flash Attention when possible\n",
        "output = F.scaled_dot_product_attention(Q, K, V, attn_mask=mask)\n",
        "```\n",
        "\n",
        "### ğŸ’¡ When Flash Attention is Used\n",
        "\n",
        "PyTorch automatically uses Flash Attention when:\n",
        "- GPU has Tensor Cores (Ampere, Hopper, etc.)\n",
        "- Inputs are on CUDA\n",
        "- Data type is float16 or bfloat16\n",
        "- Sequence length is moderate to large\n",
        "\n",
        "---\n",
        "\n",
        "## Section 8.2: RoPE - Rotary Position Embeddings\n",
        "\n",
        "### ğŸ”„ The Innovation\n",
        "\n",
        "RoPE (used in LLaMA, Mistral, etc.) encodes position by **rotating** Q and K vectors!\n",
        "\n",
        "```\n",
        "Traditional Position Encoding:\n",
        "  X_input = X_embedding + PE_position\n",
        "  (Adds position to input)\n",
        "\n",
        "RoPE:\n",
        "  Q_rotated = rotate(Q, position)\n",
        "  K_rotated = rotate(K, position)\n",
        "  (Rotates Q, K based on position)\n",
        "```\n",
        "\n",
        "### ğŸ“ Why Rotation Works\n",
        "\n",
        "The dot product of rotated vectors encodes **relative** position:\n",
        "\n",
        "$$\n",
        "\\langle R_m q, R_n k \\rangle = \\langle q, R_{n-m} k \\rangle\n",
        "$$\n",
        "\n",
        "The score depends on $(n-m)$ = relative distance!\n",
        "\n",
        "### âœ¨ Benefits of RoPE\n",
        "\n",
        "| Feature | Benefit |\n",
        "|---------|---------|\n",
        "| **Relative position** | Captures distance between tokens naturally |\n",
        "| **Extrapolation** | Better generalization to longer sequences |\n",
        "| **Efficiency** | Applied to Q, K only (not full hidden) |\n",
        "| **NTK-aware scaling** | Can extend context with interpolation |\n",
        "\n",
        "---\n",
        "\n",
        "## Section 7.3: ALiBi - Attention with Linear Biases\n",
        "\n",
        "### ğŸ“ Even Simpler: Just Add a Bias!\n",
        "\n",
        "ALiBi (used in BLOOM, MPT) doesn't modify embeddings at all:\n",
        "\n",
        "```\n",
        "Standard:\n",
        "  scores = Q @ K^T / âˆšd\n",
        "\n",
        "ALiBi:\n",
        "  scores = Q @ K^T / âˆšd - m Ã— distance_matrix\n",
        "  \n",
        "Where:\n",
        "  distance_matrix[i,j] = |i - j|  (distance between positions)\n",
        "  m = head-specific slope (different for each head)\n",
        "```\n",
        "\n",
        "### ğŸ¨ Visual Example\n",
        "\n",
        "```\n",
        "Distance matrix (how far apart are positions):\n",
        "         pos0  pos1  pos2  pos3\n",
        "  pos0 [   0,    1,    2,    3 ]\n",
        "  pos1 [   1,    0,    1,    2 ]\n",
        "  pos2 [   2,    1,    0,    1 ]\n",
        "  pos3 [   3,    2,    1,    0 ]\n",
        "\n",
        "ALiBi bias (m = 0.5):\n",
        "         pos0  pos1  pos2  pos3\n",
        "  pos0 [  0.0, -0.5, -1.0, -1.5 ]\n",
        "  pos1 [ -0.5,  0.0, -0.5, -1.0 ]\n",
        "  pos2 [ -1.0, -0.5,  0.0, -0.5 ]\n",
        "  pos3 [ -1.5, -1.0, -0.5,  0.0 ]\n",
        "\n",
        "Farther positions get LOWER scores (less attention)\n",
        "```\n",
        "\n",
        "### âœ¨ Benefits of ALiBi\n",
        "\n",
        "| Feature | Benefit |\n",
        "|---------|---------|\n",
        "| **No position embeddings** | Fewer parameters |\n",
        "| **Perfect extrapolation** | Works on any sequence length |\n",
        "| **Simple** | Just add a bias matrix |\n",
        "\n",
        "---\n",
        "\n",
        "## Section 7.4: KV Caching for Efficient Inference\n",
        "\n",
        "### ğŸš€ The Inference Problem\n",
        "\n",
        "During generation, we compute attention **repeatedly**:\n",
        "\n",
        "```\n",
        "Generating \"The cat sat on the mat\"\n",
        "\n",
        "Step 1: Generate \"The\"\n",
        "  Compute K, V for [\"The\"]\n",
        "  \n",
        "Step 2: Generate \"cat\"  \n",
        "  Compute K, V for [\"The\", \"cat\"]  â† Recomputes \"The\"! âŒ\n",
        "  \n",
        "Step 3: Generate \"sat\"\n",
        "  Compute K, V for [\"The\", \"cat\", \"sat\"]  â† Recomputes again! âŒ\n",
        "\n",
        "WASTEFUL: We keep recomputing K, V for past tokens!\n",
        "```\n",
        "\n",
        "### âœ… KV Caching Solution\n",
        "\n",
        "**Cache** the K and V for all past tokens:\n",
        "\n",
        "```\n",
        "With KV Cache:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "Step 1: Generate \"The\"\n",
        "  Compute K1, V1 for [\"The\"]\n",
        "  Cache: {K: [K1], V: [V1]}\n",
        "  \n",
        "Step 2: Generate \"cat\"\n",
        "  Compute K2, V2 for [\"cat\"] only\n",
        "  Cache: {K: [K1, K2], V: [V1, V2]}\n",
        "  Q attends to full cached K, V\n",
        "  \n",
        "Step 3: Generate \"sat\"\n",
        "  Compute K3, V3 for [\"sat\"] only\n",
        "  Cache: {K: [K1, K2, K3], V: [V1, V2, V3]}\n",
        "\n",
        "Each step: Only compute K, V for NEW token!\n",
        "Speedup: O(n) â†’ O(1) per token! ğŸš€\n",
        "```\n",
        "\n",
        "### ğŸ“Š Memory vs Speed Tradeoff\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    KV CACHE TRADEOFF                          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                               â”‚\n",
        "â”‚   WITHOUT CACHE:                                              â”‚\n",
        "â”‚   â€¢ Time per token: O(n) - slower as sequence grows          â”‚\n",
        "â”‚   â€¢ Memory: Low - recompute each time                        â”‚\n",
        "â”‚                                                               â”‚\n",
        "â”‚   WITH CACHE:                                                 â”‚\n",
        "â”‚   â€¢ Time per token: O(1) - constant time! âš¡                  â”‚\n",
        "â”‚   â€¢ Memory: O(n) - stores all past K, V                      â”‚\n",
        "â”‚                                                               â”‚\n",
        "â”‚   For long contexts, cache can be HUGE:                      â”‚\n",
        "â”‚   LLaMA-70B, 8K context: ~16 GB just for KV cache!          â”‚\n",
        "â”‚                                                               â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "This is why GQA helps so much - fewer KV heads = smaller cache!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Modern Attention Optimizations - Code Examples\n",
        "\n",
        "# ============================================================================\n",
        "# 1. Flash Attention (PyTorch 2.0+)\n",
        "# ============================================================================\n",
        "\n",
        "print('='*70)\n",
        "print('MODERN ATTENTION OPTIMIZATIONS')\n",
        "print('='*70)\n",
        "\n",
        "print('\\nğŸ“¦ 1. Flash Attention (PyTorch 2.0+)')\n",
        "print('-'*50)\n",
        "\n",
        "if hasattr(F, 'scaled_dot_product_attention'):\n",
        "    print('âœ… Flash Attention is available!')\n",
        "    \n",
        "    # Example usage\n",
        "    Q_flash = torch.randn(2, 4, 8, 16)  # (batch, heads, seq, head_dim)\n",
        "    K_flash = torch.randn(2, 4, 8, 16)\n",
        "    V_flash = torch.randn(2, 4, 8, 16)\n",
        "    \n",
        "    # Automatic Flash Attention\n",
        "    output_flash = F.scaled_dot_product_attention(Q_flash, K_flash, V_flash)\n",
        "    print(f'   Input Q shape: {Q_flash.shape}')\n",
        "    print(f'   Output shape: {output_flash.shape}')\n",
        "    print('   Flash Attention is used automatically when beneficial!')\n",
        "else:\n",
        "    print('âš ï¸ Flash Attention requires PyTorch 2.0+')\n",
        "\n",
        "# ============================================================================\n",
        "# 2. RoPE (Rotary Position Embeddings)\n",
        "# ============================================================================\n",
        "\n",
        "print('\\nğŸ“¦ 2. RoPE (Rotary Position Embeddings)')\n",
        "print('-'*50)\n",
        "\n",
        "def apply_rotary_pos_emb(x, cos, sin):\n",
        "    \"\"\"Apply rotary position embeddings to x.\"\"\"\n",
        "    # Split x into pairs for rotation\n",
        "    x1, x2 = x[..., ::2], x[..., 1::2]\n",
        "    # Apply rotation\n",
        "    rotated = torch.stack([-x2, x1], dim=-1).flatten(-2)\n",
        "    return x * cos + rotated * sin\n",
        "\n",
        "def get_rotary_embeddings(seq_len, head_dim, base=10000):\n",
        "    \"\"\"Generate rotary embeddings.\"\"\"\n",
        "    inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n",
        "    positions = torch.arange(seq_len).float()\n",
        "    freqs = torch.outer(positions, inv_freq)\n",
        "    cos = torch.cos(freqs).repeat_interleave(2, dim=-1)\n",
        "    sin = torch.sin(freqs).repeat_interleave(2, dim=-1)\n",
        "    return cos, sin\n",
        "\n",
        "# Demo RoPE\n",
        "seq_len_rope = 8\n",
        "head_dim_rope = 16\n",
        "cos, sin = get_rotary_embeddings(seq_len_rope, head_dim_rope)\n",
        "\n",
        "print(f'   Generated rotary embeddings for seq_len={seq_len_rope}, head_dim={head_dim_rope}')\n",
        "print(f'   cos shape: {cos.shape}, sin shape: {sin.shape}')\n",
        "\n",
        "# ============================================================================\n",
        "# 3. ALiBi (Attention with Linear Biases)\n",
        "# ============================================================================\n",
        "\n",
        "print('\\nğŸ“¦ 3. ALiBi (Attention with Linear Biases)')\n",
        "print('-'*50)\n",
        "\n",
        "def get_alibi_slopes(num_heads):\n",
        "    \"\"\"Get ALiBi slopes for each head.\"\"\"\n",
        "    # Slopes decrease geometrically: 2^(-8/n), 2^(-8*2/n), ...\n",
        "    ratio = 2 ** (-8 / num_heads)\n",
        "    return ratio ** torch.arange(1, num_heads + 1)\n",
        "\n",
        "def get_alibi_bias(seq_len, num_heads):\n",
        "    \"\"\"Generate ALiBi bias matrix.\"\"\"\n",
        "    slopes = get_alibi_slopes(num_heads)\n",
        "    # Distance matrix\n",
        "    positions = torch.arange(seq_len)\n",
        "    distance = positions.unsqueeze(0) - positions.unsqueeze(1)  # (seq, seq)\n",
        "    # Apply slopes: (heads, 1, 1) * (seq, seq)\n",
        "    bias = slopes.unsqueeze(-1).unsqueeze(-1) * distance.abs().unsqueeze(0)\n",
        "    return -bias  # Negative: penalize distant positions\n",
        "\n",
        "alibi_bias = get_alibi_bias(5, 4)\n",
        "print(f'   ALiBi bias shape: {alibi_bias.shape}')\n",
        "print(f'   (4 heads, 5 seq positions, 5 seq positions)')\n",
        "\n",
        "# Visualize ALiBi for one head\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.imshow(alibi_bias[0].numpy(), cmap='RdBu', vmin=-5, vmax=0)\n",
        "plt.colorbar(label='Bias (negative = less attention)')\n",
        "plt.xlabel('Key Position', fontweight='bold')\n",
        "plt.ylabel('Query Position', fontweight='bold')\n",
        "plt.title('ALiBi Bias (Head 1)\\nFarther = More Negative = Less Attention', fontweight='bold')\n",
        "for i in range(5):\n",
        "    for j in range(5):\n",
        "        plt.text(j, i, f'{alibi_bias[0,i,j]:.1f}', ha='center', va='center', fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 4. KV Cache (Simplified Demo)\n",
        "# ============================================================================\n",
        "\n",
        "print('\\nğŸ“¦ 4. KV Cache (Simplified Demo)')\n",
        "print('-'*50)\n",
        "\n",
        "class AttentionWithKVCache(nn.Module):\n",
        "    \"\"\"Attention with KV caching for efficient inference.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        \n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim)\n",
        "        self.W_o = nn.Linear(embed_dim, embed_dim)\n",
        "        \n",
        "        # KV Cache (stored as buffers, not parameters)\n",
        "        self.cache_k = None\n",
        "        self.cache_v = None\n",
        "    \n",
        "    def clear_cache(self):\n",
        "        \"\"\"Clear the KV cache (call before new generation).\"\"\"\n",
        "        self.cache_k = None\n",
        "        self.cache_v = None\n",
        "    \n",
        "    def forward(self, x, use_cache=False):\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        Q = self.W_q(x)\n",
        "        K_new = self.W_k(x)\n",
        "        V_new = self.W_v(x)\n",
        "        \n",
        "        if use_cache and self.cache_k is not None:\n",
        "            # Append new K, V to cache\n",
        "            K = torch.cat([self.cache_k, K_new], dim=1)\n",
        "            V = torch.cat([self.cache_v, V_new], dim=1)\n",
        "        else:\n",
        "            K = K_new\n",
        "            V = V_new\n",
        "        \n",
        "        # Update cache\n",
        "        if use_cache:\n",
        "            self.cache_k = K.detach()\n",
        "            self.cache_v = V.detach()\n",
        "        \n",
        "        # Reshape for attention\n",
        "        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        \n",
        "        # Attention\n",
        "        scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        out = attn @ V\n",
        "        \n",
        "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)\n",
        "        return self.W_o(out[:, -seq_len:])  # Return only new positions\n",
        "\n",
        "# Demo KV Cache\n",
        "attn_kv = AttentionWithKVCache(embed_dim=16, num_heads=2)\n",
        "attn_kv.clear_cache()\n",
        "\n",
        "# Simulate autoregressive generation\n",
        "print('   Simulating autoregressive generation with KV cache:')\n",
        "for step in range(1, 5):\n",
        "    x_step = torch.randn(1, 1, 16)  # One new token\n",
        "    output = attn_kv(x_step, use_cache=True)\n",
        "    cache_len = attn_kv.cache_k.shape[1]\n",
        "    print(f'   Step {step}: Cache size = {cache_len} tokens')\n",
        "\n",
        "print('\\n   âœ… Cache grows with each token - no recomputation!')\n",
        "print('   This is how LLMs generate text efficiently.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-9\"></a>\n",
        "# Part 9: Building a Complete Transformer Block â±ï¸ ~25 min\n",
        "\n",
        "## Section 9.1: Components of a Transformer Block\n",
        "\n",
        "### ğŸ—ï¸ The Complete Architecture\n",
        "\n",
        "A transformer block combines several components:\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    TRANSFORMER BLOCK ARCHITECTURE                       â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Input                                                                 â”‚\n",
        "â”‚     â”‚                                                                   â”‚\n",
        "â”‚     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚\n",
        "â”‚     â”‚                   â”‚ (Residual Connection)                        â”‚\n",
        "â”‚     â–¼                   â”‚                                              â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚                                              â”‚\n",
        "â”‚   â”‚  Layer Norm   â”‚     â”‚  â† Pre-Norm (modern) or Post-Norm (original)â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                                              â”‚\n",
        "â”‚           â”‚             â”‚                                              â”‚\n",
        "â”‚           â–¼             â”‚                                              â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚                                              â”‚\n",
        "â”‚   â”‚  Multi-Head   â”‚     â”‚                                              â”‚\n",
        "â”‚   â”‚  Attention    â”‚     â”‚                                              â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                                              â”‚\n",
        "â”‚           â”‚             â”‚                                              â”‚\n",
        "â”‚           â–¼             â”‚                                              â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚                                              â”‚\n",
        "â”‚   â”‚   Dropout     â”‚     â”‚                                              â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                                              â”‚\n",
        "â”‚           â”‚             â”‚                                              â”‚\n",
        "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â†’ ADD                                       â”‚\n",
        "â”‚                         â”‚     â”‚                                        â”‚\n",
        "â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                                        â”‚\n",
        "â”‚     â”‚                         â”‚ (Residual Connection)                  â”‚\n",
        "â”‚     â–¼                         â”‚                                        â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                                        â”‚\n",
        "â”‚   â”‚  Layer Norm   â”‚           â”‚                                        â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                                        â”‚\n",
        "â”‚           â”‚                   â”‚                                        â”‚\n",
        "â”‚           â–¼                   â”‚                                        â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                                        â”‚\n",
        "â”‚   â”‚  Feed-Forward â”‚           â”‚                                        â”‚\n",
        "â”‚   â”‚    Network    â”‚           â”‚                                        â”‚\n",
        "â”‚   â”‚ (MLP/FFN)     â”‚           â”‚                                        â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                                        â”‚\n",
        "â”‚           â”‚                   â”‚                                        â”‚\n",
        "â”‚           â–¼                   â”‚                                        â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚                                        â”‚\n",
        "â”‚   â”‚   Dropout     â”‚           â”‚                                        â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚                                        â”‚\n",
        "â”‚           â”‚                   â”‚                                        â”‚\n",
        "â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â†’ ADD                                 â”‚\n",
        "â”‚                               â”‚                                        â”‚\n",
        "â”‚                               â–¼                                        â”‚\n",
        "â”‚                            Output                                      â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ“Š Key Components Explained\n",
        "\n",
        "| Component | Purpose | Typical Config |\n",
        "|-----------|---------|----------------|\n",
        "| **Layer Norm** | Stabilizes training | Normalize hidden states |\n",
        "| **Multi-Head Attention** | Context mixing | 8-96 heads |\n",
        "| **Feed-Forward Network** | Non-linear processing | 4x hidden dim |\n",
        "| **Dropout** | Regularization | 0.0-0.1 |\n",
        "| **Residual Connections** | Gradient flow | x + sublayer(x) |\n",
        "\n",
        "---\n",
        "\n",
        "## Section 8.2: Layer Normalization\n",
        "\n",
        "### ğŸ“ What is Layer Norm?\n",
        "\n",
        "Normalizes across the **feature dimension** (not batch):\n",
        "\n",
        "$$\n",
        "\\text{LayerNorm}(x) = \\gamma \\cdot \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} + \\beta\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\mu, \\sigma$ = mean and std computed over features\n",
        "- $\\gamma, \\beta$ = learnable scale and shift\n",
        "- $\\epsilon$ = small constant for numerical stability\n",
        "\n",
        "```\n",
        "Input shape: (batch, seq_len, embed_dim)\n",
        "Normalize over: embed_dim dimension (last axis)\n",
        "\n",
        "Example:\n",
        "  x = [0.5, 1.0, 1.5, 2.0]  (one token, 4 dims)\n",
        "  mean = 1.25\n",
        "  std = 0.56\n",
        "  normalized = [-1.34, -0.45, 0.45, 1.34]\n",
        "```\n",
        "\n",
        "### ğŸ”„ Pre-Norm vs Post-Norm\n",
        "\n",
        "```\n",
        "Post-Norm (Original Transformer):         Pre-Norm (GPT-2, LLaMA):\n",
        "  x â†’ Attention â†’ Add â†’ LN                  x â†’ LN â†’ Attention â†’ Add\n",
        "                                            \n",
        "  More stable gradients for Pre-Norm!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Section 8.3: Feed-Forward Network (FFN/MLP)\n",
        "\n",
        "### ğŸ§  The \"Thinking\" Layer\n",
        "\n",
        "FFN applies non-linear transformation to each position independently:\n",
        "\n",
        "$$\n",
        "\\text{FFN}(x) = \\text{GELU}(x W_1 + b_1) W_2 + b_2\n",
        "$$\n",
        "\n",
        "```\n",
        "FFN Architecture:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "Input: (batch, seq, d_model)\n",
        "   â”‚\n",
        "   â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Linear(d_model, 4Ã—d) â”‚  â† Expand dimension\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚\n",
        "           â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚       GELU           â”‚  â† Non-linearity\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚\n",
        "           â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚ Linear(4Ã—d, d_model) â”‚  â† Contract back\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "           â”‚\n",
        "           â–¼\n",
        "Output: (batch, seq, d_model)\n",
        "\n",
        "Why 4Ã— expansion?\n",
        "  More capacity to learn complex patterns!\n",
        "```\n",
        "\n",
        "### ğŸ”€ Activation Functions\n",
        "\n",
        "| Activation | Formula | Used In |\n",
        "|------------|---------|---------|\n",
        "| **ReLU** | max(0, x) | Original Transformer |\n",
        "| **GELU** | xÂ·Î¦(x) | GPT, BERT |\n",
        "| **SwiGLU** | Swish(xW)Â·(xV) | LLaMA, PaLM |\n",
        "\n",
        "---\n",
        "\n",
        "## Section 8.4: Residual Connections\n",
        "\n",
        "### â†©ï¸ The Skip Connection\n",
        "\n",
        "```\n",
        "Residual Connection:\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "x â”€â”€â”€â”€â”¤                     â”œâ”€â”€â”€â”€â†’ x + f(x)\n",
        "      â”‚      f(x)           â”‚\n",
        "      â”‚   (sublayer)        â”‚\n",
        "      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "Why?\n",
        "1. Allows gradients to flow directly (skip)\n",
        "2. Each layer learns a \"delta\" to add\n",
        "3. Essential for training deep networks\n",
        "```\n",
        "\n",
        "Without residuals, gradients vanish in deep networks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete Transformer Block Implementation\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Feed-Forward Network (FFN/MLP) with GELU activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete Transformer Block with Pre-Norm architecture.\n",
        "    \n",
        "    Components:\n",
        "    1. Multi-Head Self-Attention\n",
        "    2. Layer Normalization (Pre-Norm)\n",
        "    3. Feed-Forward Network\n",
        "    4. Residual Connections\n",
        "    5. Dropout\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        ff_dim = ff_dim or embed_dim * 4  # Default: 4x expansion\n",
        "        \n",
        "        # Layer norms (Pre-Norm architecture)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        \n",
        "        # Multi-Head Attention\n",
        "        self.attention = MultiHeadAttentionWithDropout(embed_dim, num_heads, dropout)\n",
        "        \n",
        "        # Feed-Forward Network\n",
        "        self.ff = FeedForward(embed_dim, ff_dim, dropout)\n",
        "        \n",
        "        # Store config for debugging\n",
        "        self.config = {\n",
        "            'embed_dim': embed_dim,\n",
        "            'num_heads': num_heads,\n",
        "            'ff_dim': ff_dim,\n",
        "            'dropout': dropout,\n",
        "        }\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, embed_dim)\n",
        "            mask: Optional attention mask\n",
        "        \n",
        "        Returns:\n",
        "            output: (batch, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # Self-Attention with residual\n",
        "        attn_out, attn_weights = self.attention(self.norm1(x), mask)\n",
        "        x = x + attn_out  # Residual connection\n",
        "        \n",
        "        # FFN with residual\n",
        "        ff_out = self.ff(self.norm2(x))\n",
        "        x = x + ff_out  # Residual connection\n",
        "        \n",
        "        return x, attn_weights\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Stack of Transformer Blocks (Encoder).\n",
        "    \"\"\"\n",
        "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim=None, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)  # Final norm\n",
        "    \n",
        "    def forward(self, x, mask=None):\n",
        "        attentions = []\n",
        "        for layer in self.layers:\n",
        "            x, attn = layer(x, mask)\n",
        "            attentions.append(attn)\n",
        "        \n",
        "        x = self.norm(x)\n",
        "        return x, attentions\n",
        "\n",
        "\n",
        "# Build and test a complete Transformer\n",
        "print('='*70)\n",
        "print('ğŸ—ï¸  COMPLETE TRANSFORMER BLOCK')\n",
        "print('='*70)\n",
        "\n",
        "# Configuration (GPT-2 small style)\n",
        "config = {\n",
        "    'num_layers': 4,\n",
        "    'embed_dim': 64,\n",
        "    'num_heads': 4,\n",
        "    'ff_dim': 256,  # 4x embed_dim\n",
        "    'dropout': 0.1,\n",
        "}\n",
        "\n",
        "print('\\nğŸ“‹ Configuration:')\n",
        "for key, value in config.items():\n",
        "    print(f'   {key}: {value}')\n",
        "\n",
        "# Create transformer encoder\n",
        "transformer = TransformerEncoder(**config)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in transformer.parameters())\n",
        "print(f'\\nğŸ“Š Total Parameters: {total_params:,}')\n",
        "\n",
        "# Test forward pass\n",
        "batch_size = 2\n",
        "seq_len = 8\n",
        "\n",
        "x_test = torch.randn(batch_size, seq_len, config['embed_dim'])\n",
        "print(f'\\nğŸ”„ Forward Pass:')\n",
        "print(f'   Input shape: {x_test.shape}')\n",
        "\n",
        "output, all_attentions = transformer(x_test)\n",
        "print(f'   Output shape: {output.shape}')\n",
        "print(f'   Attention shapes: {len(all_attentions)} layers Ã— {all_attentions[0].shape}')\n",
        "\n",
        "# Visualize architecture\n",
        "print('\\n' + '='*70)\n",
        "print('ğŸ“ ARCHITECTURE DIAGRAM')\n",
        "print('='*70)\n",
        "print('''\n",
        "   Input (batch, seq, embed_dim)\n",
        "         â”‚\n",
        "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n",
        "    â”‚ Layer 1 â”‚ â”€â”¬â”€ Norm â†’ MHSA â†’ Add\n",
        "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚\n",
        "         â”‚       â””â”€ Norm â†’ FFN â†’ Add\n",
        "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n",
        "    â”‚ Layer 2 â”‚ â”€â”¬â”€ Norm â†’ MHSA â†’ Add\n",
        "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚\n",
        "         â”‚       â””â”€ Norm â†’ FFN â†’ Add\n",
        "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n",
        "    â”‚ Layer 3 â”‚ â”€â”¬â”€ Norm â†’ MHSA â†’ Add\n",
        "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚\n",
        "         â”‚       â””â”€ Norm â†’ FFN â†’ Add\n",
        "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n",
        "    â”‚ Layer 4 â”‚ â”€â”¬â”€ Norm â†’ MHSA â†’ Add\n",
        "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â”‚\n",
        "         â”‚       â””â”€ Norm â†’ FFN â†’ Add\n",
        "    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”\n",
        "    â”‚Final LN â”‚\n",
        "    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜\n",
        "         â”‚\n",
        "   Output (batch, seq, embed_dim)\n",
        "''')\n",
        "\n",
        "# Verify residual connections work\n",
        "print('='*70)\n",
        "print('âœ… RESIDUAL CONNECTION TEST')\n",
        "print('='*70)\n",
        "\n",
        "# Check that output isn't too different from input (residuals preserve info)\n",
        "input_norm = torch.norm(x_test).item()\n",
        "output_norm = torch.norm(output).item()\n",
        "print(f'\\n   Input norm: {input_norm:.2f}')\n",
        "print(f'   Output norm: {output_norm:.2f}')\n",
        "print(f'   Ratio: {output_norm/input_norm:.2f}x')\n",
        "print('\\n   Residual connections preserve signal magnitude! âœ…')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-10\"></a>\n",
        "# Part 10: End-to-End Project - Sentiment Classifier â±ï¸ ~40 min\n",
        "\n",
        "## ğŸ¯ Project Goal\n",
        "\n",
        "Build a complete **sentiment classifier** using attention mechanisms!\n",
        "\n",
        "```\n",
        "Input:  \"I love this movie!\"     â†’  POSITIVE âœ…\n",
        "Input:  \"This was terrible.\"     â†’  NEGATIVE âŒ\n",
        "```\n",
        "\n",
        "### ğŸ“Š Architecture\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    ATTENTION-BASED CLASSIFIER                          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   \"I love this movie\"                                                   â”‚\n",
        "â”‚         â”‚                                                               â”‚\n",
        "â”‚         â–¼                                                               â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\n",
        "â”‚   â”‚ Token Embedding â”‚  vocab_size â†’ embed_dim                          â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\n",
        "â”‚            â”‚                                                            â”‚\n",
        "â”‚            â–¼                                                            â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\n",
        "â”‚   â”‚ + Positional    â”‚                                                  â”‚\n",
        "â”‚   â”‚   Encoding      â”‚                                                  â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\n",
        "â”‚            â”‚                                                            â”‚\n",
        "â”‚            â–¼                                                            â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\n",
        "â”‚   â”‚  Transformer    â”‚  N layers of attention + FFN                     â”‚\n",
        "â”‚   â”‚    Encoder      â”‚                                                  â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\n",
        "â”‚            â”‚                                                            â”‚\n",
        "â”‚            â–¼                                                            â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\n",
        "â”‚   â”‚  Mean Pooling   â”‚  Average across sequence                         â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\n",
        "â”‚            â”‚                                                            â”‚\n",
        "â”‚            â–¼                                                            â”‚\n",
        "â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                  â”‚\n",
        "â”‚   â”‚  Classification â”‚  embed_dim â†’ num_classes                         â”‚\n",
        "â”‚   â”‚     Head        â”‚                                                  â”‚\n",
        "â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                  â”‚\n",
        "â”‚            â”‚                                                            â”‚\n",
        "â”‚            â–¼                                                            â”‚\n",
        "â”‚      [POSITIVE, NEGATIVE]                                              â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "### ğŸ“š What We'll Build\n",
        "\n",
        "1. **Token Embedding Layer** - Convert words to vectors\n",
        "2. **Positional Encoding** - Add position information  \n",
        "3. **Transformer Encoder** - Multiple attention layers\n",
        "4. **Classification Head** - Predict sentiment\n",
        "5. **Training Loop** - Learn from examples\n",
        "6. **Attention Visualization** - See what the model learns!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ§  Check Your Understanding: Before the Project\n",
        "\n",
        "**Before building the sentiment classifier, review the key concepts:**\n",
        "\n",
        "### Quick Quiz\n",
        "\n",
        "1. **What components make up a Transformer block?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   LayerNorm â†’ Multi-Head Attention â†’ Dropout â†’ Residual Add â†’ LayerNorm â†’ FFN â†’ Dropout â†’ Residual Add\n",
        "   </details>\n",
        "\n",
        "2. **What does the FFN (Feed-Forward Network) do?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   Expands dimension (usually 4Ã—), applies non-linearity (ReLU/GELU), then projects back. This adds model capacity.\n",
        "   </details>\n",
        "\n",
        "3. **Why are residual connections important?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   They allow gradients to flow directly through the network, preventing vanishing gradients and enabling training of deep models.\n",
        "   </details>\n",
        "\n",
        "4. **How do we convert variable-length sequences to fixed-size for classification?**\n",
        "   <details>\n",
        "   <summary>Click for answer</summary>\n",
        "   Common approaches: mean pooling (average all tokens), use [CLS] token embedding, or max pooling.\n",
        "   </details>\n",
        "\n",
        "### âœ… Ready to build!\n",
        "You now have all the knowledge needed to build a complete sentiment classifier!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# END-TO-END PROJECT: SENTIMENT CLASSIFIER\n",
        "# ============================================================================\n",
        "\n",
        "class AttentionClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple Transformer-based Sentiment Classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, \n",
        "                 num_classes, max_seq_len=128, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embed_dim = embed_dim\n",
        "        \n",
        "        # Token embedding\n",
        "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        \n",
        "        # Positional encoding (learnable)\n",
        "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
        "        \n",
        "        # Transformer encoder\n",
        "        self.encoder = TransformerEncoder(\n",
        "            num_layers=num_layers,\n",
        "            embed_dim=embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            ff_dim=embed_dim * 4,\n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    \n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_ids: (batch, seq_len) token indices\n",
        "            attention_mask: (batch, seq_len) 1 for real, 0 for padding\n",
        "        \n",
        "        Returns:\n",
        "            logits: (batch, num_classes)\n",
        "            attention_weights: list of attention weights per layer\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        \n",
        "        # Token + Position embeddings\n",
        "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
        "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Create attention mask (expand to 4D)\n",
        "        mask = None\n",
        "        if attention_mask is not None:\n",
        "            mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "        \n",
        "        # Transformer encoder\n",
        "        x, attentions = self.encoder(x, mask)\n",
        "        \n",
        "        # Mean pooling (average over sequence, respecting mask)\n",
        "        if attention_mask is not None:\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).float()\n",
        "            x = (x * mask_expanded).sum(dim=1) / mask_expanded.sum(dim=1).clamp(min=1)\n",
        "        else:\n",
        "            x = x.mean(dim=1)\n",
        "        \n",
        "        # Classify\n",
        "        logits = self.classifier(x)\n",
        "        \n",
        "        return logits, attentions\n",
        "\n",
        "\n",
        "# Create a simple vocabulary and dataset\n",
        "print('='*70)\n",
        "print('ğŸ¯ END-TO-END PROJECT: SENTIMENT CLASSIFIER')\n",
        "print('='*70)\n",
        "\n",
        "# Simple vocabulary\n",
        "vocab = {\n",
        "    '<pad>': 0, '<unk>': 1,\n",
        "    'i': 2, 'love': 3, 'hate': 4, 'this': 5, 'movie': 6, \n",
        "    'was': 7, 'great': 8, 'terrible': 9, 'amazing': 10, \n",
        "    'awful': 11, 'it': 12, 'the': 13, 'best': 14, 'worst': 15,\n",
        "    'ever': 16, 'good': 17, 'bad': 18, 'really': 19, 'so': 20,\n",
        "    'absolutely': 21, 'fantastic': 22, 'horrible': 23\n",
        "}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Simple dataset (toy examples)\n",
        "train_data = [\n",
        "    # (text, label)  0=negative, 1=positive\n",
        "    (\"i love this movie\", 1),\n",
        "    (\"this was great\", 1),\n",
        "    (\"i hate this movie\", 0),\n",
        "    (\"this was terrible\", 0),\n",
        "    (\"it was amazing\", 1),\n",
        "    (\"it was awful\", 0),\n",
        "    (\"the best movie ever\", 1),\n",
        "    (\"the worst movie ever\", 0),\n",
        "    (\"really good movie\", 1),\n",
        "    (\"really bad movie\", 0),\n",
        "    (\"absolutely fantastic\", 1),\n",
        "    (\"absolutely horrible\", 0),\n",
        "    (\"so good\", 1),\n",
        "    (\"so bad\", 0),\n",
        "    (\"i love it\", 1),\n",
        "    (\"i hate it\", 0),\n",
        "]\n",
        "\n",
        "def tokenize(text, vocab, max_len=8):\n",
        "    \"\"\"Convert text to token IDs.\"\"\"\n",
        "    tokens = text.lower().split()\n",
        "    ids = [vocab.get(t, vocab['<unk>']) for t in tokens]\n",
        "    # Pad or truncate\n",
        "    ids = ids[:max_len]\n",
        "    attention_mask = [1] * len(ids)\n",
        "    ids += [vocab['<pad>']] * (max_len - len(ids))\n",
        "    attention_mask += [0] * (max_len - len(attention_mask))\n",
        "    return ids, attention_mask\n",
        "\n",
        "# Prepare dataset\n",
        "max_len = 8\n",
        "X_train = []\n",
        "masks_train = []\n",
        "y_train = []\n",
        "\n",
        "for text, label in train_data:\n",
        "    ids, mask = tokenize(text, vocab, max_len)\n",
        "    X_train.append(ids)\n",
        "    masks_train.append(mask)\n",
        "    y_train.append(label)\n",
        "\n",
        "X_train = torch.tensor(X_train)\n",
        "masks_train = torch.tensor(masks_train)\n",
        "y_train = torch.tensor(y_train)\n",
        "\n",
        "print(f'\\nğŸ“Š Dataset:')\n",
        "print(f'   Training examples: {len(train_data)}')\n",
        "print(f'   Vocabulary size: {vocab_size}')\n",
        "print(f'   Max sequence length: {max_len}')\n",
        "print(f'   Classes: Negative (0), Positive (1)')\n",
        "\n",
        "# Create model\n",
        "model_config = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'embed_dim': 32,\n",
        "    'num_heads': 2,\n",
        "    'num_layers': 2,\n",
        "    'num_classes': 2,\n",
        "    'max_seq_len': max_len,\n",
        "    'dropout': 0.1,\n",
        "}\n",
        "\n",
        "model = AttentionClassifier(**model_config)\n",
        "\n",
        "print(f'\\nğŸ¤– Model Configuration:')\n",
        "for key, value in model_config.items():\n",
        "    print(f'   {key}: {value}')\n",
        "\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f'   Total parameters: {total_params:,}')\n",
        "\n",
        "# Training\n",
        "print('\\n' + '='*70)\n",
        "print('ğŸ‹ï¸ TRAINING')\n",
        "print('='*70)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 100\n",
        "losses = []\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    logits, _ = model(X_train, masks_train)\n",
        "    loss = criterion(logits, y_train)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "    \n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        acc = (logits.argmax(dim=-1) == y_train).float().mean()\n",
        "        print(f'   Epoch {epoch+1:3d} | Loss: {loss.item():.4f} | Acc: {acc:.2%}')\n",
        "\n",
        "# Plot training curve\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(losses, linewidth=2)\n",
        "plt.xlabel('Epoch', fontweight='bold')\n",
        "plt.ylabel('Loss', fontweight='bold')\n",
        "plt.title('Training Loss', fontweight='bold')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluation\n",
        "print('\\n' + '='*70)\n",
        "print('ğŸ“ˆ EVALUATION')\n",
        "print('='*70)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits, attentions = model(X_train, masks_train)\n",
        "    predictions = logits.argmax(dim=-1)\n",
        "    accuracy = (predictions == y_train).float().mean()\n",
        "\n",
        "print(f'\\n   Final Accuracy: {accuracy:.2%}')\n",
        "\n",
        "# Show some predictions\n",
        "print('\\n   Sample Predictions:')\n",
        "for i in range(min(6, len(train_data))):\n",
        "    text = train_data[i][0]\n",
        "    true_label = \"POS\" if train_data[i][1] == 1 else \"NEG\"\n",
        "    pred_label = \"POS\" if predictions[i] == 1 else \"NEG\"\n",
        "    match = \"âœ…\" if true_label == pred_label else \"âŒ\"\n",
        "    print(f'   \"{text:<25}\" â†’ Pred: {pred_label}, True: {true_label} {match}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizing Learned Attention Patterns\n",
        "\n",
        "print('='*70)\n",
        "print('ğŸ” VISUALIZING LEARNED ATTENTION')\n",
        "print('='*70)\n",
        "\n",
        "def visualize_model_attention(model, text, vocab, max_len=8):\n",
        "    \"\"\"Visualize attention patterns for a given input.\"\"\"\n",
        "    # Tokenize\n",
        "    ids, mask = tokenize(text, vocab, max_len)\n",
        "    input_ids = torch.tensor([ids])\n",
        "    attention_mask = torch.tensor([mask])\n",
        "    \n",
        "    # Get tokens for labels\n",
        "    inv_vocab = {v: k for k, v in vocab.items()}\n",
        "    tokens = [inv_vocab.get(i, '<unk>') for i in ids]\n",
        "    actual_len = sum(mask)\n",
        "    tokens = tokens[:actual_len]\n",
        "    \n",
        "    # Forward pass\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        logits, attentions = model(input_ids, attention_mask)\n",
        "    \n",
        "    pred = logits.argmax(dim=-1).item()\n",
        "    pred_label = \"POSITIVE ğŸ˜Š\" if pred == 1 else \"NEGATIVE ğŸ˜\"\n",
        "    \n",
        "    print(f'\\nInput: \"{text}\"')\n",
        "    print(f'Prediction: {pred_label}')\n",
        "    print(f'Confidence: {torch.softmax(logits, dim=-1)[0].max():.2%}')\n",
        "    \n",
        "    # Visualize attention from each layer\n",
        "    num_layers = len(attentions)\n",
        "    num_heads = attentions[0].shape[1]\n",
        "    \n",
        "    fig, axes = plt.subplots(num_layers, num_heads, \n",
        "                             figsize=(4*num_heads, 3.5*num_layers))\n",
        "    \n",
        "    if num_layers == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "    \n",
        "    for layer_idx, attn in enumerate(attentions):\n",
        "        for head_idx in range(num_heads):\n",
        "            ax = axes[layer_idx, head_idx]\n",
        "            \n",
        "            # Get attention for actual tokens only\n",
        "            a = attn[0, head_idx, :actual_len, :actual_len].numpy()\n",
        "            \n",
        "            im = ax.imshow(a, cmap='Blues', vmin=0, vmax=a.max())\n",
        "            ax.set_xticks(range(actual_len))\n",
        "            ax.set_yticks(range(actual_len))\n",
        "            ax.set_xticklabels(tokens, rotation=45, ha='right', fontsize=9)\n",
        "            ax.set_yticklabels(tokens, fontsize=9)\n",
        "            ax.set_title(f'Layer {layer_idx+1}, Head {head_idx+1}', fontsize=10)\n",
        "            \n",
        "            # Add values\n",
        "            for i in range(actual_len):\n",
        "                for j in range(actual_len):\n",
        "                    val = a[i, j]\n",
        "                    color = 'white' if val > 0.5 else 'black'\n",
        "                    ax.text(j, i, f'{val:.2f}', ha='center', va='center', \n",
        "                            fontsize=8, color=color)\n",
        "    \n",
        "    plt.suptitle(f'Learned Attention Patterns\\n\"{text}\" â†’ {pred_label}', \n",
        "                 fontsize=14, fontweight='bold', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Visualize attention for different inputs\n",
        "test_sentences = [\n",
        "    \"i love this movie\",\n",
        "    \"i hate this movie\",\n",
        "    \"really good\",\n",
        "    \"really bad\",\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    visualize_model_attention(model, sentence, vocab, max_len)\n",
        "\n",
        "print('\\n' + '='*70)\n",
        "print('ğŸ’¡ WHAT TO OBSERVE')\n",
        "print('='*70)\n",
        "print('''\n",
        "1. Sentiment words (love/hate, good/bad) often get high self-attention\n",
        "2. Different heads may focus on different patterns:\n",
        "   - Some heads: attend to neighboring words\n",
        "   - Some heads: attend to sentiment words\n",
        "   - Some heads: broader attention patterns\n",
        "3. After training, the model learns which words matter for classification!\n",
        "\n",
        "This is the power of attention: learning WHAT to focus on automatically.\n",
        "''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-11\"></a>\n",
        "# Part 11: Summary, Common Mistakes & Next Steps â±ï¸ ~10 min\n",
        "\n",
        "## ğŸ“Š Complete Comparison of Attention Mechanisms\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                        ATTENTION MECHANISMS COMPARISON                              â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚   Mechanism   â”‚    Time      â”‚    Space     â”‚  Best For   â”‚       Used In          â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚ Self-Attn     â”‚ O(nÂ²)        â”‚ O(nÂ²)        â”‚ Learning    â”‚ Educational            â”‚\n",
        "â”‚ MHSA          â”‚ O(nÂ²)        â”‚ O(hÃ—nÂ²)      â”‚ Rich ctx    â”‚ BERT, GPT-2            â”‚\n",
        "â”‚ GQA           â”‚ O(nÂ²)        â”‚ O(nÂ²/g)      â”‚ Memory      â”‚ LLaMA-2, Mistral       â”‚\n",
        "â”‚ MQA           â”‚ O(nÂ²)        â”‚ O(nÂ²)        â”‚ Inference   â”‚ PaLM, Falcon           â”‚\n",
        "â”‚ MLA           â”‚ O(nÂ²)        â”‚ O(nÃ—d_c)     â”‚ Long ctx    â”‚ DeepSeek-V2/V3         â”‚\n",
        "â”‚ SWA           â”‚ O(nÃ—w)       â”‚ O(nÃ—w)       â”‚ Long docs   â”‚ Longformer, Mistral    â”‚\n",
        "â”‚ Cross-Attn    â”‚ O(nÃ—m)       â”‚ O(nÃ—m)       â”‚ Enc-Dec     â”‚ T5, BART, Whisper      â”‚\n",
        "â”‚ Flash Attn    â”‚ O(nÂ²)        â”‚ O(n)         â”‚ GPU speed   â”‚ All modern LLMs        â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "n = sequence length, h = heads, g = group size, w = window, m = encoder length\n",
        "d_c = MLA compression dimension (much smaller than hÃ—d_h)\n",
        "```\n",
        "\n",
        "## âš ï¸ Common Mistakes & Gotchas\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                    COMMON ATTENTION MISTAKES                            â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  1. FORGETTING TO SCALE                                                â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                â”‚\n",
        "â”‚  âŒ scores = Q @ K.T                                                   â”‚\n",
        "â”‚  âœ… scores = Q @ K.T / sqrt(d_k)                                       â”‚\n",
        "â”‚     â†’ Without scaling, softmax saturates = no gradients!               â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  2. WRONG DIMENSION FOR SOFTMAX                                        â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚\n",
        "â”‚  âŒ attn = softmax(scores, dim=0)                                      â”‚\n",
        "â”‚  âœ… attn = softmax(scores, dim=-1)                                     â”‚\n",
        "â”‚     â†’ Must be over keys (last dim), not queries!                       â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  3. MASK APPLIED AFTER SOFTMAX                                         â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚\n",
        "â”‚  âŒ attn = softmax(scores) * mask                                      â”‚\n",
        "â”‚  âœ… scores = scores.masked_fill(mask==0, -inf); attn = softmax(scores) â”‚\n",
        "â”‚     â†’ Masking after softmax doesn't zero out properly!                 â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  4. FORGETTING TRANSPOSE FOR K                                         â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                          â”‚\n",
        "â”‚  âŒ scores = Q @ K                                                     â”‚\n",
        "â”‚  âœ… scores = Q @ K.transpose(-2, -1)                                   â”‚\n",
        "â”‚     â†’ Dimensions won't match without transpose!                        â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  5. WRONG HEAD RESHAPING                                               â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚\n",
        "â”‚  âŒ Q.view(batch, heads, seq, head_dim)                                â”‚\n",
        "â”‚  âœ… Q.view(batch, seq, heads, head_dim).transpose(1, 2)                â”‚\n",
        "â”‚     â†’ Order matters: split THEN transpose!                             â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  6. NOT USING EVAL() FOR INFERENCE                                     â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                      â”‚\n",
        "â”‚  âŒ output = model(x)  # With dropout active!                          â”‚\n",
        "â”‚  âœ… model.eval(); output = model(x)                                    â”‚\n",
        "â”‚     â†’ Dropout changes output during training mode!                     â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚  7. CAUSAL MASK SHAPE WRONG                                            â”‚\n",
        "â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                             â”‚\n",
        "â”‚  âŒ mask = torch.triu(ones)  # Upper triangular                        â”‚\n",
        "â”‚  âœ… mask = torch.tril(ones)  # Lower triangular                        â”‚\n",
        "â”‚     â†’ Causal = see PAST, so LOWER triangle!                            â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## ğŸ“š Glossary\n",
        "\n",
        "| Term | Definition |\n",
        "|------|------------|\n",
        "| **Query (Q)** | \"What am I looking for?\" - What each token seeks |\n",
        "| **Key (K)** | \"What do I offer?\" - What each token advertises |\n",
        "| **Value (V)** | \"What information do I have?\" - Content to aggregate |\n",
        "| **Attention Score** | Unnormalized similarity between Q and K |\n",
        "| **Attention Weight** | Normalized probability (after softmax) |\n",
        "| **Head** | One attention mechanism; multi-head = multiple in parallel |\n",
        "| **Causal Mask** | Prevents attending to future tokens (for generation) |\n",
        "| **Padding Mask** | Prevents attending to PAD tokens |\n",
        "| **KV Cache** | Stored past K,V for efficient autoregressive generation |\n",
        "| **MLA** | Multi-head Latent Attention - low-rank KV compression (DeepSeek) |\n",
        "| **Latent Vector (c_KV)** | Compressed representation of K and V in MLA |\n",
        "| **Decoupled RoPE** | Separating content and position for RoPE compatibility |\n",
        "| **Absorption Trick** | Pre-computing W_Q @ W_UK^T to avoid materializing keys |\n",
        "| **Flash Attention** | Memory-efficient attention that avoids materializing full matrix |\n",
        "| **RoPE** | Rotary Position Embedding - encodes position via rotation |\n",
        "| **ALiBi** | Attention with Linear Biases - adds distance penalty |\n",
        "| **Pre-Norm** | Layer norm before sublayer (modern, more stable) |\n",
        "| **Post-Norm** | Layer norm after sublayer (original Transformer) |\n",
        "\n",
        "## ğŸ“– Papers to Read (In Order)\n",
        "\n",
        "| # | Paper | Year | Key Contribution |\n",
        "|---|-------|------|------------------|\n",
        "| 1 | \"Attention Is All You Need\" | 2017 | The Transformer |\n",
        "| 2 | \"BERT\" | 2018 | Bidirectional pretraining |\n",
        "| 3 | \"GPT-2\" | 2019 | Autoregressive generation |\n",
        "| 4 | \"RoBERTa\" | 2019 | Better BERT training |\n",
        "| 5 | \"Longformer\" | 2020 | Sliding window attention |\n",
        "| 6 | \"GPT-3\" | 2020 | Scaling laws |\n",
        "| 7 | \"FlashAttention\" | 2022 | Memory-efficient attention |\n",
        "| 8 | \"LLaMA\" | 2023 | Efficient LLM architecture |\n",
        "| 9 | \"Mistral\" | 2023 | GQA + SWA combined |\n",
        "| 10 | \"DeepSeek-V2\" | 2024 | Multi-head Latent Attention (MLA) |\n",
        "| 11 | \"DeepSeek-V3\" | 2024 | MLA + MoE at scale |\n",
        "\n",
        "## ğŸš€ What to Build Next\n",
        "\n",
        "```\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                      YOUR LEARNING PATH                                 â”‚\n",
        "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   BEGINNER (You are here! âœ…)                                          â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                         â”‚\n",
        "â”‚   [x] Understand attention mechanisms                                  â”‚\n",
        "â”‚   [x] Implement from scratch                                           â”‚\n",
        "â”‚   [x] Build a simple classifier                                        â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   INTERMEDIATE (Next steps)                                            â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚\n",
        "â”‚   [ ] Fine-tune a pretrained model (BERT, GPT-2)                       â”‚\n",
        "â”‚   [ ] Build a text generation model                                    â”‚\n",
        "â”‚   [ ] Implement beam search decoding                                   â”‚\n",
        "â”‚   [ ] Train on a real dataset (IMDB, SST-2)                           â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   ADVANCED (Future goals)                                              â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                           â”‚\n",
        "â”‚   [ ] Pre-train your own small LLM                                     â”‚\n",
        "â”‚   [ ] Implement RLHF (reinforcement learning from human feedback)      â”‚\n",
        "â”‚   [ ] Build a RAG (retrieval augmented generation) system              â”‚\n",
        "â”‚   [ ] Contribute to open-source LLM projects                           â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "```\n",
        "\n",
        "## ğŸ”§ Useful Resources\n",
        "\n",
        "**Libraries:**\n",
        "- ğŸ¤— Hugging Face Transformers: Pre-trained models\n",
        "- PyTorch: Deep learning framework\n",
        "- einops: Clean tensor operations\n",
        "- FlashAttention: Efficient attention library\n",
        "\n",
        "**Courses:**\n",
        "- Stanford CS224N: NLP with Deep Learning\n",
        "- fast.ai: Practical Deep Learning\n",
        "- Andrej Karpathy's lectures: Building GPT from scratch\n",
        "\n",
        "**Communities:**\n",
        "- r/MachineLearning\n",
        "- Hugging Face Discord\n",
        "- EleutherAI Discord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FINAL CELEBRATION & SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print('='*70)\n",
        "print('ğŸ‰ CONGRATULATIONS! YOU\\'VE COMPLETED THE ATTENTION TUTORIAL! ğŸ‰')\n",
        "print('='*70)\n",
        "\n",
        "print('''\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   ğŸ† WHAT YOU MASTERED                                                 â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                 â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   Part 0:  âœ… Tensors, embeddings, and PyTorch basics                  â”‚\n",
        "â”‚   Part 1:  âœ… Dot products (algebraic and geometric)                   â”‚\n",
        "â”‚   Part 2:  âœ… Self-Attention (Q, K, V mechanism)                       â”‚\n",
        "â”‚            âœ… Causal masking for autoregressive models                 â”‚\n",
        "â”‚            âœ… Positional encodings (sinusoidal, RoPE, ALiBi)           â”‚\n",
        "â”‚            âœ… Padding masks for batched sequences                      â”‚\n",
        "â”‚   Part 3:  âœ… Multi-Head Self-Attention                                â”‚\n",
        "â”‚            âœ… Head visualization and interpretation                    â”‚\n",
        "â”‚            âœ… Attention dropout                                        â”‚\n",
        "â”‚   Part 4:  âœ… Grouped Query Attention (GQA)                            â”‚\n",
        "â”‚            âœ… Memory efficiency benchmarking                           â”‚\n",
        "â”‚   Part 5:  âœ… Sliding Window Attention (SWA)                           â”‚\n",
        "â”‚   Part 6:  âœ… Cross-Attention for encoder-decoder                      â”‚\n",
        "â”‚   Part 7:  âœ… Flash Attention, RoPE, ALiBi, KV Caching                 â”‚\n",
        "â”‚   Part 8:  âœ… Complete Transformer Block                               â”‚\n",
        "â”‚   Part 9:  âœ… End-to-End Sentiment Classifier                          â”‚\n",
        "â”‚   Part 10: âœ… Common mistakes, glossary, next steps                    â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â”‚   ğŸ“Š STATISTICS                                                        â”‚\n",
        "â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                        â”‚\n",
        "â”‚   â€¢ 100+ cells of content                                              â”‚\n",
        "â”‚   â€¢ 20+ exercises with solutions                                       â”‚\n",
        "â”‚   â€¢ 15+ ASCII diagrams                                                 â”‚\n",
        "â”‚   â€¢ 1 complete end-to-end project                                      â”‚\n",
        "â”‚   â€¢ Estimated time: 4-5 hours                                          â”‚\n",
        "â”‚                                                                         â”‚\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "\n",
        "''')\n",
        "\n",
        "print('ğŸš€ YOU ARE NOW READY TO:')\n",
        "print('   â€¢ Read and understand Transformer papers')\n",
        "print('   â€¢ Implement attention mechanisms from scratch')\n",
        "print('   â€¢ Debug attention-related issues in code')\n",
        "print('   â€¢ Choose the right attention variant for your use case')\n",
        "print('   â€¢ Build and train your own Transformer models')\n",
        "print('   â€¢ Fine-tune pretrained LLMs like BERT, GPT, or LLaMA')\n",
        "print()\n",
        "print('ğŸ’ª The attention mechanism you\\'ve mastered powers:')\n",
        "print('   â€¢ ChatGPT, Claude, Gemini, LLaMA')\n",
        "print('   â€¢ BERT, RoBERTa, T5')\n",
        "print('   â€¢ Stable Diffusion, DALL-E')\n",
        "print('   â€¢ And virtually all modern AI!')\n",
        "print()\n",
        "print('='*70)\n",
        "print('ğŸ‘ THANK YOU FOR YOUR DEDICATION TO LEARNING!')\n",
        "print('   Go build something amazing! ğŸŒŸ')\n",
        "print('='*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"part-12\"></a>\n",
        "# Part 12: Advanced Topics (Bonus) â±ï¸ ~30 min\n",
        "\n",
        "This section covers cutting-edge attention variants and optimization techniques used in production LLMs.\n",
        "\n",
        "## Section 12.1: Sparse Attention Patterns\n",
        "\n",
        "### ğŸ¯ Beyond Dense Attention\n",
        "\n",
        "Full attention is O(nÂ²), but many real-world patterns don't need all connections!\n",
        "\n",
        "```\n",
        "FULL ATTENTION (O(nÂ²))          SPARSE ATTENTION (O(nâˆšn) or O(n log n))\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "â–  â–  â–  â–  â–  â–  â–  â–                  â–  â–  â–  . . . . â–   â† Local + global\n",
        "â–  â–  â–  â–  â–  â–  â–  â–                  â–  â–  â–  â–  . . . â– \n",
        "â–  â–  â–  â–  â–  â–  â–  â–                  â–  â–  â–  â–  â–  . . â– \n",
        "â–  â–  â–  â–  â–  â–  â–  â–      â†’          . â–  â–  â–  â–  â–  . â–   â† Sliding window\n",
        "â–  â–  â–  â–  â–  â–  â–  â–                  . . â–  â–  â–  â–  â–  â– \n",
        "â–  â–  â–  â–  â–  â–  â–  â–                  . . . â–  â–  â–  â–  â– \n",
        "â–  â–  â–  â–  â–  â–  â–  â–                  . . . . â–  â–  â–  â– \n",
        "â–  â–  â–  â–  â–  â–  â–  â–                  â–  â–  â–  â–  â–  â–  â–  â–   â† Global token\n",
        "```\n",
        "\n",
        "### ğŸ“Š Sparse Attention Patterns\n",
        "\n",
        "| Pattern | Complexity | Key Idea | Used In |\n",
        "|---------|------------|----------|---------|\n",
        "| **Longformer** | O(n) | Local + global tokens | Long documents |\n",
        "| **BigBird** | O(n) | Local + global + random | Scientific papers |\n",
        "| **Sparse Transformer** | O(nâˆšn) | Strided + local | OpenAI research |\n",
        "| **Block Sparse** | O(nÃ—b) | Block-level attention | Efficient GPU |\n",
        "\n",
        "### ğŸ’¡ When to Use\n",
        "\n",
        "- **Long documents** (books, papers): Longformer/BigBird\n",
        "- **Memory constrained**: Any sparse variant\n",
        "- **Research/experimentation**: Sparse Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sparse Attention Patterns Visualization\n",
        "\n",
        "def create_longformer_pattern(seq_len, window_size=2, global_tokens=[0]):\n",
        "    \"\"\"Create Longformer-style attention pattern.\"\"\"\n",
        "    pattern = torch.zeros(seq_len, seq_len)\n",
        "    \n",
        "    # Local sliding window\n",
        "    for i in range(seq_len):\n",
        "        for j in range(max(0, i - window_size), min(seq_len, i + window_size + 1)):\n",
        "            pattern[i, j] = 1\n",
        "    \n",
        "    # Global tokens (can attend to/from all positions)\n",
        "    for g in global_tokens:\n",
        "        pattern[g, :] = 1  # Global token attends to all\n",
        "        pattern[:, g] = 1  # All attend to global token\n",
        "    \n",
        "    return pattern\n",
        "\n",
        "def create_bigbird_pattern(seq_len, window_size=2, global_tokens=[0], num_random=2):\n",
        "    \"\"\"Create BigBird-style attention pattern (local + global + random).\"\"\"\n",
        "    pattern = create_longformer_pattern(seq_len, window_size, global_tokens)\n",
        "    \n",
        "    # Add random attention (simplified)\n",
        "    for i in range(seq_len):\n",
        "        # Each position randomly attends to num_random positions\n",
        "        random_positions = torch.randperm(seq_len)[:num_random]\n",
        "        for j in random_positions:\n",
        "            pattern[i, j] = 1\n",
        "    \n",
        "    return pattern\n",
        "\n",
        "# Visualize different patterns\n",
        "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
        "\n",
        "seq_len = 16\n",
        "\n",
        "# Full attention\n",
        "axes[0].imshow(torch.ones(seq_len, seq_len).numpy(), cmap='Blues', vmin=0, vmax=1)\n",
        "axes[0].set_title('Full Attention\\nO(nÂ²)', fontweight='bold')\n",
        "axes[0].set_xlabel('Key')\n",
        "axes[0].set_ylabel('Query')\n",
        "\n",
        "# Sliding window\n",
        "window_pattern = create_longformer_pattern(seq_len, window_size=2, global_tokens=[])\n",
        "axes[1].imshow(window_pattern.numpy(), cmap='Blues', vmin=0, vmax=1)\n",
        "axes[1].set_title('Sliding Window\\nO(nÃ—w)', fontweight='bold')\n",
        "axes[1].set_xlabel('Key')\n",
        "axes[1].set_ylabel('Query')\n",
        "\n",
        "# Longformer\n",
        "longformer_pattern = create_longformer_pattern(seq_len, window_size=2, global_tokens=[0, seq_len-1])\n",
        "axes[2].imshow(longformer_pattern.numpy(), cmap='Blues', vmin=0, vmax=1)\n",
        "axes[2].set_title('Longformer\\n(Local + Global)', fontweight='bold')\n",
        "axes[2].set_xlabel('Key')\n",
        "axes[2].set_ylabel('Query')\n",
        "\n",
        "# BigBird\n",
        "torch.manual_seed(42)\n",
        "bigbird_pattern = create_bigbird_pattern(seq_len, window_size=2, global_tokens=[0], num_random=3)\n",
        "axes[3].imshow(bigbird_pattern.numpy(), cmap='Blues', vmin=0, vmax=1)\n",
        "axes[3].set_title('BigBird\\n(Local + Global + Random)', fontweight='bold')\n",
        "axes[3].set_xlabel('Key')\n",
        "axes[3].set_ylabel('Query')\n",
        "\n",
        "plt.suptitle('Sparse Attention Patterns Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Count connections\n",
        "print('\\nğŸ“Š Connection Counts (n=16):')\n",
        "print(f'   Full:      {seq_len**2} connections')\n",
        "print(f'   Window:    {int(window_pattern.sum())} connections')\n",
        "print(f'   Longformer: {int(longformer_pattern.sum())} connections')\n",
        "print(f'   BigBird:   {int(bigbird_pattern.sum())} connections')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 12.2: Linear Attention\n",
        "\n",
        "### ğŸ¯ O(n) Attention?!\n",
        "\n",
        "The quadratic bottleneck comes from computing the full attention matrix:\n",
        "\n",
        "$$\n",
        "\\text{Attention} = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
        "$$\n",
        "\n",
        "**Key insight**: If we can rewrite this without computing the full $n \\times n$ matrix, we get linear time!\n",
        "\n",
        "### ğŸ”¢ The Linear Attention Trick\n",
        "\n",
        "```\n",
        "Standard:   softmax(QK^T) V = [nÃ—d] Ã— [dÃ—n] Ã— [nÃ—d] = O(nÂ²d)\n",
        "                      â†‘\n",
        "                  nÃ—n matrix\n",
        "\n",
        "Linear:     Q Ã— (K^T V) = [nÃ—d] Ã— ([dÃ—n] Ã— [nÃ—d]) = O(ndÂ²)\n",
        "                              â†‘\n",
        "                          dÃ—d matrix (d << n!)\n",
        "```\n",
        "\n",
        "### ğŸ“Š Linear Attention Variants\n",
        "\n",
        "| Variant | Key Innovation | Trade-off |\n",
        "|---------|---------------|-----------|\n",
        "| **Performer** | Random feature approximation | Quality loss |\n",
        "| **Linear Transformer** | ELU+1 kernel | No softmax |\n",
        "| **RWKV** | Recurrent formulation | Strong results |\n",
        "| **Mamba** | Selective state spaces | SOTA efficiency |\n",
        "\n",
        "### âš ï¸ Trade-offs\n",
        "\n",
        "- âœ… O(n) complexity - handles very long sequences\n",
        "- âŒ Often lower quality than full attention\n",
        "- âŒ Less interpretable (no clear attention pattern)\n",
        "- âš ï¸ Active research area (Mamba is promising!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple Linear Attention Implementation\n",
        "\n",
        "class LinearAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple linear attention using ELU+1 as the kernel function.\n",
        "    \n",
        "    Instead of softmax(QK^T)V, we compute:\n",
        "    Q' = elu(Q) + 1\n",
        "    K' = elu(K) + 1\n",
        "    Output = Q' @ (K'^T @ V) / (Q' @ K'^T.sum())\n",
        "    \n",
        "    This is O(ndÂ²) instead of O(nÂ²d)!\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim: int):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        self.W_o = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "    \n",
        "    def feature_map(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"ELU+1 feature map (ensures positivity).\"\"\"\n",
        "        return F.elu(x) + 1\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        \n",
        "        Q = self.feature_map(self.W_q(x))  # (batch, seq, d)\n",
        "        K = self.feature_map(self.W_k(x))  # (batch, seq, d)\n",
        "        V = self.W_v(x)  # (batch, seq, d)\n",
        "        \n",
        "        # Key insight: compute K^T @ V first! This is (d Ã— d), not (n Ã— n)\n",
        "        KV = torch.bmm(K.transpose(1, 2), V)  # (batch, d, d)\n",
        "        \n",
        "        # Normalization factor\n",
        "        K_sum = K.sum(dim=1, keepdim=True)  # (batch, 1, d)\n",
        "        \n",
        "        # Output\n",
        "        numerator = torch.bmm(Q, KV)  # (batch, seq, d)\n",
        "        denominator = torch.bmm(Q, K_sum.transpose(1, 2)) + 1e-6  # (batch, seq, 1)\n",
        "        \n",
        "        output = numerator / denominator\n",
        "        return self.W_o(output)\n",
        "\n",
        "# Test linear attention\n",
        "print('='*70)\n",
        "print('LINEAR ATTENTION DEMONSTRATION')\n",
        "print('='*70)\n",
        "\n",
        "linear_attn = LinearAttention(embed_dim=64)\n",
        "\n",
        "# Test with different sequence lengths\n",
        "for seq_len in [100, 1000, 10000]:\n",
        "    x = torch.randn(1, seq_len, 64)\n",
        "    \n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        _ = linear_attn(x)\n",
        "    elapsed = (time.time() - start) * 1000\n",
        "    \n",
        "    print(f'Seq len {seq_len:>6}: {elapsed:.2f} ms')\n",
        "\n",
        "print('\\nğŸ’¡ Key Insight: Time grows LINEARLY with sequence length!')\n",
        "print('   Compare to O(nÂ²) for standard attention.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 12.3: Mixture of Experts (MoE)\n",
        "\n",
        "### ğŸ¯ Scaling Without Proportional Compute\n",
        "\n",
        "**Problem**: Larger models = better, but also MUCH more expensive!\n",
        "\n",
        "**Solution**: Only activate SOME parameters for each input!\n",
        "\n",
        "```\n",
        "DENSE MODEL                        MIXTURE OF EXPERTS (MoE)\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Input                              Input\n",
        "  â”‚                                  â”‚\n",
        "  â–¼                                  â–¼\n",
        "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
        "â”‚   All FFN   â”‚ â† 100% compute    â”‚    Router    â”‚ â† Chooses experts\n",
        "â”‚  parameters â”‚                   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜\n",
        "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚\n",
        "                                   â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”\n",
        "                                   â–¼     â–¼     â–¼\n",
        "                               [Expert] [Expert] [Expert]\n",
        "                                  1       2     ...N\n",
        "                                   â”‚     \n",
        "                                   â–¼     \n",
        "                              Only 1-2 activated!\n",
        "                              (e.g., 2 of 8 experts)\n",
        "```\n",
        "\n",
        "### ğŸ“Š MoE Architecture Details\n",
        "\n",
        "| Component | Description | Example |\n",
        "|-----------|-------------|---------|\n",
        "| **Experts** | Independent FFN networks | 8-64 experts |\n",
        "| **Router** | Decides which experts to use | Top-k gating |\n",
        "| **Capacity** | Max tokens per expert | Load balancing |\n",
        "\n",
        "### ğŸ”¢ The Router\n",
        "\n",
        "```python\n",
        "# Simplified router\n",
        "router_logits = input @ router_weights  # (batch, seq, num_experts)\n",
        "expert_weights = softmax(router_logits)  # Soft assignment\n",
        "top_k_experts = topk(expert_weights, k=2)  # Select top 2\n",
        "```\n",
        "\n",
        "### ğŸ’¡ MoE in Production\n",
        "\n",
        "| Model | Total Params | Active Params | Experts |\n",
        "|-------|-------------|---------------|---------|\n",
        "| Mixtral 8x7B | 47B | 12B | 8 experts |\n",
        "| DeepSeek-V2 | 236B | 21B | 160 experts |\n",
        "| GPT-4 (rumored) | 1.8T | ~110B | 16 experts |\n",
        "\n",
        "**Key Insight**: 47B params but only 12B compute per token!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simplified Mixture of Experts Implementation\n",
        "\n",
        "class SimpleMoE(nn.Module):\n",
        "    \"\"\"\n",
        "    Simplified Mixture of Experts layer.\n",
        "    \n",
        "    Each token is routed to top-k experts based on learned gating.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim: int, hidden_dim: int, num_experts: int, top_k: int = 2):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        \n",
        "        # Router: decides which experts to use\n",
        "        self.router = nn.Linear(embed_dim, num_experts, bias=False)\n",
        "        \n",
        "        # Experts: independent FFN networks\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(embed_dim, hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Linear(hidden_dim, embed_dim)\n",
        "            )\n",
        "            for _ in range(num_experts)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, dict]:\n",
        "        batch_size, seq_len, embed_dim = x.shape\n",
        "        \n",
        "        # Get router probabilities\n",
        "        router_logits = self.router(x)  # (batch, seq, num_experts)\n",
        "        router_probs = F.softmax(router_logits, dim=-1)\n",
        "        \n",
        "        # Select top-k experts\n",
        "        top_k_probs, top_k_indices = torch.topk(router_probs, self.top_k, dim=-1)\n",
        "        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)  # Renormalize\n",
        "        \n",
        "        # Compute weighted expert outputs\n",
        "        output = torch.zeros_like(x)\n",
        "        for k in range(self.top_k):\n",
        "            expert_idx = top_k_indices[:, :, k]  # (batch, seq)\n",
        "            weight = top_k_probs[:, :, k].unsqueeze(-1)  # (batch, seq, 1)\n",
        "            \n",
        "            # For each expert, process tokens assigned to it\n",
        "            for e in range(self.num_experts):\n",
        "                mask = (expert_idx == e)  # (batch, seq)\n",
        "                if mask.any():\n",
        "                    expert_input = x * mask.unsqueeze(-1).float()\n",
        "                    expert_output = self.experts[e](expert_input)\n",
        "                    output = output + expert_output * weight * mask.unsqueeze(-1).float()\n",
        "        \n",
        "        # Statistics\n",
        "        expert_usage = torch.bincount(\n",
        "            top_k_indices.reshape(-1), \n",
        "            minlength=self.num_experts\n",
        "        ).float() / (batch_size * seq_len * self.top_k)\n",
        "        \n",
        "        stats = {\n",
        "            'expert_usage': expert_usage,\n",
        "            'router_entropy': -(router_probs * torch.log(router_probs + 1e-10)).sum(dim=-1).mean()\n",
        "        }\n",
        "        \n",
        "        return output, stats\n",
        "\n",
        "# Test MoE\n",
        "print('='*70)\n",
        "print('MIXTURE OF EXPERTS DEMONSTRATION')\n",
        "print('='*70)\n",
        "\n",
        "moe = SimpleMoE(embed_dim=64, hidden_dim=256, num_experts=8, top_k=2)\n",
        "x = torch.randn(2, 16, 64)\n",
        "\n",
        "output, stats = moe(x)\n",
        "\n",
        "print(f'\\nInput shape:  {x.shape}')\n",
        "print(f'Output shape: {output.shape}')\n",
        "print(f'\\nğŸ“Š Expert Usage (should be ~equal for load balance):')\n",
        "for i, usage in enumerate(stats['expert_usage']):\n",
        "    bar = 'â–ˆ' * int(usage * 50)\n",
        "    print(f'   Expert {i}: {usage:.2%} {bar}')\n",
        "\n",
        "print(f'\\nğŸ² Router Entropy: {stats[\"router_entropy\"]:.3f}')\n",
        "print('   (Higher = more uniform routing, Lower = more specialized)')\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in moe.parameters())\n",
        "active_params = sum(p.numel() for p in moe.experts[0].parameters()) * 2 + \\\n",
        "                sum(p.numel() for p in [moe.router.weight])\n",
        "print(f'\\nğŸ“ Total parameters:  {total_params:,}')\n",
        "print(f'   Active per token: {active_params:,} ({100*active_params/total_params:.1f}%)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 12.4: Speculative Decoding\n",
        "\n",
        "### ğŸ¯ Faster Inference Without Quality Loss\n",
        "\n",
        "**Problem**: Autoregressive generation is inherently sequential!\n",
        "\n",
        "```\n",
        "Standard Generation (SLOW):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\"The\" â†’ Model â†’ \"cat\" â†’ Model â†’ \"sat\" â†’ Model â†’ \"on\" â†’ ...\n",
        "                 â†‘              â†‘              â†‘\n",
        "             Full forward   Full forward   Full forward\n",
        "             \n",
        "Each token requires a FULL model forward pass!\n",
        "```\n",
        "\n",
        "**Solution**: Draft with a small model, verify with the large model!\n",
        "\n",
        "```\n",
        "Speculative Decoding (FAST):\n",
        "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "Step 1: Small model drafts k tokens\n",
        "        \"The\" â†’ Draft Model â†’ \"cat sat on the\"  (fast!)\n",
        "        \n",
        "Step 2: Large model verifies in PARALLEL\n",
        "        [\"cat\", \"sat\", \"on\", \"the\"] â†’ Large Model â†’ verify all at once!\n",
        "        \n",
        "Step 3: Accept correct tokens, reject wrong ones\n",
        "        \"cat\" âœ…  \"sat\" âœ…  \"on\" âŒ  â†’ reject \"on\" and later\n",
        "        \n",
        "Result: Got 2 tokens with only 1 large model call!\n",
        "```\n",
        "\n",
        "### ğŸ“Š Why This Works\n",
        "\n",
        "| Aspect | Traditional | Speculative |\n",
        "|--------|-------------|-------------|\n",
        "| Draft | None | Fast small model |\n",
        "| Verify | 1 token at a time | k tokens parallel |\n",
        "| Speedup | Baseline | 2-3x typical |\n",
        "| Quality | Exact | **Exact** (no approximation!) |\n",
        "\n",
        "### ğŸ”‘ Key Insight\n",
        "\n",
        "The large model can verify k tokens **in parallel** because:\n",
        "- Verification is just computing probabilities\n",
        "- We already have the draft tokens\n",
        "- One forward pass gives all k probabilities!\n",
        "\n",
        "### ğŸ’¡ Production Usage\n",
        "\n",
        "- **Anthropic Claude**: Uses speculative decoding\n",
        "- **Google Gemini**: Uses speculative decoding  \n",
        "- **DeepSeek**: Tree-based speculative decoding\n",
        "- **Medusa**: Multi-head prediction for drafting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Speculative Decoding Simulation\n",
        "\n",
        "def simulate_speculative_decoding(num_tokens: int, draft_length: int = 4, \n",
        "                                   acceptance_rate: float = 0.7):\n",
        "    \"\"\"\n",
        "    Simulate speculative decoding speedup.\n",
        "    \n",
        "    Args:\n",
        "        num_tokens: Total tokens to generate\n",
        "        draft_length: Number of tokens drafted per step\n",
        "        acceptance_rate: Probability each draft token is accepted\n",
        "    \"\"\"\n",
        "    # Standard generation\n",
        "    standard_calls = num_tokens\n",
        "    \n",
        "    # Speculative generation\n",
        "    speculative_calls = 0\n",
        "    tokens_generated = 0\n",
        "    \n",
        "    while tokens_generated < num_tokens:\n",
        "        # Draft k tokens (assumed instant for simulation)\n",
        "        draft = draft_length\n",
        "        \n",
        "        # Verify with large model (1 call)\n",
        "        speculative_calls += 1\n",
        "        \n",
        "        # Accept tokens based on acceptance rate\n",
        "        accepted = 0\n",
        "        for _ in range(draft):\n",
        "            if torch.rand(1).item() < acceptance_rate:\n",
        "                accepted += 1\n",
        "            else:\n",
        "                accepted += 1  # Accept up to and including first rejection\n",
        "                break\n",
        "        \n",
        "        tokens_generated += accepted\n",
        "    \n",
        "    speedup = standard_calls / speculative_calls\n",
        "    return {\n",
        "        'standard_calls': standard_calls,\n",
        "        'speculative_calls': speculative_calls,\n",
        "        'speedup': speedup\n",
        "    }\n",
        "\n",
        "print('='*70)\n",
        "print('SPECULATIVE DECODING SPEEDUP SIMULATION')\n",
        "print('='*70)\n",
        "\n",
        "# Simulate different scenarios\n",
        "scenarios = [\n",
        "    {'draft_length': 4, 'acceptance_rate': 0.5, 'label': 'Conservative (50% accept)'},\n",
        "    {'draft_length': 4, 'acceptance_rate': 0.7, 'label': 'Typical (70% accept)'},\n",
        "    {'draft_length': 4, 'acceptance_rate': 0.9, 'label': 'Aggressive (90% accept)'},\n",
        "    {'draft_length': 8, 'acceptance_rate': 0.7, 'label': 'Long draft (k=8, 70%)'},\n",
        "]\n",
        "\n",
        "num_tokens = 100\n",
        "results = []\n",
        "\n",
        "for scenario in scenarios:\n",
        "    result = simulate_speculative_decoding(\n",
        "        num_tokens=num_tokens,\n",
        "        draft_length=scenario['draft_length'],\n",
        "        acceptance_rate=scenario['acceptance_rate']\n",
        "    )\n",
        "    result['label'] = scenario['label']\n",
        "    results.append(result)\n",
        "\n",
        "print(f'\\nGenerating {num_tokens} tokens:\\n')\n",
        "print(f'{\"Scenario\":<30} {\"Standard\":<12} {\"Speculative\":<14} {\"Speedup\":<10}')\n",
        "print('-' * 66)\n",
        "for r in results:\n",
        "    print(f'{r[\"label\"]:<30} {r[\"standard_calls\"]:<12} {r[\"speculative_calls\"]:<14} {r[\"speedup\"]:.2f}x')\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
        "labels = [r['label'] for r in results]\n",
        "speedups = [r['speedup'] for r in results]\n",
        "\n",
        "bars = ax.bar(range(len(labels)), speedups, color=['#3498db', '#2ecc71', '#e74c3c', '#9b59b6'])\n",
        "ax.set_xticks(range(len(labels)))\n",
        "ax.set_xticklabels(labels, rotation=15, ha='right')\n",
        "ax.set_ylabel('Speedup Factor', fontweight='bold')\n",
        "ax.set_title('Speculative Decoding Speedup by Acceptance Rate', fontweight='bold')\n",
        "ax.axhline(y=1.0, color='black', linestyle='--', alpha=0.5, label='Baseline')\n",
        "\n",
        "for bar, speedup in zip(bars, speedups):\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, \n",
        "            f'{speedup:.2f}x', ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('\\nğŸ’¡ Key Insights:')\n",
        "print('   â€¢ Higher acceptance rate = better speedup')\n",
        "print('   â€¢ Speedup depends on how well draft model matches target model')\n",
        "print('   â€¢ No quality loss - rejected tokens are regenerated correctly')\n",
        "print('   â€¢ Works best when draft model is much faster than target model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "<a name=\"cheat-sheet\"></a>\n",
        "# ğŸ“‹ Attention Cheat Sheet (Print-Friendly Reference)\n",
        "\n",
        "## Quick Formula Reference\n",
        "\n",
        "### Basic Attention\n",
        "```python\n",
        "# Scaled Dot-Product Attention\n",
        "scores = Q @ K.T / sqrt(d_k)\n",
        "weights = softmax(scores, dim=-1)\n",
        "output = weights @ V\n",
        "```\n",
        "\n",
        "### Multi-Head Attention\n",
        "```python\n",
        "# Split into heads\n",
        "Q = Q.view(batch, seq, heads, d_k).transpose(1, 2)  # (batch, heads, seq, d_k)\n",
        "K = K.view(batch, seq, heads, d_k).transpose(1, 2)\n",
        "V = V.view(batch, seq, heads, d_k).transpose(1, 2)\n",
        "\n",
        "# Attention per head\n",
        "scores = Q @ K.transpose(-2, -1) / sqrt(d_k)\n",
        "attn = softmax(scores, dim=-1)\n",
        "out = attn @ V\n",
        "\n",
        "# Concatenate heads\n",
        "out = out.transpose(1, 2).reshape(batch, seq, embed_dim)\n",
        "output = W_o(out)\n",
        "```\n",
        "\n",
        "### Causal Mask\n",
        "```python\n",
        "mask = torch.triu(torch.ones(seq, seq), diagonal=1).bool()\n",
        "scores = scores.masked_fill(mask, float('-inf'))\n",
        "```\n",
        "\n",
        "### Padding Mask\n",
        "```python\n",
        "# attention_mask: (batch, seq) with 1=real, 0=padding\n",
        "mask = attention_mask.unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, seq)\n",
        "scores = scores.masked_fill(mask == 0, float('-inf'))\n",
        "```\n",
        "\n",
        "## Position Encodings\n",
        "\n",
        "### Sinusoidal (Fixed)\n",
        "```python\n",
        "pe = torch.zeros(max_len, d_model)\n",
        "position = torch.arange(0, max_len).unsqueeze(1)\n",
        "div_term = torch.exp(torch.arange(0, d_model, 2) * -(log(10000.0) / d_model))\n",
        "pe[:, 0::2] = torch.sin(position * div_term)\n",
        "pe[:, 1::2] = torch.cos(position * div_term)\n",
        "x = x + pe[:seq_len]\n",
        "```\n",
        "\n",
        "### RoPE (Rotary)\n",
        "```python\n",
        "# Apply rotation to Q and K (not V!)\n",
        "cos = precomputed_cos[positions]\n",
        "sin = precomputed_sin[positions]\n",
        "q_rotated = q * cos + rotate_half(q) * sin\n",
        "k_rotated = k * cos + rotate_half(k) * sin\n",
        "```\n",
        "\n",
        "## Memory & Complexity\n",
        "\n",
        "| Mechanism | Time | Space | Parameters |\n",
        "|-----------|------|-------|------------|\n",
        "| Self-Attention | O(nÂ²d) | O(nÂ²) | 4dÂ² |\n",
        "| MHSA | O(nÂ²d) | O(nÂ²h) | 4dÂ² |\n",
        "| GQA (g groups) | O(nÂ²d) | O(nÂ²/g) | 2dÂ² + 2dÂ²/g |\n",
        "| SWA (window w) | O(nwd) | O(nw) | 4dÂ² |\n",
        "| Flash Attention | O(nÂ²d) | O(n) | 4dÂ² |\n",
        "| Linear Attention | O(ndÂ²) | O(nd) | 4dÂ² |\n",
        "\n",
        "## Common Shapes\n",
        "\n",
        "```\n",
        "Input:      (batch, seq_len, embed_dim)\n",
        "Q, K, V:    (batch, seq_len, embed_dim)\n",
        "Scores:     (batch, seq_len, seq_len)  # or (batch, heads, seq, seq)\n",
        "Weights:    (batch, seq_len, seq_len)  # sum to 1 per row\n",
        "Output:     (batch, seq_len, embed_dim)\n",
        "```\n",
        "\n",
        "## Debugging Tips\n",
        "\n",
        "| Issue | Likely Cause | Fix |\n",
        "|-------|--------------|-----|\n",
        "| NaN in output | Missing mask or divide by 0 | Add `+ 1e-9` to denominators |\n",
        "| All attention = uniform | Scale factor wrong | Check `sqrt(d_k)` |\n",
        "| Gradient explosion | Scores too large | Lower learning rate, add LayerNorm |\n",
        "| OOM error | Sequence too long | Use Flash Attention or GQA |\n",
        "| Poor quality | Missing position encoding | Add sinusoidal or RoPE |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ğŸ§ª Unit Tests for All Implementations\n",
        "\n",
        "Run these tests to verify your implementations are correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Unit Tests for Attention Implementations\n",
        "\n",
        "def run_all_tests():\n",
        "    \"\"\"Run all unit tests and report results.\"\"\"\n",
        "    \n",
        "    print('='*70)\n",
        "    print('ğŸ§ª RUNNING COMPREHENSIVE UNIT TESTS')\n",
        "    print('='*70)\n",
        "    \n",
        "    tests_passed = 0\n",
        "    tests_failed = 0\n",
        "    \n",
        "    # Test 1: Attention weights sum to 1\n",
        "    def test_attention_sums_to_one():\n",
        "        Q = torch.randn(2, 8, 64)\n",
        "        K = torch.randn(2, 8, 64)\n",
        "        scores = Q @ K.transpose(-2, -1) / math.sqrt(64)\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        row_sums = weights.sum(dim=-1)\n",
        "        assert torch.allclose(row_sums, torch.ones_like(row_sums), atol=1e-5), \\\n",
        "            f\"Weights should sum to 1, got {row_sums}\"\n",
        "        return True\n",
        "    \n",
        "    # Test 2: Causal mask prevents future attention\n",
        "    def test_causal_mask():\n",
        "        seq_len = 8\n",
        "        mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "        scores = torch.randn(seq_len, seq_len)\n",
        "        masked_scores = scores.masked_fill(mask.bool(), float('-inf'))\n",
        "        weights = F.softmax(masked_scores, dim=-1)\n",
        "        \n",
        "        # Check upper triangle is zero\n",
        "        upper = torch.triu(weights, diagonal=1)\n",
        "        assert torch.allclose(upper, torch.zeros_like(upper), atol=1e-5), \\\n",
        "            \"Causal mask should zero out future positions\"\n",
        "        return True\n",
        "    \n",
        "    # Test 3: Multi-head attention output shape\n",
        "    def test_mhsa_output_shape():\n",
        "        batch, seq, embed = 2, 16, 64\n",
        "        num_heads = 8\n",
        "        \n",
        "        x = torch.randn(batch, seq, embed)\n",
        "        mhsa = nn.MultiheadAttention(embed, num_heads, batch_first=True)\n",
        "        out, _ = mhsa(x, x, x)\n",
        "        \n",
        "        assert out.shape == (batch, seq, embed), \\\n",
        "            f\"MHSA output should be {(batch, seq, embed)}, got {out.shape}\"\n",
        "        return True\n",
        "    \n",
        "    # Test 4: Scaled dot product\n",
        "    def test_scaling():\n",
        "        d_k = 64\n",
        "        Q = torch.randn(1, 8, d_k)\n",
        "        K = torch.randn(1, 8, d_k)\n",
        "        \n",
        "        unscaled = Q @ K.transpose(-2, -1)\n",
        "        scaled = unscaled / math.sqrt(d_k)\n",
        "        \n",
        "        # Scaled should have smaller variance\n",
        "        assert scaled.var() < unscaled.var(), \\\n",
        "            \"Scaling should reduce variance\"\n",
        "        return True\n",
        "    \n",
        "    # Test 5: Positional encoding dimensions\n",
        "    def test_position_encoding_shape():\n",
        "        d_model = 64\n",
        "        max_len = 100\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
        "                            (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        assert pe.shape == (max_len, d_model), \\\n",
        "            f\"PE shape should be {(max_len, d_model)}, got {pe.shape}\"\n",
        "        assert -1 <= pe.min() <= pe.max() <= 1, \\\n",
        "            \"Sinusoidal PE should be in [-1, 1]\"\n",
        "        return True\n",
        "    \n",
        "    # Test 6: GQA memory savings\n",
        "    def test_gqa_parameters():\n",
        "        embed_dim = 64\n",
        "        num_heads = 8\n",
        "        num_kv_heads = 2\n",
        "        head_dim = embed_dim // num_heads\n",
        "        \n",
        "        mhsa_kv_params = 2 * embed_dim * embed_dim  # W_K + W_V\n",
        "        gqa_kv_params = 2 * embed_dim * (num_kv_heads * head_dim)\n",
        "        \n",
        "        assert gqa_kv_params < mhsa_kv_params, \\\n",
        "            \"GQA should have fewer KV parameters\"\n",
        "        reduction = mhsa_kv_params / gqa_kv_params\n",
        "        assert reduction == num_heads / num_kv_heads, \\\n",
        "            f\"Parameter reduction should be {num_heads/num_kv_heads}x\"\n",
        "        return True\n",
        "    \n",
        "    # Test 7: Sliding window mask\n",
        "    def test_sliding_window_mask():\n",
        "        seq_len = 8\n",
        "        window_size = 2\n",
        "        \n",
        "        mask = torch.zeros(seq_len, seq_len)\n",
        "        for i in range(seq_len):\n",
        "            for j in range(max(0, i - window_size), min(seq_len, i + window_size + 1)):\n",
        "                mask[i, j] = 1\n",
        "        \n",
        "        # Check that only nearby positions are 1\n",
        "        for i in range(seq_len):\n",
        "            for j in range(seq_len):\n",
        "                expected = 1 if abs(i - j) <= window_size else 0\n",
        "                assert mask[i, j] == expected, \\\n",
        "                    f\"Position ({i},{j}) should be {expected}\"\n",
        "        return True\n",
        "    \n",
        "    # Test 8: Cross-attention shapes\n",
        "    def test_cross_attention_shapes():\n",
        "        batch = 2\n",
        "        encoder_len = 10\n",
        "        decoder_len = 8\n",
        "        embed_dim = 64\n",
        "        \n",
        "        encoder_out = torch.randn(batch, encoder_len, embed_dim)\n",
        "        decoder_in = torch.randn(batch, decoder_len, embed_dim)\n",
        "        \n",
        "        W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "        \n",
        "        Q = W_q(decoder_in)  # Query from decoder\n",
        "        K = W_k(encoder_out)  # Key from encoder\n",
        "        V = W_v(encoder_out)  # Value from encoder\n",
        "        \n",
        "        scores = Q @ K.transpose(-2, -1) / math.sqrt(embed_dim)\n",
        "        assert scores.shape == (batch, decoder_len, encoder_len), \\\n",
        "            f\"Cross-attention scores should be (batch, dec, enc)\"\n",
        "        \n",
        "        attn = F.softmax(scores, dim=-1)\n",
        "        out = attn @ V\n",
        "        assert out.shape == (batch, decoder_len, embed_dim), \\\n",
        "            f\"Cross-attention output should match decoder sequence\"\n",
        "        return True\n",
        "    \n",
        "    # Run all tests\n",
        "    tests = [\n",
        "        ('Attention weights sum to 1', test_attention_sums_to_one),\n",
        "        ('Causal mask blocks future', test_causal_mask),\n",
        "        ('MHSA output shape', test_mhsa_output_shape),\n",
        "        ('Scaling reduces variance', test_scaling),\n",
        "        ('Position encoding shape', test_position_encoding_shape),\n",
        "        ('GQA parameter reduction', test_gqa_parameters),\n",
        "        ('Sliding window mask', test_sliding_window_mask),\n",
        "        ('Cross-attention shapes', test_cross_attention_shapes),\n",
        "    ]\n",
        "    \n",
        "    for name, test_fn in tests:\n",
        "        try:\n",
        "            result = test_fn()\n",
        "            if result:\n",
        "                print(f'  âœ… {name}')\n",
        "                tests_passed += 1\n",
        "            else:\n",
        "                print(f'  âŒ {name} (returned False)')\n",
        "                tests_failed += 1\n",
        "        except Exception as e:\n",
        "            print(f'  âŒ {name}: {str(e)[:50]}...')\n",
        "            tests_failed += 1\n",
        "    \n",
        "    print('\\n' + '='*70)\n",
        "    print(f'RESULTS: {tests_passed}/{tests_passed + tests_failed} tests passed')\n",
        "    if tests_failed == 0:\n",
        "        print('ğŸ‰ All tests passed! Your implementations are correct!')\n",
        "    else:\n",
        "        print(f'âš ï¸ {tests_failed} test(s) failed. Review the implementations above.')\n",
        "    print('='*70)\n",
        "    \n",
        "    return tests_passed, tests_failed\n",
        "\n",
        "# Run the tests\n",
        "run_all_tests()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
